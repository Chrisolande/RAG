{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellUniqueIdByVincent": "db184"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import asyncio\n",
    "import logging \n",
    "from pathlib import Path\n",
    "from typing import Tuple, Any, List, Dict, Optional\n",
    "from dataclasses import dataclass\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from langchain_cohere import CohereEmbeddings\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.document_loaders import (\n",
    "    TextLoader, \n",
    "    UnstructuredMarkdownLoader,\n",
    "    JSONLoader,\n",
    "    UnstructuredHTMLLoader,\n",
    "    PyPDFLoader\n",
    ")\n",
    "\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.schema import Document\n",
    "from langchain.prompts import ChatPromptTemplate, PromptTemplate\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "from langchain_core.runnables import RunnableParallel, RunnableLambda\n",
    "from langchain.callbacks.base import AsyncCallbackHandler\n",
    "from langchain.schema import LLMResult\n",
    "\n",
    "load_dotenv()\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cellUniqueIdByVincent": "dbac3"
   },
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class SelfRAGResponse:\n",
    "    \"\"\"Complete self rag with reflection\"\"\"\n",
    "    answer: str\n",
    "    retrieved_docs: List[Document]\n",
    "    reflection_score: float\n",
    "    needs_retrieval: bool\n",
    "    citations: List[str]\n",
    "    retrieval_decision_reasoning: str\n",
    "\n",
    "\n",
    "class RateLimitCallback(AsyncCallbackHandler):\n",
    "    \"\"\"Callback handler to manage API rate limiting with semaphores\"\"\"\n",
    "    \n",
    "    def __init__(self, semaphore: asyncio.Semaphore):\n",
    "        self.semaphore = semaphore\n",
    "        \n",
    "    async def on_llm_start(self, serialized: Dict[str, Any], prompts: List[str], **kwargs: Any) -> None:\n",
    "        await self.semaphore.acquire()\n",
    "        \n",
    "    async def on_llm_end(self, response: LLMResult, **kwargs: Any) -> None:\n",
    "        self.semaphore.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cellUniqueIdByVincent": "a00b3"
   },
   "outputs": [],
   "source": [
    "class DocumentLoader:\n",
    "    def __init__(self):\n",
    "        self.loaders = {\n",
    "            '.txt': TextLoader,\n",
    "            '.md': UnstructuredMarkdownLoader,\n",
    "            '.json': self._create_json_loader,\n",
    "            '.html': UnstructuredHTMLLoader,\n",
    "            '.py': TextLoader,\n",
    "            '.js': TextLoader,\n",
    "            '.css': TextLoader,\n",
    "            '.pdf': PyPDFLoader\n",
    "        }\n",
    "\n",
    "    def _create_json_loader(self, file_path: str):\n",
    "        \"\"\"Create JSON loader with custom jq_schema\"\"\"\n",
    "        return JSONLoader(\n",
    "            file_path=file_path,\n",
    "            jq_schema='.[]',\n",
    "            text_content=False\n",
    "        )\n",
    "\n",
    "    async def load_documents(self, kb_folder: str) -> List[Document]:\n",
    "        \"\"\"Load all documents from the knowledge base folder\"\"\"\n",
    "        documents = []\n",
    "        kb_path = Path(kb_folder)\n",
    "\n",
    "        if not kb_path.exists():\n",
    "            raise FileNotFoundError(f\"Knowledge base folder not found: {kb_path}\")\n",
    "\n",
    "        for file_path in kb_path.glob(\"**/*\"):\n",
    "            if file_path.is_file() and file_path.suffix.is_lower() in self.loaders:\n",
    "                try:\n",
    "                    loader_class = self.loaders[file_path.suffix.is_lower()]\n",
    "\n",
    "                    if file_path.suffix.lower() == \".json\":\n",
    "                        loader = loader_class(str(file_path))\n",
    "                    else:\n",
    "                        loader = loader_class(str(file_path))\n",
    "\n",
    "                    docs = loader.load()\n",
    "\n",
    "                    # Add metadata\n",
    "                    for doc in docs:\n",
    "                        doc.metadata.update({\n",
    "                            'file_path': str(file_path),\n",
    "                            'file_type': file_path.suffix,\n",
    "                            'file_name': file_path.name\n",
    "                        })\n",
    "\n",
    "                    documents.extend(docs)\n",
    "                \n",
    "                except Exception as e:\n",
    "                    logger.warning(f\"There was an error loading the knowledge base: {str(e)}\")\n",
    "                    # Fallback to TextLoader for unknown formats\n",
    "                    try:\n",
    "                        loader = TextLoader(str(file_path))\n",
    "                        docs = loader.load()\n",
    "                        # Add metadata\n",
    "                        for doc in docs:\n",
    "                            doc.metadata.update({\n",
    "                                'file_path': str(file_path),\n",
    "                                'file_type': file_path.suffix,\n",
    "                                'file_name': file_path.name\n",
    "                            })\n",
    "\n",
    "                        documents.extend(docs)\n",
    "\n",
    "                    except Exception as fallback_error:\n",
    "                        logger.error(f\"Failed to load {file_path} with fallback: {fallback_error}\")\n",
    "        \n",
    "        logger.info(f\"Loaded {len(documents)} documents from {kb_folder}\")\n",
    "        return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellUniqueIdByVincent": "fb77a"
   },
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class RAGSystem:\n",
    "    cohere_api_key: str\n",
    "    openrouter_api_key: str\n",
    "    kb_folder: str\n",
    "    max_concurrent_requests: int = 5\n",
    "    chunk_size: int = 2000\n",
    "    chunk_overlap: int = 200\n",
    "\n",
    "    def __post_init__(self):\n",
    "        # Initialize the components\n",
    "        self.embeddings = CohereEmbeddings(model = \"embed-v4.0\",\n",
    "                                         cohere_api_key = os.getenv(\"COHERE_API_KEY\"))\n",
    "\n",
    "        self.llm = ChatOpenAI(\n",
    "            model=\"meta-llama/llama-3.3-70b-instruct\",\n",
    "            openai_api_key=openrouter_api_key,\n",
    "            openai_api_base=\"https://openrouter.ai/api/v1\",\n",
    "            temperature=0.6,\n",
    "            max_tokens=1500\n",
    "        )\n",
    "\n",
    "        # Text Splitter\n",
    "        self.text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size = self.chunk_size,\n",
    "            chunk_overlap = self.chunk_overlap,\n",
    "            length_function = len\n",
    "        )\n",
    "\n",
    "        document_loader = DocumentLoader()\n",
    "\n",
    "        # Vector store\n",
    "        self.vector_store = Optional[FAISS] = None\n",
    "        self.kb_folder = kb_folder\n",
    "\n",
    "        # Semaphores for rate limiting\n",
    "        self.llm_semaphore = Semaphore(max_concurrent_requests)\n",
    "        self.embeddings_semaphore = Semaphore(max_concurrent_requests)\n",
    "\n",
    "        self.is_initialized = False\n",
    "\n",
    "        # Set up prompts\n",
    "        self._setup_prompts()\n",
    "\n",
    "    def _setup_prompts(self):\n",
    "        \"\"\"Set up prompts for different stages\"\"\"\n",
    "\n",
    "        # Retrieval decision prompt\n",
    "        self.retrieval_decision_prompt = PromptTemplate(\n",
    "            input_variables=[\"query\"],\n",
    "            template=\"\"\"\n",
    "            Analyze the following query to determine if it requires external knowledge retrieval.\n",
    "            \n",
    "            Query: \"{query}\"\n",
    "            \n",
    "            Consider:\n",
    "            1. Does this query ask for specific facts, data, or domain-specific information?\n",
    "            2. Would the answer benefit from external documents or knowledge base?\n",
    "            3. Is this asking about general knowledge that can be answered without retrieval?\n",
    "            4. Does it require recent or specialized information?\n",
    "            \n",
    "            Provide your reasoning and then answer with either \"RETRIEVE\" or \"NO_RETRIEVE\".\n",
    "            \n",
    "            Reasoning: [Explain your decision]\n",
    "            Decision: [RETRIEVE or NO_RETRIEVE]\n",
    "            \"\"\"\n",
    "        )\n",
    "\n",
    "        # Answer generation with retrieval prompt\n",
    "        self.rag_prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "            You are a helpful AI assistant. Use the following context documents to answer the user's question accurately and comprehensively.\n",
    "            \n",
    "            Context Documents:\n",
    "            {context}\n",
    "            \n",
    "            Question: {question}\n",
    "            \n",
    "            Instructions:\n",
    "            - Base your answer primarily on the provided context\n",
    "            - If the context doesn't contain sufficient information, acknowledge this\n",
    "            - Cite specific documents when referencing information\n",
    "            - Be accurate, detailed, and helpful\n",
    "            - If you need to use general knowledge to supplement the context, clearly indicate this\n",
    "            \n",
    "            Answer:\n",
    "        \"\"\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3748489",
   "metadata": {
    "cellUniqueIdByVincent": "363a6"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  },
  "vincent": {
   "sessionId": "3f2153659d5b596a4189cf1d_2025-05-30T15-07-59-793Z"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
