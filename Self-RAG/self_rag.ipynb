{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ab6155cf",
   "metadata": {
    "cellUniqueIdByVincent": "9eee7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Standard library imports\n",
    "import os\n",
    "import asyncio\n",
    "import logging\n",
    "from pathlib import Path\n",
    "from typing import Tuple, Any, List, Dict, Optional\n",
    "from dataclasses import dataclass\n",
    "from asyncio import sleep, Semaphore\n",
    "from tenacity import retry, stop_after_attempt, wait_exponential, retry_if_exception_type # To implement exponential backoff\n",
    "import httpx\n",
    "import numpy as np\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Embeddings and LLM imports\n",
    "from langchain_cohere import CohereEmbeddings\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# Vector store and text splitters imports \n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.docstore import InMemoryDocstore\n",
    "import faiss\n",
    "\n",
    "# Document Loader Imports\n",
    "from langchain_community.document_loaders import (\n",
    "    TextLoader, \n",
    "    UnstructuredMarkdownLoader,\n",
    "    JSONLoader,\n",
    "    UnstructuredHTMLLoader,\n",
    "    PyPDFLoader\n",
    ")\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "# Prompt and template imports\n",
    "from langchain.prompts import ChatPromptTemplate, PromptTemplate\n",
    "\n",
    "# Langchain  runnables and pipeline imports\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "from langchain_core.runnables import RunnableParallel, RunnableLambda\n",
    "\n",
    "# Callbacks and logging imports\n",
    "from langchain.callbacks.base import AsyncCallbackHandler\n",
    "from langchain.schema import LLMResult\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Configure environment variables\n",
    "load_dotenv()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "20293f49",
   "metadata": {
    "cellUniqueIdByVincent": "2ee0a"
   },
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class SelfRAGResponse:\n",
    "    \"\"\"Complete self rag with reflection\"\"\"\n",
    "    answer: str\n",
    "    retrieved_docs: List[Document]\n",
    "    reflection_score: float\n",
    "    needs_retrieval: bool\n",
    "    citations: List[str]\n",
    "    retrieval_decision_reasoning: str\n",
    "\n",
    "class RateLimitCallback(AsyncCallbackHandler):\n",
    "    \"\"\"Callback handler to manage API rate limiting with semaphores\"\"\"\n",
    "    \n",
    "    def __init__(self, semaphore: asyncio.Semaphore):\n",
    "        self.semaphore = semaphore\n",
    "        \n",
    "    async def on_llm_start(self, serialized: Dict[str, Any], prompts: List[str], **kwargs: Any) -> None:\n",
    "        await self.semaphore.acquire()\n",
    "        \n",
    "    async def on_llm_end(self, response: LLMResult, **kwargs: Any) -> None:\n",
    "        self.semaphore.release()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f0e1ab11",
   "metadata": {
    "cellUniqueIdByVincent": "948ec"
   },
   "outputs": [],
   "source": [
    "class RateLimitedCohereEmbeddings:\n",
    "    \"\"\"Wrapper for Cohere embeddings with rate limiting and retry logic\"\"\"\n",
    "    \n",
    "    def __init__(self, model: str, cohere_api_key: str, max_concurrent: int = 2, delay_between_calls: float = 0.5, batch_size: int = 30):\n",
    "        self.base_embeddings = CohereEmbeddings(\n",
    "            model=model,\n",
    "            cohere_api_key=cohere_api_key\n",
    "        )\n",
    "        self.semaphore = Semaphore(max_concurrent)\n",
    "        self.delay_between_calls = delay_between_calls\n",
    "        self.batch_size = batch_size\n",
    "        self.last_call_time = 0\n",
    "    \n",
    "    @retry(\n",
    "        stop=stop_after_attempt(5),\n",
    "        wait=wait_exponential(multiplier=1, min=1, max=60),\n",
    "        retry=retry_if_exception_type((httpx.HTTPStatusError, Exception))\n",
    "    )\n",
    "    async def _embed_with_retry(self, texts: List[str]) -> List[List[float]]:\n",
    "        \"\"\"Embed texts with retry logic\"\"\"\n",
    "        async with self.semaphore:\n",
    "            # Ensure minimum delay between calls\n",
    "            current_time = asyncio.get_event_loop().time()\n",
    "            time_since_last_call = current_time - self.last_call_time\n",
    "            if time_since_last_call < self.delay_between_calls:\n",
    "                await sleep(self.delay_between_calls - time_since_last_call)\n",
    "            \n",
    "            try:\n",
    "                logger.debug(f\"Embedding batch of {len(texts)} texts\")\n",
    "                result = await self.base_embeddings.aembed_documents(texts)\n",
    "                self.last_call_time = asyncio.get_event_loop().time()\n",
    "                return result\n",
    "                \n",
    "            except Exception as e:\n",
    "                if \"429\" in str(e) or \"Too Many Requests\" in str(e):\n",
    "                    logger.warning(f\"Rate limit hit, retrying after delay...\")\n",
    "                    await sleep(2)  # Additional delay for rate limits\n",
    "                    raise\n",
    "                else:\n",
    "                    logger.error(f\"Embedding error: {e}\")\n",
    "                    raise\n",
    "    \n",
    "    async def aembed_documents(self, texts: List[str]) -> List[List[float]]:\n",
    "        \"\"\"Embed documents with batching and rate limiting\"\"\"\n",
    "        if not texts:\n",
    "            return []\n",
    "        \n",
    "        # Use configurable batch size\n",
    "        all_embeddings = []\n",
    "        total_batches = (len(texts) + self.batch_size - 1) // self.batch_size\n",
    "        \n",
    "        logger.info(f\"Processing {len(texts)} texts in {total_batches} batches of {self.batch_size}\")\n",
    "        \n",
    "        for i in range(0, len(texts), self.batch_size):\n",
    "            batch_num = (i // self.batch_size) + 1\n",
    "            batch = texts[i:i + self.batch_size]\n",
    "            \n",
    "            logger.info(f\"Processing batch {batch_num}/{total_batches} ({len(batch)} texts)\")\n",
    "            \n",
    "            batch_embeddings = await self._embed_with_retry(batch)\n",
    "            all_embeddings.extend(batch_embeddings)\n",
    "            \n",
    "            # Delay between batches (except for the last one)\n",
    "            if i + self.batch_size < len(texts):\n",
    "                await sleep(self.delay_between_calls)\n",
    "        \n",
    "        return all_embeddings\n",
    "    \n",
    "    async def aembed_query(self, text: str) -> List[float]:\n",
    "        \"\"\"Embed a single query\"\"\"\n",
    "        async with self.semaphore:\n",
    "            current_time = asyncio.get_event_loop().time()\n",
    "            time_since_last_call = current_time - self.last_call_time\n",
    "            if time_since_last_call < self.delay_between_calls:\n",
    "                await sleep(self.delay_between_calls - time_since_last_call)\n",
    "            \n",
    "            result = await self.base_embeddings.aembed_query(text)\n",
    "            self.last_call_time = asyncio.get_event_loop().time()\n",
    "            return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "843e8cb6",
   "metadata": {
    "cellUniqueIdByVincent": "52d9a"
   },
   "outputs": [],
   "source": [
    "class DocumentLoader:\n",
    "    def __init__(self):\n",
    "        self.loaders = {\n",
    "            \".txt\": TextLoader,\n",
    "            \".md\": UnstructuredMarkdownLoader,\n",
    "            \".json\": self._create_json_loader,\n",
    "            \".pdf\": PyPDFLoader,\n",
    "            \".html\": UnstructuredHTMLLoader,\n",
    "            \".py\": TextLoader,\n",
    "            \".js\": TextLoader,\n",
    "            \".css\": TextLoader\n",
    "        }\n",
    "\n",
    "    def _create_json_loader(self, file_path: str):\n",
    "        \"\"\"Create JSON loader with custom jq_schema\"\"\"\n",
    "        return JSONLoader(\n",
    "            file_path=file_path,\n",
    "            jq_schema='.[]',\n",
    "            text_content=False\n",
    "        )\n",
    "\n",
    "    async def load_documents(self, kb_folder: str) -> List[Document]:\n",
    "        \"\"\"Load all documents from the knowledge base folder\"\"\"\n",
    "        documents = []\n",
    "        kb_path = Path(kb_folder)\n",
    "\n",
    "        if not kb_path.exists():\n",
    "            raise FileNotFoundError(f\"Knowledge base folder not found: {kb_path}\")\n",
    "\n",
    "        for file_path in kb_path.glob(\"**/*\"):\n",
    "            if file_path.is_file() and file_path.suffix.lower() in self.loaders:\n",
    "                try:\n",
    "                    loader_class = self.loaders[file_path.suffix.lower()]\n",
    "\n",
    "                    if file_path.suffix.lower() == \".json\":\n",
    "                        loader = loader_class(str(file_path))\n",
    "                    else:\n",
    "                        loader = loader_class(str(file_path))\n",
    "\n",
    "                    docs = loader.load()\n",
    "\n",
    "                    # Add metadata\n",
    "                    for doc in docs:\n",
    "                        doc.metadata.update({\n",
    "                            'file_path': str(file_path),\n",
    "                            'file_type': file_path.suffix,\n",
    "                            'file_name': file_path.name\n",
    "                        })\n",
    "\n",
    "                    documents.extend(docs)\n",
    "                \n",
    "                except Exception as e:\n",
    "                    logger.warning(f\"There was an error loading the knowledge base: {str(e)}\")\n",
    "                    # Fallback to TextLoader for unknown formats\n",
    "                    try:\n",
    "                        loader = TextLoader(str(file_path))\n",
    "                        docs = loader.load()\n",
    "                        # Add metadata\n",
    "                        for doc in docs:\n",
    "                            doc.metadata.update({\n",
    "                                'file_path': str(file_path),\n",
    "                                'file_type': file_path.suffix,\n",
    "                                'file_name': file_path.name\n",
    "                            })\n",
    "\n",
    "                        documents.extend(docs)\n",
    "\n",
    "                    except Exception as fallback_error:\n",
    "                        logger.error(f\"Failed to load {file_path} with fallback: {fallback_error}\")\n",
    "        \n",
    "        logger.info(f\"Loaded {len(documents)} documents from {kb_folder}\")\n",
    "        return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b5fb11cd",
   "metadata": {
    "cellUniqueIdByVincent": "a8e95"
   },
   "outputs": [],
   "source": [
    "\n",
    "@dataclass\n",
    "class RAGSystem:\n",
    "    cohere_api_key: str\n",
    "    openrouter_api_key: str\n",
    "    kb_folder: str\n",
    "    vector_store_path: str = None\n",
    "    max_concurrent_requests: int = 3  \n",
    "    max_concurrent_embeddings: int = 5  \n",
    "    embedding_delay: float = 0.2  \n",
    "    embedding_batch_size: int = 96  \n",
    "    chunk_size: int = 2000\n",
    "    chunk_overlap: int = 200\n",
    "    auto_save_vector_store: bool = True\n",
    "\n",
    "    def __post_init__(self):\n",
    "        # Initialize with rate-limited embeddings\n",
    "        self.embeddings = RateLimitedCohereEmbeddings(\n",
    "            model=\"embed-v4.0\",\n",
    "            cohere_api_key=self.cohere_api_key,\n",
    "            max_concurrent=self.max_concurrent_embeddings,\n",
    "            delay_between_calls=self.embedding_delay,\n",
    "            batch_size=self.embedding_batch_size\n",
    "        )\n",
    "\n",
    "        self.llm = ChatOpenAI(\n",
    "            model=\"meta-llama/llama-3.3-70b-instruct\",\n",
    "            openai_api_key=self.openrouter_api_key,\n",
    "            openai_api_base=\"https://openrouter.ai/api/v1\",\n",
    "            temperature=0.6,\n",
    "            max_tokens=1500\n",
    "        )\n",
    "\n",
    "        # Text Splitter\n",
    "        self.text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=self.chunk_size,\n",
    "            chunk_overlap=self.chunk_overlap,\n",
    "            length_function=len\n",
    "        )\n",
    "\n",
    "        self.document_loader = DocumentLoader()\n",
    "\n",
    "        # Vector store\n",
    "        self.vector_store: Optional[FAISS] = None\n",
    "        \n",
    "        # Set default vector store path if not provided\n",
    "        if self.vector_store_path is None:\n",
    "            self.vector_store_path = os.path.join(self.kb_folder, \"vector_store\")\n",
    "\n",
    "        # Semaphores for rate limiting\n",
    "        self.llm_semaphore = Semaphore(self.max_concurrent_requests)\n",
    "\n",
    "        self.rate_limit_callback = RateLimitCallback(self.llm_semaphore)\n",
    "\n",
    "        self.is_initialized = False\n",
    "\n",
    "        # Set up prompts\n",
    "        self._setup_prompts()\n",
    "\n",
    "    def _setup_prompts(self):\n",
    "        \"\"\"Set up prompts for different stages\"\"\"\n",
    "\n",
    "        # Retrieval decision prompt\n",
    "        self.retrieval_decision_prompt = PromptTemplate(\n",
    "            input_variables=[\"query\"],\n",
    "            template=\"\"\"\n",
    "            Analyze the following query to determine if it requires external knowledge retrieval.\n",
    "            \n",
    "            Query: \"{query}\"\n",
    "            \n",
    "            Consider:\n",
    "            1. Does this query ask for specific facts, data, or domain-specific information?\n",
    "            2. Would the answer benefit from external documents or knowledge base?\n",
    "            3. Is this asking about general knowledge that can be answered without retrieval?\n",
    "            4. Does it require recent or specialized information?\n",
    "            \n",
    "            Provide your reasoning and then answer with either \"RETRIEVE\" or \"NO_RETRIEVE\".\n",
    "            \n",
    "            Reasoning: [Explain your decision]\n",
    "            Decision: [RETRIEVE or NO_RETRIEVE]\n",
    "            \"\"\"\n",
    "        )\n",
    "\n",
    "        # Answer generation with retrieval prompt\n",
    "        self.rag_prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "            You are a helpful AI assistant. Use the following context documents to answer the user's question accurately and comprehensively.\n",
    "            \n",
    "            Context Documents:\n",
    "            {context}\n",
    "            \n",
    "            Question: {question}\n",
    "            \n",
    "            Instructions:\n",
    "            - Base your answer primarily on the provided context\n",
    "            - If the context doesn't contain sufficient information, acknowledge this\n",
    "            - Cite specific documents when referencing information\n",
    "            - Be accurate, detailed, and helpful\n",
    "            - If you need to use general knowledge to supplement the context, clearly indicate this\n",
    "            \n",
    "            Answer:\n",
    "        \"\"\")\n",
    "\n",
    "        # Answer generation without retrieval prompt\n",
    "        self.no_retrieval_prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "            You are a helpful AI assistant. Answer the following question using your general knowledge.\n",
    "            \n",
    "            Question: {question}\n",
    "            \n",
    "            Provide a comprehensive and accurate answer based on your training knowledge.\n",
    "            \n",
    "            Answer:\n",
    "        \"\"\")\n",
    "\n",
    "        # Reflection prompt\n",
    "        self.reflection_prompt = PromptTemplate(\n",
    "            input_variables=[\"query\", \"answer\", \"context\"],\n",
    "            template=\"\"\"\n",
    "            Evaluate the quality of the following answer based on the query and available context.\n",
    "            \n",
    "            Query: {query}\n",
    "            \n",
    "            Context:\n",
    "            {context}\n",
    "            \n",
    "            Answer: {answer}\n",
    "            \n",
    "            Rate the answer on a scale of 0-10 considering:\n",
    "            - Accuracy and factual correctness\n",
    "            - Completeness and comprehensiveness\n",
    "            - Relevance to the query\n",
    "            - Proper use of available context\n",
    "            - Clarity and helpfulness\n",
    "            \n",
    "            Provide only a single number between 0 and 10 as your rating.\n",
    "            \n",
    "            Rating:\n",
    "            \"\"\"\n",
    "        )\n",
    "\n",
    "    async def initialize(self, force_rebuild: bool = False):\n",
    "        \"\"\"Initialize the RAG System with option to force rebuild\"\"\"\n",
    "        if self.is_initialized and not force_rebuild:\n",
    "            return\n",
    "        \n",
    "        logger.info(\"Initializing the RAG system ...\")\n",
    "\n",
    "        # Try to load existing vector store first (unless force rebuild)\n",
    "        if not force_rebuild and os.path.exists(self.vector_store_path):\n",
    "            try:\n",
    "                await self.load_vector_store(self.vector_store_path)\n",
    "                logger.info(\"Loaded existing vector store successfully\")\n",
    "                return\n",
    "            except Exception as e:\n",
    "                logger.warning(f\"Failed to load existing vector store: {e}. Building new one...\")\n",
    "\n",
    "        # Load the documents\n",
    "        documents = await self.document_loader.load_documents(self.kb_folder)\n",
    "\n",
    "        if not documents:\n",
    "            logger.warning(\"No documents were found in the knowledge base ...\")\n",
    "            self.vector_store = None\n",
    "            self.is_initialized = True\n",
    "            return\n",
    "\n",
    "        # Split the documents into chunks\n",
    "        split_docs = self.text_splitter.split_documents(documents)\n",
    "        logger.info(f\"Split {len(documents)} documents into {len(split_docs)} chunks\")\n",
    "\n",
    "        # Create the vector store with progress tracking\n",
    "        logger.info(\"Creating embeddings (this may take a while due to rate limiting)...\")\n",
    "        \n",
    "        try:\n",
    "            self.vector_store = await self._create_vector_store_with_progress(split_docs)\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to create vector store: {e}\")\n",
    "            raise\n",
    "\n",
    "        # ALWAYS SAVE THE VECTOR STORE AFTER CREATION\n",
    "        if self.auto_save_vector_store:\n",
    "            await self.save_vector_store(self.vector_store_path)\n",
    "            logger.info(f\"Vector store automatically saved to {self.vector_store_path}\")\n",
    "\n",
    "        self.is_initialized = True\n",
    "        logger.info(\"Self-RAG system initialized successfully\")\n",
    "\n",
    "    async def _create_vector_store_with_progress(self, documents: List[Document]) -> FAISS:\n",
    "        \n",
    "        logger.info(f\"Creating embeddings for {len(documents)} documents...\")\n",
    "        texts = [doc.page_content for doc in documents]\n",
    "        embeddings_list = await self.embeddings.aembed_documents(texts)\n",
    "        \n",
    "        # Convert embeddings to numpy array\n",
    "        embedding_matrix = np.array(embeddings_list).astype('float32')\n",
    "        \n",
    "        # Create FAISS index\n",
    "        dimension = embedding_matrix.shape[1]\n",
    "        index = faiss.IndexFlatL2(dimension)\n",
    "        index.add(embedding_matrix)\n",
    "        \n",
    "        # Create docstore and index mapping\n",
    "        docstore = InMemoryDocstore({str(i): doc for i, doc in enumerate(documents)})\n",
    "        index_to_docstore_id = {i: str(i) for i in range(len(documents))}\n",
    "        \n",
    "        # Create FAISS vector store\n",
    "        vector_store = FAISS(\n",
    "            embedding_function=self.embeddings.base_embeddings,\n",
    "            index=index,\n",
    "            docstore=docstore,\n",
    "            index_to_docstore_id=index_to_docstore_id\n",
    "        )\n",
    "        \n",
    "        logger.info(\"Vector store created successfully\")\n",
    "        return vector_store\n",
    "\n",
    "    async def _should_retrieve(self, query: str) -> Tuple[bool, str]:\n",
    "        \"\"\" Determine if the retrieval is needed and get reasoning\"\"\"\n",
    "        chain = self.retrieval_decision_prompt | self.llm.with_config(callbacks=[self.rate_limit_callback])\n",
    "\n",
    "        result = await chain.ainvoke({\"query\": query})\n",
    "\n",
    "        # Parse the result\n",
    "        lines = result.content.strip().split('\\n')\n",
    "        reasoning = \"\"\n",
    "        decision = False\n",
    "\n",
    "        for line in lines:\n",
    "            if line.startswith(\"Reasoning:\"):\n",
    "                reasoning = line.replace(\"Reasoning:\", \"\").strip()\n",
    "            elif line.startswith(\"Decision:\"):\n",
    "                decision_text = line.replace(\"Decision:\", \"\").strip()\n",
    "                decision = \"RETRIEVE\" in decision_text.upper()\n",
    "        \n",
    "        return decision, reasoning\n",
    "\n",
    "    async def _retrieve_documents(self, query: str, k:int = 5) -> List[Document]:\n",
    "        \"\"\"Retrieve relevant documents\"\"\"\n",
    "        if not self.vector_store:\n",
    "            return []\n",
    "\n",
    "        # Use similarity search with scores\n",
    "        doc_with_scores = await self.vector_store.asimilarity_search_with_score(query, k = k)\n",
    "        # Filter by relevance score\n",
    "        relevant_docs = [doc for doc, score in doc_with_scores if score > 0.8]\n",
    "\n",
    "        return relevant_docs\n",
    "\n",
    "    async def _generate_answer_with_retrieval(self, query: str, documents: List[Document]) -> str:\n",
    "        \"\"\"Generate answers using retrieved documents\"\"\"\n",
    "        context = \"\\n\\n\".join([\n",
    "            f\"Document: {doc.metadata.get('file_name', 'Unknown')}\\n{doc.page_content}\"\n",
    "            for doc in documents\n",
    "        ])\n",
    "        \n",
    "        chain = self.rag_prompt | self.llm | StrOutputParser()\n",
    "        \n",
    "        result = await chain.ainvoke({\n",
    "            \"context\": context,\n",
    "            \"question\": query\n",
    "        })\n",
    "        \n",
    "        return result\n",
    "        \n",
    "    async def _generate_answer_without_retrieval(self, query: str) -> str:\n",
    "        \"\"\"Generate answer without retrieval\"\"\"\n",
    "        chain = self.no_retrieval_prompt | self.llm | StrOutputParser()\n",
    "        \n",
    "        result = await chain.ainvoke({\"question\": query})\n",
    "        return result\n",
    "\n",
    "    async def _reflect_on_answer(self, query: str, answer: str, documents: List[Document]) -> float:\n",
    "        \"\"\"Reflect on answer quality\"\"\"\n",
    "        context = \"\\n\\n\".join([doc.page_content for doc in documents]) if documents else \"No context provided\"\n",
    "        \n",
    "        chain = self.reflection_prompt | self.llm.with_config(callbacks=[self.rate_limit_callback])\n",
    "        \n",
    "        result = await chain.ainvoke({\n",
    "            \"query\": query,\n",
    "            \"answer\": answer,\n",
    "            \"context\": context\n",
    "        })\n",
    "        \n",
    "        try:\n",
    "            content = result.content if hasattr(result, 'content') else str(result)\n",
    "            score_str = content.strip().split('\\n')[-1]\n",
    "            \n",
    "            # Extract digits and decimal point from the score string\n",
    "            score_chars = ''.join(c for c in score_str if c.isdigit() or c == '.')\n",
    "            \n",
    "            if score_chars:\n",
    "                score = float(score_chars)\n",
    "                return min(10.0, max(0.0, score))  # Clamp between 0 and 10\n",
    "            else:\n",
    "                logger.warning(f\"No numeric score found in: {score_str}\")\n",
    "                return 5.0\n",
    "                \n",
    "        except (ValueError, IndexError) as e:\n",
    "            logger.warning(f\"Could not parse reflection score: {result}. Error: {e}\")\n",
    "            return 5.0\n",
    "\n",
    "    async def query(self, query: str, force_retrieval: bool = False) -> SelfRAGResponse:\n",
    "        \"\"\"Process query using the SelfRAG\"\"\"\n",
    "        if not self.is_initialized:\n",
    "            await self.initialize()\n",
    "\n",
    "        # Decide whether we need retrieval\n",
    "        if force_retrieval:\n",
    "            needs_retrieval = True\n",
    "            reasoning = \"Forced retrieval requested\"\n",
    "        else:\n",
    "            needs_retrieval, reasoning = await self._should_retrieve(query)\n",
    "\n",
    "        retrieved_docs = []\n",
    "        citations = []\n",
    "\n",
    "        # Retrieve documents if needed\n",
    "        if needs_retrieval and self.vector_store:\n",
    "            retrieved_docs = await self._retrieve_documents(query)\n",
    "            citations = [\n",
    "                doc.metadata.get('file_path', f\"Document {i}\")\n",
    "                for i, doc in enumerate(retrieved_docs)\n",
    "            ]\n",
    "\n",
    "        # Generate answer\n",
    "        if retrieved_docs:\n",
    "            answer = await self._generate_answer_with_retrieval(query, retrieved_docs)\n",
    "        else:\n",
    "            answer = await self._generate_answer_without_retrieval(query)\n",
    "\n",
    "        # Reflect on answer quality\n",
    "        reflection_score = await self._reflect_on_answer(query, answer, retrieved_docs)\n",
    "\n",
    "        return SelfRAGResponse(\n",
    "            answer=answer,\n",
    "            retrieved_docs=retrieved_docs,\n",
    "            reflection_score=reflection_score,\n",
    "            needs_retrieval=needs_retrieval,\n",
    "            citations=list(set(citations)),  # Remove duplicates\n",
    "            retrieval_decision_reasoning=reasoning\n",
    "        )\n",
    "\n",
    "    async def save_vector_store(self, path: str = None):\n",
    "        \"\"\"Save vector store to disk\"\"\"\n",
    "        if not self.vector_store:\n",
    "            logger.warning(\"No vector store to save\")\n",
    "            return\n",
    "            \n",
    "        save_path = path or self.vector_store_path\n",
    "        \n",
    "        # Create directory if it doesn't exist\n",
    "        os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "        \n",
    "        try:\n",
    "            self.vector_store.save_local(save_path)\n",
    "            logger.info(f\"Vector store saved to {save_path}\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to save vector store: {e}\")\n",
    "            raise\n",
    "    \n",
    "    async def load_vector_store(self, path: str = None):\n",
    "        from langchain_community.vectorstores import FAISS\n",
    "        load_path = path or self.vector_store_path\n",
    "        try:\n",
    "            self.vector_store = FAISS.load_local(\n",
    "                load_path,\n",
    "                self.embeddings.base_embeddings,\n",
    "                allow_dangerous_deserialization=True\n",
    "            )\n",
    "            if not isinstance(self.vector_store, FAISS):\n",
    "                raise ValueError(\"Loaded vector store is not a valid FAISS object\")\n",
    "            self.is_initialized = True\n",
    "            logger.info(f\"Vector store loaded from {load_path}\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to load vector store from {load_path}: {e}\")\n",
    "            self.vector_store = None\n",
    "            raise\n",
    "\n",
    "    async def rebuild_vector_store(self):\n",
    "        logger.info(\"Force rebuilding vector store...\")\n",
    "        self.vector_store = None\n",
    "        await self.initialize(force_rebuild=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7bab832c",
   "metadata": {
    "cellUniqueIdByVincent": "83e4e"
   },
   "outputs": [],
   "source": [
    "class BatchProcessor:\n",
    "    \"\"\"Process multiple queries in batch with concurrency control\"\"\"\n",
    "    \n",
    "    def __init__(self, rag_system: RAGSystem, max_concurrent: int = 3):\n",
    "        self.rag_system = rag_system\n",
    "        self.semaphore = asyncio.Semaphore(max_concurrent)\n",
    "    \n",
    "    async def process_query(self, query: str) -> Tuple[str, SelfRAGResponse]:\n",
    "        \"\"\"Process a single query with semaphore\"\"\"\n",
    "        async with self.semaphore:\n",
    "            response = await self.rag_system.query(query)\n",
    "            return query, response\n",
    "    \n",
    "    async def process_batch(self, queries: List[str]) -> Dict[str, SelfRAGResponse]:\n",
    "        \"\"\"Process multiple queries concurrently\"\"\"\n",
    "        tasks = [self.process_query(query) for query in queries]\n",
    "        results = await asyncio.gather(*tasks, return_exceptions=True)\n",
    "        \n",
    "        output = {}\n",
    "        for result in results:\n",
    "            if isinstance(result, Exception):\n",
    "                logger.error(f\"Error processing query: {result}\")\n",
    "            else:\n",
    "                query, response = result\n",
    "                output[query] = response\n",
    "        \n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be260d94",
   "metadata": {
    "cellUniqueIdByVincent": "4beb4"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Initializing the RAG system ...\n",
      "INFO:__main__:Loaded 7 documents from books\n",
      "INFO:__main__:Split 7 documents into 1291 chunks\n",
      "INFO:__main__:Creating embeddings (this may take a while due to rate limiting)...\n",
      "INFO:__main__:Creating embeddings for 1291 documents...\n",
      "INFO:__main__:Processing 1291 texts in 14 batches of 96\n",
      "INFO:__main__:Processing batch 1/14 (96 texts)\n",
      "INFO:httpx:HTTP Request: POST https://api.cohere.com/v1/embed \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Processing batch 2/14 (96 texts)\n",
      "INFO:httpx:HTTP Request: POST https://api.cohere.com/v1/embed \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Processing batch 3/14 (96 texts)\n",
      "INFO:httpx:HTTP Request: POST https://api.cohere.com/v1/embed \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Processing batch 4/14 (96 texts)\n",
      "INFO:httpx:HTTP Request: POST https://api.cohere.com/v1/embed \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Processing batch 5/14 (96 texts)\n",
      "INFO:httpx:HTTP Request: POST https://api.cohere.com/v1/embed \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Processing batch 6/14 (96 texts)\n",
      "INFO:httpx:HTTP Request: POST https://api.cohere.com/v1/embed \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Processing batch 7/14 (96 texts)\n",
      "INFO:httpx:HTTP Request: POST https://api.cohere.com/v1/embed \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Processing batch 8/14 (96 texts)\n",
      "INFO:httpx:HTTP Request: POST https://api.cohere.com/v1/embed \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Processing batch 9/14 (96 texts)\n",
      "INFO:httpx:HTTP Request: POST https://api.cohere.com/v1/embed \"HTTP/1.1 429 Too Many Requests\"\n",
      "WARNING:langchain_cohere.utils:Retrying langchain_cohere.embeddings.CohereEmbeddings.aembed_with_retry.<locals>._embed_with_retry in 4.0 seconds as it raised TooManyRequestsError: status_code: 429, body: {'message': 'trial token rate limit exceeded, limit is 100000 tokens per minute'}.\n",
      "INFO:httpx:HTTP Request: POST https://api.cohere.com/v1/embed \"HTTP/1.1 429 Too Many Requests\"\n",
      "WARNING:langchain_cohere.utils:Retrying langchain_cohere.embeddings.CohereEmbeddings.aembed_with_retry.<locals>._embed_with_retry in 4.0 seconds as it raised TooManyRequestsError: status_code: 429, body: {'message': 'trial token rate limit exceeded, limit is 100000 tokens per minute'}.\n",
      "INFO:httpx:HTTP Request: POST https://api.cohere.com/v1/embed \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Processing batch 10/14 (96 texts)\n",
      "INFO:httpx:HTTP Request: POST https://api.cohere.com/v1/embed \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Processing batch 11/14 (96 texts)\n",
      "INFO:httpx:HTTP Request: POST https://api.cohere.com/v1/embed \"HTTP/1.1 429 Too Many Requests\"\n",
      "WARNING:langchain_cohere.utils:Retrying langchain_cohere.embeddings.CohereEmbeddings.aembed_with_retry.<locals>._embed_with_retry in 4.0 seconds as it raised TooManyRequestsError: status_code: 429, body: {'message': 'trial token rate limit exceeded, limit is 100000 tokens per minute'}.\n",
      "INFO:httpx:HTTP Request: POST https://api.cohere.com/v1/embed \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Processing batch 12/14 (96 texts)\n",
      "INFO:httpx:HTTP Request: POST https://api.cohere.com/v1/embed \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Processing batch 13/14 (96 texts)\n",
      "INFO:httpx:HTTP Request: POST https://api.cohere.com/v1/embed \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Processing batch 14/14 (43 texts)\n",
      "INFO:httpx:HTTP Request: POST https://api.cohere.com/v1/embed \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Vector store created successfully\n",
      "INFO:__main__:Vector store saved to books/vector_store\n",
      "INFO:__main__:Vector store automatically saved to books/vector_store\n",
      "INFO:__main__:Self-RAG system initialized successfully\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "Query: Who is Chris Olande?\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.cohere.com/v1/embed \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Needs Retrieval: True\n",
      "Reasoning: The query \"Who is Chris Olande?\" asks for specific information about an individual, which suggests it requires external knowledge retrieval. This type of query typically seeks factual or biographical data that may not be readily available without accessing external sources such as databases, documents, or the internet. The answer would indeed benefit from consulting external documents or a knowledge base, as the information about Chris Olande is not general knowledge that everyone would be expected to know. Furthermore, without more context, it's unclear if Chris Olande is a public figure, historical person, or someone who has recently gained prominence, which means the query could require recent or specialized information to answer accurately.\n",
      "Retrieved Documents: 5\n",
      "Reflection Score: 9.0/10\n",
      "Citations: ['books/olande.txt', 'books/frankenstein.txt']\n",
      "\n",
      "Answer:\n",
      "Chris Olande is a dynamic and intellectually curious student of Statistics and Programming at Kenyatta University, as described in the document \"olande.txt\". According to this document, Chris has a growing portfolio of sophisticated projects that blend statistical rigor with cutting-edge machine learning techniques, demonstrating a rare combination of technical excellence, strategic thinking, and human-centered values.\n",
      "\n",
      "As stated in \"olande.txt\", Chris's academic background in statistics is complemented by a robust understanding of programming, especially in Python, with proficiency in various libraries such as NumPy, pandas, and Hugging Face Transformers. His technical expertise includes applied knowledge of exploratory factor analysis, hypothesis testing, regression modeling, and time series analysis, as well as fluency in building modular systems, implementing neural networks, and developing natural language processing (NLP) pipelines.\n",
      "\n",
      "The document \"olande.txt\" also highlights Chris's leadership in education, particularly in organizing a field study for Grade 7 students to visit Impala Glass Industries in Nairobi, showcasing his commitment to nurturing curiosity and STEM literacy among younger learners. Additionally, it mentions his work on machine learning and NLP projects, including training and fine-tuning transformer models, such as BERT and DistilBERT, and exploring retrieval-augmented generation (RAG) systems.\n",
      "\n",
      "Chris's professional traits and work ethic are characterized as methodical, results-oriented, and deeply inquisitive, with a willingness to experiment with new frameworks and learn from feedback, as noted in \"olande.txt\". His career aspirations and potential are outlined as being well-positioned for roles in data science, machine learning engineering, research, or AI systems development, with a current focus on agentic RAGs suggesting a future in intelligent information systems and AI product development.\n",
      "\n",
      "In summary, based on the information provided in \"olande.txt\", Chris Olande is a promising young professional with a strong foundation in statistics and programming, a passion for innovative applications in real-world contexts, and a commitment to education and community engagement. The document \"frankenstein.txt\" does not provide any relevant information about Chris Olande, as it appears to be an excerpt from the novel \"Frankenstein\" and is not related to the context of Chris Olande.\n",
      "\n",
      "================================================================================\n",
      "Query: What has Chris Olande studied at Kenyatta University?\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.cohere.com/v1/embed \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Needs Retrieval: True\n",
      "Reasoning: The query is specific to an individual's academic background at a particular institution, which necessitates accessing external documents or a knowledge base for an accurate answer. General knowledge or non-retrieval methods are unlikely to provide the specific information requested.\n",
      "Retrieved Documents: 5\n",
      "Reflection Score: 9.0/10\n",
      "Citations: ['books/olande.txt', 'books/frankenstein.txt']\n",
      "\n",
      "Answer:\n",
      "According to the provided context, specifically in the document \"olande.txt\", Chris Olande has studied Statistics and Programming at Kenyatta University. This is mentioned in the introduction of the document, which states: \"Chris Olande is a dynamic and intellectually curious student of Statistics and Programming at Kenyatta University...\" (Document: olande.txt). \n",
      "\n",
      "Additionally, the document highlights his academic background and technical expertise, mentioning that his academic foundation in statistics is complemented by a robust understanding of programming, especially in Python (Document: olande.txt). \n",
      "\n",
      "Therefore, based on the context provided, Chris Olande's studies at Kenyatta University include Statistics and Programming, with a focus on Python programming.\n",
      "\n",
      "================================================================================\n",
      "Query: What is the central conflict in Romeo and Juliet?\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.cohere.com/v1/embed \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Needs Retrieval: True\n",
      "Reasoning: The query is about a well-known piece of literature and asks for information that is considered general knowledge within the literary domain. While detailed analysis might benefit from external sources, the basic answer to the central conflict does not require retrieval of new information.\n",
      "Retrieved Documents: 5\n",
      "Reflection Score: 9.0/10\n",
      "Citations: ['books/romeo_and_juliet.txt']\n",
      "\n",
      "Answer:\n",
      "The central conflict in Romeo and Juliet is the feud between the two rival families, the Montagues and the Capulets, which ultimately leads to the tragic demise of the two lovers. According to the Dramatis Personæ in the provided context (Document: romeo_and_juliet.txt), the Montagues and Capulets are described as \"two households, both alike in dignity, / In fair Verona, where we lay our scene, / From ancient grudge break to new mutiny\" (THE PROLOGUE, Document: romeo_and_juliet.txt). This long-standing grudge between the families creates a sense of tension and conflict that permeates the entire play.\n",
      "\n",
      "The conflict is further highlighted in Act I, Scene I, where Sampson and Gregory, servants of the Capulets, discuss their hatred for the Montagues (Document: romeo_and_juliet.txt). The feud is also evident in the interactions between Tybalt, a Capulet, and Romeo, a Montague, which ultimately leads to a duel and Tybalt's death (Document: romeo_and_juliet.txt).\n",
      "\n",
      "The love between Romeo and Juliet, which blossoms despite the families' animosity towards each other, exacerbates the conflict. As Juliet says, \"My only love sprung from my only hate! / Too early seen unknown, and known too late! / Prodigious birth of love it is to me, / That I must love a loathed enemy\" (Document: romeo_and_juliet.txt). The fact that Romeo is a Montague and Juliet is a Capulet makes their love impossible in the eyes of their families, leading to a series of tragic events.\n",
      "\n",
      "While the provided context does not contain the entire play, it is clear that the central conflict in Romeo and Juliet is the feud between the Montagues and Capulets, which ultimately leads to the tragic demise of the two lovers. This is supported by the Dramatis Personæ, the dialogue between characters, and the overall tone of the play. (General knowledge of the play's plot and themes is also consistent with this interpretation, but the provided context is sufficient to establish the central conflict.)\n",
      "\n",
      "================================================================================\n",
      "Query: How does Shakespeare portray love and fate in Romeo and Juliet?\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.cohere.com/v1/embed \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Needs Retrieval: True\n",
      "Reasoning: This query asks for an analysis of Shakespeare's portrayal of love and fate in Romeo and Juliet, which requires a deep understanding of the play's themes, characters, and literary devices. The answer would benefit from external documents or a knowledge base, such as literary critiques, analyses, or summaries of the play. While general knowledge of the play's plot and characters may be sufficient to provide a basic answer, a more nuanced and detailed analysis would require retrieval of external information. The query does not ask for specific facts or data, but rather an interpretation of the play's themes, which suggests that external knowledge retrieval would be necessary to provide a comprehensive answer. Additionally, the query does not require recent information, as the play is a classic work of literature, but it does require specialized knowledge of literary analysis and criticism.\n",
      "Retrieved Documents: 5\n",
      "Reflection Score: 8.0/10\n",
      "Citations: ['books/romeo_and_juliet.txt']\n",
      "\n",
      "Answer:\n",
      "Shakespeare portrays love and fate in Romeo and Juliet as intertwined and inescapable forces that drive the plot and ultimately lead to the tragic conclusion. In the provided context, specifically in \"romeo_and_juliet.txt,\" Juliet's speech in Act 3, Scene 2, showcases her intense longing for Romeo and her desire for night to fall so they can be together (Document: romeo_and_juliet.txt). This highlights the all-consuming nature of their love, which is further emphasized by Juliet's declaration that \"I have bought the mansion of a love, / But not possess'd it; and though I am sold, / Not yet enjoy'd\" (Document: romeo_and_juliet.txt). This suggests that their love is a powerful and overwhelming force that cannot be denied.\n",
      "\n",
      "Fate, on the other hand, is depicted as a force that is beyond the control of the characters. In the same document, the Chorus notes that \"Now old desire doth in his deathbed lie, / And young affection gapes to be his heir\" (Document: romeo_and_juliet.txt), implying that the course of events is predetermined and inevitable. The feud between the Montagues and Capulets, which is a central aspect of the play, is also portrayed as a fate that the characters cannot escape. Lady Capulet's statement that \"We will have vengeance for it, fear thou not\" (Document: romeo_and_juliet.txt) suggests that the conflict between the families is a self-perpetuating cycle that will ultimately lead to tragedy.\n",
      "\n",
      "The intersection of love and fate is perhaps most clearly seen in the character of Romeo, who is torn between his love for Juliet and his loyalty to his family. His decision to kill Tybalt, which leads to his banishment, is a pivotal moment in the play that sets in motion the events that will ultimately lead to the tragic conclusion. As Juliet notes, \"O serpent heart, hid with a flowering face! / Did ever dragon keep so fair a cave?\" (Document: romeo_and_juliet.txt), highlighting the contradictions and complexities of Romeo's character.\n",
      "\n",
      "While the provided context does not contain the entire play, it is clear that Shakespeare portrays love and fate as intertwined forces that drive the plot of Romeo and Juliet. The play suggests that the characters are subject to forces beyond their control, and that their attempts to navigate and manipulate these forces ultimately lead to tragedy. This is a common theme in Shakespeare's works, and is supported by general knowledge of the play as a whole. However, it is worth noting that a more detailed analysis of the play would require a broader range of texts and sources.\n",
      "\n",
      "In conclusion, based on the provided context, it is clear that Shakespeare portrays love and fate as complex and intertwined forces that drive the plot of Romeo and Juliet. The play suggests that the characters are subject to forces beyond their control, and that their attempts to navigate and manipulate these forces ultimately lead to tragedy. As the Chorus notes, \"Being held a foe, he may not have access / To breathe such vows as lovers use to swear\" (Document: romeo_and_juliet.txt), highlighting the impossible nature of the lovers' situation and the inevitability of their fate.\n",
      "\n",
      "================================================================================\n",
      "Query: What are the main ideas expressed in the Declaration of Independence?\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.cohere.com/v1/embed \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Needs Retrieval: True\n",
      "Reasoning: The query is about understanding the content of a specific historical document, which implies the need for detailed, accurate information that is typically found in external sources such as the document itself or scholarly analyses.\n",
      "Retrieved Documents: 5\n",
      "Reflection Score: 8.0/10\n",
      "Citations: ['books/declaration_of_independence_of_the_united_states.txt']\n",
      "\n",
      "Answer:\n",
      "The main ideas expressed in the Declaration of Independence are centered around the colonies' desire for independence from Great Britain and the establishment of the United States of America as a sovereign nation. \n",
      "\n",
      "As stated in the document \"declaration_of_independence_of_the_united_states.txt,\" the Declaration of Independence asserts that \"it becomes necessary for one people to dissolve the political bands which have connected them with another, and to assume, among the Powers of the earth, the separate and equal station to which the Laws of Nature and of Nature's God entitle them\" (Document: declaration_of_independence_of_the_united_states.txt). This quote highlights the primary motivation behind the Declaration: the colonies' need to break free from British rule and establish themselves as an independent entity.\n",
      "\n",
      "The document also emphasizes the colonies' repeated attempts to petition for redress and the British government's refusal to listen, which ultimately led to the decision to seek independence. As mentioned in \"declaration_of_independence_of_the_united_states.txt,\" \"In every stage of these Oppressions We have Petitioned for Redress in the most humble terms: Our repeated Petitions have been answered only by repeated injury\" (Document: declaration_of_independence_of_the_united_states.txt). This demonstrates the colonies' efforts to resolve their grievances through peaceful means before resorting to independence.\n",
      "\n",
      "Furthermore, the Declaration of Independence outlines the fundamental principles of equality, justice, and individual rights. It asserts that all men are created equal and are endowed with certain unalienable rights, such as life, liberty, and the pursuit of happiness (although this specific phrase is not present in the provided context, it is a well-known aspect of the Declaration of Independence, and I am relying on general knowledge to supplement the context).\n",
      "\n",
      "The document also formally declares the 13 colonies to be free and independent states, absolving them of all allegiance to the British Crown. As stated in \"declaration_of_independence_of_the_united_states.txt,\" \"We, therefore, the Representatives of the United States of America, in General Congress, Assembled, appealing to the Supreme Judge of the world for the rectitude of our intentions, do, in the Name, and by the Authority of the good People of these Colonies, solemnly publish and declare, That these United Colonies are, and of Right ought to be Free and Independent States\" (Document: declaration_of_independence_of_the_united_states.txt). This declaration marks the official establishment of the United States of America as a sovereign nation.\n",
      "\n",
      "In conclusion, the main ideas expressed in the Declaration of Independence, as evident from the provided context, are the colonies' desire for independence, their repeated attempts to petition for redress, and the formal declaration of the 13 colonies as free and independent states. These ideas are primarily based on the information present in \"declaration_of_independence_of_the_united_states.txt.\"\n",
      "\n",
      "================================================================================\n",
      "Query: How does the Declaration justify the colonies break from Britain?\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.cohere.com/v1/embed \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 502 Bad Gateway\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 0.437600 seconds\n",
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Needs Retrieval: True\n",
      "Reasoning: The query is specific to historical events and the content of a historical document, requiring detailed knowledge that is typically found in external sources or specialized knowledge bases.\n",
      "Retrieved Documents: 5\n",
      "Reflection Score: 9.0/10\n",
      "Citations: ['books/declaration_of_independence_of_the_united_states.txt']\n",
      "\n",
      "Answer:\n",
      "The Declaration of Independence justifies the colonies' break from Britain by citing the long history of abuses and usurpations by the British government, particularly by the King of Great Britain. According to the document \"declaration_of_independence_of_the_united_states.txt\" (multiple versions), the colonies have petitioned for redress in humble terms, but their repeated petitions have been answered only by repeated injury (Document: declaration_of_independence_of_the_united_states.txt). This suggests that the British government has consistently disregarded the colonies' rights and interests.\n",
      "\n",
      "The Declaration also appeals to the principles of natural law and the concept of consent of the governed, as stated in the document: \"We hold these truths to be self-evident, that all men are created equal, that they are endowed by their Creator with certain unalienable Rights, that among these are Life, Liberty, and the pursuit of Happiness\" (Document: declaration_of_independence_of_the_united_states.txt). This implies that the colonies have the right to alter or abolish their government if it becomes destructive of these ends.\n",
      "\n",
      "Furthermore, the Declaration lists specific grievances against the King of Great Britain, including his refusal to assent to laws necessary for the public good, his forbidding of governors to pass laws without his assent, and his neglect to attend to suspended laws (Document: declaration_of_independence_of_the_united_states.txt). These grievances demonstrate a pattern of behavior by the British government that is designed to establish an absolute tyranny over the colonies.\n",
      "\n",
      "The Declaration concludes that, given this history of abuses and usurpations, it is the right and duty of the colonies to throw off the British government and establish a new government based on the principles of liberty and justice (Document: declaration_of_independence_of_the_united_states.txt). This is stated explicitly in the document: \"That whenever any Form of Government becomes destructive of these ends, it is the Right of the People to alter or to abolish it, and to institute new Government, laying its foundation on such principles and organizing its powers in such form, as to them shall seem most likely to effect their Safety and Happiness.\"\n",
      "\n",
      "In summary, the Declaration of Independence justifies the colonies' break from Britain by citing the British government's long history of abuses and usurpations, appealing to the principles of natural law and consent of the governed, and listing specific grievances against the King of Great Britain. These justifications are based on the idea that the colonies have the right to alter or abolish their government if it becomes destructive of their rights and interests.\n",
      "\n",
      "================================================================================\n",
      "Query: How does Frankenstein's pursuit of knowledge ultimately lead to his downfall?\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.cohere.com/v1/embed \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Needs Retrieval: True\n",
      "Reasoning: The query \"How does Frankenstein's pursuit of knowledge ultimately lead to his downfall?\" is a literary analysis question that requires an understanding of the novel \"Frankenstein\" by Mary Shelley. To answer this question, one would need to have knowledge of the plot, characters, and themes in the novel. While the question does ask for specific information about the novel, it is not asking for factual data or domain-specific information that would require external documents or a knowledge base. The answer can be provided based on general knowledge of the novel, and the question does not require recent or specialized information. The analysis of the novel's plot and themes can be done without retrieving external information, as it is a well-known classic novel that has been widely studied and analyzed.\n",
      "Retrieved Documents: 5\n",
      "Reflection Score: 9.0/10\n",
      "Citations: ['books/frankenstein.txt']\n",
      "\n",
      "Answer:\n",
      "Based on the provided context documents, Frankenstein's pursuit of knowledge ultimately leads to his downfall through his creation of the monster and the subsequent events that unfold. \n",
      "\n",
      "In Document: frankenstein.txt, the monster states, \"I have devoted my creator, the select specimen of all that is worthy of love and admiration among men, to misery; I have pursued him even to that irremediable ruin.\" This suggests that Frankenstein's creation of the monster and his subsequent rejection of it lead to a series of events that ultimately result in his own downfall.\n",
      "\n",
      "The monster's existence and actions are a direct result of Frankenstein's pursuit of knowledge and his desire to create life. As the monster says in Document: frankenstein.txt, \"Remember, thou hast made me more powerful than thyself; my height is superior to thine, my joints more supple.\" This implies that Frankenstein's creation has become a force beyond his control, leading to devastating consequences.\n",
      "\n",
      "Furthermore, in Document: frankenstein.txt, the monster explains that it was Frankenstein's rejection and the subsequent loneliness and isolation that drove him to seek revenge. The monster states, \"Everywhere I see bliss, from which I alone am irrevocably excluded. I was benevolent and good; misery made me a fiend.\" This suggests that Frankenstein's pursuit of knowledge and his creation of the monster led to a chain of events that ultimately resulted in the monster's desire for revenge and Frankenstein's downfall.\n",
      "\n",
      "It is also worth noting that in Document: frankenstein.txt, Frankenstein himself acknowledges the dangers of unchecked ambition and the pursuit of knowledge. He warns Walton, \"Seek happiness in tranquillity and avoid ambition, even if it be only the apparently innocent one of distinguishing yourself in science and discoveries.\" This warning suggests that Frankenstein has come to realize the dangers of his own pursuit of knowledge and the devastating consequences that it has led to.\n",
      "\n",
      "In conclusion, based on the provided context documents, Frankenstein's pursuit of knowledge ultimately leads to his downfall through his creation of the monster and the subsequent events that unfold. His rejection of the monster and the monster's subsequent desire for revenge lead to a chain of events that result in Frankenstein's downfall. \n",
      "\n",
      "It is worth noting that while the context documents provide significant insight into the events that lead to Frankenstein's downfall, they do not provide a detailed explanation of the scientific knowledge that Frankenstein pursued or the exact nature of his experiments. To fully understand the scientific aspects of Frankenstein's pursuit of knowledge, additional context or general knowledge of the novel would be necessary.\n",
      "\n",
      "================================================================================\n",
      "Query: In what ways does the creature demonstrate human qualities, and how is he rejected by the society?\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.cohere.com/v1/embed \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Needs Retrieval: True\n",
      "Reasoning: The query asks about the creature demonstrating human qualities and being rejected by society, which suggests it is referring to a specific character or entity, likely from a literary work such as Frankenstein. To answer this question, one would need to have knowledge of the character's actions, behaviors, and interactions within the story. This requires specific, domain-specific information about the literary work, which may not be general knowledge. The answer would likely benefit from external documents or a knowledge base, such as a summary or analysis of the literary work, to provide a detailed and accurate response. Additionally, the query does not ask for recent or specialized information, but rather an analysis of a character's traits and societal interactions, which is typically found in literary works.\n",
      "Retrieved Documents: 5\n",
      "Reflection Score: 9.0/10\n",
      "Citations: ['books/frankenstein.txt']\n",
      "\n",
      "Answer:\n",
      "The creature in the provided context demonstrates human qualities in several ways, despite being rejected by society. \n",
      "\n",
      "Firstly, the creature exhibits a desire for companionship and understanding, as evident in his request to his creator to make a female companion for him (Document: frankenstein.txt, Chapter 17). He expresses a need for \"the interchange of those sympathies necessary for my being,\" showcasing his capacity for emotional connection and intimacy. This desire for human connection is a fundamental aspect of human nature, highlighting the creature's inherent humanity.\n",
      "\n",
      "Secondly, the creature displays a range of emotions, including sorrow, anger, and a deep sense of rejection. In Document: frankenstein.txt, he laments his existence, stating, \"I was, besides, endued with a figure hideously deformed and loathsome; I was not even of the same nature as man.\" This emotional response demonstrates his self-awareness and capacity for complex emotions, further emphasizing his human-like qualities.\n",
      "\n",
      "Thirdly, the creature showcases his intellectual capabilities through his ability to learn, reason, and communicate effectively. In Document: frankenstein.txt, he engages in a thoughtful and articulate conversation with his creator, presenting a well-structured argument for why he deserves a companion. This intellectual capacity is a distinctive human trait, underscoring the creature's similarity to humans.\n",
      "\n",
      "However, despite these human-like qualities, the creature is rejected by society due to his physical appearance and perceived monstrous nature. His creator, in particular, is horrified by his appearance, describing him as a \"filthy mass that moved and talked\" (Document: frankenstein.txt). This rejection is further exacerbated by the creature's experiences with the cottagers, who flee from him in terror upon seeing his deformed figure (implied in Document: frankenstein.txt, though not directly stated in the provided context).\n",
      "\n",
      "The creature's rejection by society is also rooted in his \"otherness,\" as he is not considered to be of the same nature as humans. In Document: frankenstein.txt, he acknowledges this, stating, \"I was not even of the same nature as man.\" This perceived difference leads to his isolation and exclusion from human society, highlighting the themes of prejudice, rejection, and the consequences of playing God.\n",
      "\n",
      "In conclusion, the creature demonstrates human qualities through his emotional, intellectual, and social capacities, despite being rejected by society due to his physical appearance and perceived monstrous nature. The provided context highlights the complexities of human nature and the consequences of rejection, prejudice, and the blurring of lines between creator and creation.\n",
      "\n",
      "================================================================================\n",
      "Query: In what ways does Elizabeth Benner challenge the gender norms of her society?\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "async def main():\n",
    "    rag_system = RAGSystem(\n",
    "    cohere_api_key=os.getenv(\"COHERE_API_KEY\"),\n",
    "    openrouter_api_key=os.getenv(\"OPENROUTER_API_KEY\"),\n",
    "    kb_folder=\"books\"\n",
    "    )\n",
    "    \n",
    "    # Initialize the system\n",
    "    await rag_system.initialize()\n",
    "    \n",
    "    \n",
    "    queries = [\n",
    "    \"Who is Chris Olande?\",\n",
    "    \"What has Chris Olande studied at Kenyatta University?\",\n",
    "    \n",
    "    # Romeo and Juliet\n",
    "    \"What is the central conflict in Romeo and Juliet?\",\n",
    "    \"How does Shakespeare portray love and fate in Romeo and Juliet?\",\n",
    "\n",
    "    # Declaration of Independence\n",
    "    \"What are the main ideas expressed in the Declaration of Independence?\",\n",
    "    \"How does the Declaration justify the colonies break from Britain?\",\n",
    "\n",
    "    # Frankenstein\n",
    "    \"How does Frankenstein's pursuit of knowledge ultimately lead to his downfall?\",\n",
    "    \"In what ways does the creature demonstrate human qualities, and how is he rejected by the society?\",\n",
    "\n",
    "    # Pride and Prejudice\n",
    "    \"In what ways does Elizabeth Benner challenge the gender norms of her society?\",\n",
    "    \"How does Mr. Darcy's character evolve, and what causes this change?\",\n",
    "\n",
    "    # Scarlett Letter\n",
    "    \"How does the forest contrast with the Puritan settlement, and wha does this signify?\",\n",
    "    \"In what ways does Dimmesdale suffer due to his hidden sin?, Explain what his hidden sin was in the first place\",\n",
    "\n",
    "    # U.S Bill of rights\n",
    "    \"What is the difference between the sixth and seventh ammendments?\",\n",
    "    \"How does the eighth ammendment protections relate to modern debates on criminal justice?\",\n",
    "    \"Which rights are guaranteed by the first ammendment?\"\n",
    "]\n",
    "    \n",
    "    # Process queries individually\n",
    "    for query in queries:\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"Query: {query}\")\n",
    "        print('='*80)\n",
    "        \n",
    "        try:\n",
    "            response = await rag_system.query(query)\n",
    "            \n",
    "            print(f\"Needs Retrieval: {response.needs_retrieval}\")\n",
    "            print(f\"Reasoning: {response.retrieval_decision_reasoning}\")\n",
    "            print(f\"Retrieved Documents: {len(response.retrieved_docs)}\")\n",
    "            print(f\"Reflection Score: {response.reflection_score}/10\")\n",
    "            \n",
    "            if response.citations:\n",
    "                print(f\"Citations: {response.citations}\")\n",
    "            \n",
    "            print(f\"\\nAnswer:\\n{response.answer}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing query: {e}\")\n",
    "    \n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"BATCH PROCESSING EXAMPLE\")\n",
    "    print('='*60)\n",
    "    \n",
    "    batch_processor = BatchProcessor(rag_system, max_concurrent=3)\n",
    "    batch_queries = queries[:4]  # Process first 4 queries in batch\n",
    "    \n",
    "    batch_results = await batch_processor.process_batch(batch_queries)\n",
    "    \n",
    "    for query, response in batch_results.items():\n",
    "        print(f\"\\nBatch Query: {query}\")\n",
    "        print(f\"Reflection Score: {response.reflection_score}/10\")\n",
    "        print(f\"Answer Length: {len(response.answer)} characters\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    await main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "118052ee",
   "metadata": {
    "cellUniqueIdByVincent": "6d452"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  },
  "vincent": {
   "sessionId": "3f2153659d5b596a4189cf1d_2025-05-30T15-07-59-793Z"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
