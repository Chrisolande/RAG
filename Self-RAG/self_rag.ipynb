{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellUniqueIdByVincent": "db184"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import asyncio\n",
    "import logging \n",
    "from pathlib import Path\n",
    "from typing import Tuple, Any, List, Dict, Optional\n",
    "from dataclasses import dataclass\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from langchain_cohere import CohereEmbeddings\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.document_loaders import (\n",
    "    TextLoader, \n",
    "    UnstructuredMarkdownLoader,\n",
    "    JSONLoader,\n",
    "    UnstructuredHTMLLoader,\n",
    "    PyPDFLoader\n",
    ")\n",
    "\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.schema import Document\n",
    "from langchain.prompts import ChatPromptTemplate, PromptTemplate\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "from langchain_core.runnables import RunnableParallel, RunnableLambda\n",
    "from langchain.callbacks.base import AsyncCallbackHandler\n",
    "from langchain.schema import LLMResult\n",
    "\n",
    "load_dotenv()\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cellUniqueIdByVincent": "dbac3"
   },
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class SelfRAGResponse:\n",
    "    \"\"\"Complete self rag with reflection\"\"\"\n",
    "    answer: str\n",
    "    retrieved_docs: List[Document]\n",
    "    reflection_score: float\n",
    "    needs_retrieval: bool\n",
    "    citations: List[str]\n",
    "    retrieval_decision_reasoning: str\n",
    "\n",
    "\n",
    "class RateLimitCallback(AsyncCallbackHandler):\n",
    "    \"\"\"Callback handler to manage API rate limiting with semaphores\"\"\"\n",
    "    \n",
    "    def __init__(self, semaphore: asyncio.Semaphore):\n",
    "        self.semaphore = semaphore\n",
    "        \n",
    "    async def on_llm_start(self, serialized: Dict[str, Any], prompts: List[str], **kwargs: Any) -> None:\n",
    "        await self.semaphore.acquire()\n",
    "        \n",
    "    async def on_llm_end(self, response: LLMResult, **kwargs: Any) -> None:\n",
    "        self.semaphore.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cellUniqueIdByVincent": "a00b3"
   },
   "outputs": [],
   "source": [
    "class DocumentLoader:\n",
    "    def __init__(self):\n",
    "        self.loaders = {\n",
    "            '.txt': TextLoader,\n",
    "            '.md': UnstructuredMarkdownLoader,\n",
    "            '.json': self._create_json_loader,\n",
    "            '.html': UnstructuredHTMLLoader,\n",
    "            '.py': TextLoader,\n",
    "            '.js': TextLoader,\n",
    "            '.css': TextLoader,\n",
    "            '.pdf': PyPDFLoader\n",
    "        }\n",
    "\n",
    "    def _create_json_loader(self, file_path: str):\n",
    "        \"\"\"Create JSON loader with custom jq_schema\"\"\"\n",
    "        return JSONLoader(\n",
    "            file_path=file_path,\n",
    "            jq_schema='.[]',\n",
    "            text_content=False\n",
    "        )\n",
    "\n",
    "    async def load_documents(self, kb_folder: str) -> List[Document]:\n",
    "        \"\"\"Load all documents from the knowledge base folder\"\"\"\n",
    "        documents = []\n",
    "        kb_path = Path(kb_folder)\n",
    "\n",
    "        if not kb_path.exists():\n",
    "            raise FileNotFoundError(f\"Knowledge base folder not found: {kb_path}\")\n",
    "\n",
    "        for file_path in kb_path.glob(\"**/*\"):\n",
    "            if file_path.is_file() and file_path.suffix.lower() in self.loaders:\n",
    "                try:\n",
    "                    loader_class = self.loaders[file_path.suffix.lower()]\n",
    "\n",
    "                    if file_path.suffix.lower() == \".json\":\n",
    "                        loader = loader_class(str(file_path))\n",
    "                    else:\n",
    "                        loader = loader_class(str(file_path))\n",
    "\n",
    "                    docs = loader.load()\n",
    "\n",
    "                    # Add metadata\n",
    "                    for doc in docs:\n",
    "                        doc.metadata.update({\n",
    "                            'file_path': str(file_path),\n",
    "                            'file_type': file_path.suffix,\n",
    "                            'file_name': file_path.name\n",
    "                        })\n",
    "\n",
    "                    documents.extend(docs)\n",
    "                \n",
    "                except Exception as e:\n",
    "                    logger.warning(f\"There was an error loading the knowledge base: {str(e)}\")\n",
    "                    # Fallback to TextLoader for unknown formats\n",
    "                    try:\n",
    "                        loader = TextLoader(str(file_path))\n",
    "                        docs = loader.load()\n",
    "                        # Add metadata\n",
    "                        for doc in docs:\n",
    "                            doc.metadata.update({\n",
    "                                'file_path': str(file_path),\n",
    "                                'file_type': file_path.suffix,\n",
    "                                'file_name': file_path.name\n",
    "                            })\n",
    "\n",
    "                        documents.extend(docs)\n",
    "\n",
    "                    except Exception as fallback_error:\n",
    "                        logger.error(f\"Failed to load {file_path} with fallback: {fallback_error}\")\n",
    "        \n",
    "        logger.info(f\"Loaded {len(documents)} documents from {kb_folder}\")\n",
    "        return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "cellUniqueIdByVincent": "fb77a"
   },
   "outputs": [],
   "source": [
    "from asyncio import Semaphore\n",
    "@dataclass\n",
    "class RAGSystem:\n",
    "    cohere_api_key: str\n",
    "    openrouter_api_key: str\n",
    "    kb_folder: str\n",
    "    max_concurrent_requests: int = 5\n",
    "    chunk_size: int = 2000\n",
    "    chunk_overlap: int = 200\n",
    "\n",
    "    def __post_init__(self):\n",
    "        # Initialize the components\n",
    "        self.embeddings = CohereEmbeddings(model = \"embed-v4.0\",\n",
    "                                         cohere_api_key = os.getenv(\"COHERE_API_KEY\"))\n",
    "\n",
    "        self.llm = ChatOpenAI(\n",
    "            model=\"meta-llama/llama-3.3-70b-instruct\",\n",
    "            openai_api_key=self.openrouter_api_key,\n",
    "            openai_api_base=\"https://openrouter.ai/api/v1\",\n",
    "            temperature=0.6,\n",
    "            max_tokens=1500\n",
    "        )\n",
    "\n",
    "        # Text Splitter\n",
    "        self.text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size = self.chunk_size,\n",
    "            chunk_overlap = self.chunk_overlap,\n",
    "            length_function = len\n",
    "        )\n",
    "\n",
    "        self.document_loader = DocumentLoader()\n",
    "\n",
    "        # Vector store\n",
    "        self.vector_store: Optional[FAISS] = None\n",
    "        #self.kb_folder = kb_folder\n",
    "\n",
    "        # Semaphores for rate limiting\n",
    "        self.llm_semaphore = Semaphore(self.max_concurrent_requests)\n",
    "        self.embeddings_semaphore = Semaphore(self.max_concurrent_requests)\n",
    "\n",
    "        self.rate_limit_callback = RateLimitCallback(self.llm_semaphore)\n",
    "\n",
    "        self.is_initialized = False\n",
    "\n",
    "        # Set up prompts\n",
    "        self._setup_prompts()\n",
    "\n",
    "    def _setup_prompts(self):\n",
    "        \"\"\"Set up prompts for different stages\"\"\"\n",
    "\n",
    "        # Retrieval decision prompt\n",
    "        self.retrieval_decision_prompt = PromptTemplate(\n",
    "            input_variables=[\"query\"],\n",
    "            template=\"\"\"\n",
    "            Analyze the following query to determine if it requires external knowledge retrieval.\n",
    "            \n",
    "            Query: \"{query}\"\n",
    "            \n",
    "            Consider:\n",
    "            1. Does this query ask for specific facts, data, or domain-specific information?\n",
    "            2. Would the answer benefit from external documents or knowledge base?\n",
    "            3. Is this asking about general knowledge that can be answered without retrieval?\n",
    "            4. Does it require recent or specialized information?\n",
    "            \n",
    "            Provide your reasoning and then answer with either \"RETRIEVE\" or \"NO_RETRIEVE\".\n",
    "            \n",
    "            Reasoning: [Explain your decision]\n",
    "            Decision: [RETRIEVE or NO_RETRIEVE]\n",
    "            \"\"\"\n",
    "        )\n",
    "\n",
    "        # Answer generation with retrieval prompt\n",
    "        self.rag_prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "            You are a helpful AI assistant. Use the following context documents to answer the user's question accurately and comprehensively.\n",
    "            \n",
    "            Context Documents:\n",
    "            {context}\n",
    "            \n",
    "            Question: {question}\n",
    "            \n",
    "            Instructions:\n",
    "            - Base your answer primarily on the provided context\n",
    "            - If the context doesn't contain sufficient information, acknowledge this\n",
    "            - Cite specific documents when referencing information\n",
    "            - Be accurate, detailed, and helpful\n",
    "            - If you need to use general knowledge to supplement the context, clearly indicate this\n",
    "            \n",
    "            Answer:\n",
    "        \"\"\")\n",
    "\n",
    "        # Answer generation without retrieval prompt\n",
    "        self.no_retrieval_prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "            You are a helpful AI assistant. Answer the following question using your general knowledge.\n",
    "            \n",
    "            Question: {question}\n",
    "            \n",
    "            Provide a comprehensive and accurate answer based on your training knowledge.\n",
    "            \n",
    "            Answer:\n",
    "        \"\"\")\n",
    "\n",
    "        # Reflection prompt\n",
    "        self.reflection_prompt = PromptTemplate(\n",
    "            input_variables=[\"query\", \"answer\", \"context\"],\n",
    "            template=\"\"\"\n",
    "            Evaluate the quality of the following answer based on the query and available context.\n",
    "            \n",
    "            Query: {query}\n",
    "            \n",
    "            Context:\n",
    "            {context}\n",
    "            \n",
    "            Answer: {answer}\n",
    "            \n",
    "            Rate the answer on a scale of 0-10 considering:\n",
    "            - Accuracy and factual correctness\n",
    "            - Completeness and comprehensiveness\n",
    "            - Relevance to the query\n",
    "            - Proper use of available context\n",
    "            - Clarity and helpfulness\n",
    "            \n",
    "            Provide only a single number between 0 and 10 as your rating.\n",
    "            \n",
    "            Rating:\n",
    "            \"\"\"\n",
    "        )\n",
    "\n",
    "    async def initialize(self):\n",
    "        \"\"\"Initialize the RAG System \"\"\"\n",
    "        if self.is_initialized:\n",
    "            return\n",
    "        \n",
    "        logger.info(\"Initializing the RAG system ...\")\n",
    "\n",
    "        # Load the document\n",
    "        documents = await self.document_loader.load_documents(self.kb_folder)\n",
    "\n",
    "        if not documents:\n",
    "            logger.warning(\"No documents were found in the knowledge base ...\")\n",
    "            self.vector_store = None\n",
    "            self.is_initialized = True\n",
    "            return\n",
    "\n",
    "        # Split the documents into chunks\n",
    "        split_docs = self.text_splitter.split_documents(documents)\n",
    "        logger.info(f\"Split {len(documents)} documents into {len(split_docs)} chunks\")\n",
    "\n",
    "        # Create the vector store\n",
    "        self.vector_store = await FAISS.afrom_documents(\n",
    "            split_docs,\n",
    "            self.embeddings\n",
    "        )\n",
    "\n",
    "        self.is_initialized = True\n",
    "        logger.info(\"Self-RAG system initialized successfully\")\n",
    "\n",
    "    async def _should_retrieve(self, query: str) -> Tuple[bool, str]:\n",
    "        \"\"\" Determine if the retrieval is needed and get reasoning\"\"\"\n",
    "        chain = self.retrieval_decision_prompt | self.llm.with_config(callbacks=[self.rate_limit_callback])\n",
    "\n",
    "        result = await chain.ainvoke({\"query\": query})\n",
    "\n",
    "        # Parse the result\n",
    "        lines = result.content.strip().split('\\n')\n",
    "        reasoning = \"\"\n",
    "        decision = False\n",
    "\n",
    "        for line in lines:\n",
    "            if line.startswith(\"Reasoning:\"):\n",
    "                reasoning = line.replace(\"Reasoning:\", \"\").strip()\n",
    "            elif line.startswith(\"Decision:\"):\n",
    "                decision_text = line.replace(\"Decision:\", \"\").strip()\n",
    "                decision = \"RETRIEVE\" in decision_text.upper()\n",
    "        \n",
    "        return decision, reasoning\n",
    "\n",
    "    async def _retrieve_documents(self, query: str, k:int = 5) -> List[Document]:\n",
    "        \"\"\"Retrieve relevant documents\"\"\"\n",
    "        if not self.vector_store:\n",
    "            return []\n",
    "\n",
    "        # Use similarity search with scores\n",
    "        doc_with_scores = await self.vector_store.asimilarity_search_with_score(query, k = k)\n",
    "        # Filter by relevance score\n",
    "        relevant_docs = [doc for doc, score in doc_with_scores if score > 0.8]\n",
    "\n",
    "        return relevant_docs\n",
    "\n",
    "    async def _generate_answer_with_retrieval(self, query: str, documents: Document) -> str:\n",
    "        \"\"\"Generate answers using retrieved documents\"\"\"\n",
    "        context = \"\\n\\n\".join([\n",
    "            f\"Document: {doc.metadata.get('file_name', 'Unknown')}\\n{doc.page_content}\"\n",
    "            for doc in documents\n",
    "        ])\n",
    "        \n",
    "        chain = self.rag_prompt | self.llm | StrOutputParser()\n",
    "        \n",
    "        result = await chain.ainvoke({\n",
    "            \"context\": context,\n",
    "            \"question\": query\n",
    "        })\n",
    "        \n",
    "        return result\n",
    "\n",
    "    async def _generate_answer_without_retrieval(self, query: str) -> str:\n",
    "        \"\"\"Generate answer without retrieval\"\"\"\n",
    "        chain = self.no_retrieval_prompt | self.llm | StrOutputParser()\n",
    "        \n",
    "        result = await chain.ainvoke({\"question\": query})\n",
    "        return result\n",
    "\n",
    "    async def _reflect_on_answer(self, query: str, answer: str, documents: List[Document]) -> float:\n",
    "        \"\"\"Reflect on answer quality\"\"\"\n",
    "        context = \"\\n\\n\".join([doc.page_content for doc in documents]) if documents else \"No context provided\"\n",
    "        \n",
    "        chain = self.reflection_prompt | self.llm.with_config(callbacks=[self.rate_limit_callback])\n",
    "        \n",
    "        result = await chain.ainvoke({\n",
    "            \"query\": query,\n",
    "            \"answer\": answer,\n",
    "            \"context\": context\n",
    "        })\n",
    "        \n",
    "        try:\n",
    "            \n",
    "            content = result.content if hasattr(result, 'content') else str(result)\n",
    "            score_str = content.strip().split('\\n')[-1]\n",
    "            \n",
    "            # Extract digits and decimal point from the score string\n",
    "            score_chars = ''.join(c for c in score_str if c.isdigit() or c == '.')\n",
    "            \n",
    "            if score_chars:\n",
    "                score = float(score_chars)\n",
    "                return min(10.0, max(0.0, score))  # Clamp between 0 and 10\n",
    "            else:\n",
    "                logger.warning(f\"No numeric score found in: {score_str}\")\n",
    "                return 5.0\n",
    "                \n",
    "        except (ValueError, IndexError) as e:\n",
    "            logger.warning(f\"Could not parse reflection score: {result}. Error: {e}\")\n",
    "            return 5.0\n",
    "\n",
    "    async def query(self, query: str, force_retrieval: bool = False) -> SelfRAGResponse:\n",
    "        \"\"\"Process query using the SelfRAG\"\"\"\n",
    "        if not self.is_initialized:\n",
    "            await self.initialize()\n",
    "\n",
    "        # Decide whether we need retrieval\n",
    "        if force_retrieval:\n",
    "            needs_retrieval = True\n",
    "            reasoning = \"Forced retrieval requested\"\n",
    "        else:\n",
    "            needs_retrieval, reasoning = await self._should_retrieve(query)\n",
    "\n",
    "        retrieved_docs = []\n",
    "        citations = []\n",
    "\n",
    "        # Retrieve documents if needed\n",
    "        if needs_retrieval and self.vector_store:\n",
    "            retrieved_docs = await self._retrieve_documents(query)\n",
    "            citations = [\n",
    "                doc.metadata.get('file_path', f\"Document {i}\")\n",
    "                for i, doc in enumerate(retrieved_docs)\n",
    "            ]\n",
    "\n",
    "        # Generate answer\n",
    "        if retrieved_docs:\n",
    "            answer = await self._generate_answer_with_retrieval(query, retrieved_docs)\n",
    "        else:\n",
    "            answer = await self._generate_answer_without_retrieval(query)\n",
    "\n",
    "        # Reflect on answer quality\n",
    "        reflection_score = await self._reflect_on_answer(query, answer, retrieved_docs)\n",
    "\n",
    "        return SelfRAGResponse(\n",
    "            answer=answer,\n",
    "            retrieved_docs=retrieved_docs,\n",
    "            reflection_score=reflection_score,\n",
    "            needs_retrieval=needs_retrieval,\n",
    "            citations=list(set(citations)),  # Remove duplicates\n",
    "            retrieval_decision_reasoning=reasoning\n",
    "        )\n",
    "\n",
    "    async def save_vector_store(self, path: str):\n",
    "        \"\"\"Save vector store to disk\"\"\"\n",
    "        if self.vector_store:\n",
    "            self.vector_store.save_local(path)\n",
    "            logger.info(f\"Vector store saved to {path}\")\n",
    "    \n",
    "    async def load_vector_store(self, path: str):\n",
    "        \"\"\"Load vector store from disk\"\"\"\n",
    "        try:\n",
    "            self.vector_store = FAISS.load_local(path, self.embeddings)\n",
    "            self.is_initialized = True\n",
    "            logger.info(f\"Vector store loaded from {path}\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to load vector store: {e}\")\n",
    "            await self.initialize()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e84635bd",
   "metadata": {
    "cellUniqueIdByVincent": "6ebcf"
   },
   "outputs": [],
   "source": [
    "class BatchProcessor:\n",
    "    \"\"\"Process multiple queries in batch with concurrency control\"\"\"\n",
    "    \n",
    "    def __init__(self, rag_system: RAGSystem, max_concurrent: int = 5):\n",
    "        self.rag_system = rag_system\n",
    "        self.semaphore = asyncio.Semaphore(max_concurrent)\n",
    "    \n",
    "    async def process_query(self, query: str) -> Tuple[str, SelfRAGResponse]:\n",
    "        \"\"\"Process a single query with semaphore\"\"\"\n",
    "        async with self.semaphore:\n",
    "            response = await self.rag_system.query(query)\n",
    "            return query, response\n",
    "    \n",
    "    async def process_batch(self, queries: List[str]) -> Dict[str, SelfRAGResponse]:\n",
    "        \"\"\"Process multiple queries concurrently\"\"\"\n",
    "        tasks = [self.process_query(query) for query in queries]\n",
    "        results = await asyncio.gather(*tasks, return_exceptions=True)\n",
    "        \n",
    "        output = {}\n",
    "        for result in results:\n",
    "            if isinstance(result, Exception):\n",
    "                logger.error(f\"Error processing query: {result}\")\n",
    "            else:\n",
    "                query, response = result\n",
    "                output[query] = response\n",
    "        \n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a599a2c5",
   "metadata": {
    "cellUniqueIdByVincent": "55e97"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Initializing the RAG system ...\n",
      "INFO:__main__:Loaded 3 documents from books\n",
      "INFO:__main__:Split 3 documents into 170 chunks\n",
      "INFO:httpx:HTTP Request: POST https://api.cohere.com/v1/embed \"HTTP/1.1 200 OK\"\n",
      "WARNING:langchain_cohere.utils:Retrying langchain_cohere.embeddings.CohereEmbeddings.aembed_with_retry.<locals>._embed_with_retry in 4.0 seconds as it raised ConnectError: All connection attempts failed.\n",
      "WARNING:langchain_cohere.utils:Retrying langchain_cohere.embeddings.CohereEmbeddings.aembed_with_retry.<locals>._embed_with_retry in 4.0 seconds as it raised ReadError: .\n",
      "INFO:httpx:HTTP Request: POST https://api.cohere.com/v1/embed \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.cohere.com/v1/embed \"HTTP/1.1 200 OK\"\n",
      "INFO:faiss.loader:Loading faiss.\n",
      "INFO:faiss.loader:Successfully loaded faiss.\n",
      "INFO:faiss:Failed to load GPU Faiss: name 'GpuIndexIVFFlat' is not defined. Will not load constructor refs for GPU indexes. This is only an error if you're trying to use GPU Faiss.\n",
      "INFO:__main__:Self-RAG system initialized successfully\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Query: Who is Chris Olande?\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:langchain_cohere.utils:Retrying langchain_cohere.embeddings.CohereEmbeddings.aembed_with_retry.<locals>._embed_with_retry in 4.0 seconds as it raised ConnectError: .\n",
      "INFO:httpx:HTTP Request: POST https://api.cohere.com/v1/embed \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Needs Retrieval: True\n",
      "Reasoning: The query \"Who is Chris Olande?\" asks for specific information about an individual, which suggests it is seeking factual data. This type of information would typically be found in external documents, databases, or knowledge bases, as it pertains to a specific person who may not be widely known or may have information that is not readily available without some form of retrieval or lookup. The query does not seem to be asking for general knowledge that can be answered without retrieval, as the name \"Chris Olande\" is not a commonly recognized figure in broad, everyday conversations. It requires specialized or specific information that would likely need to be retrieved from an external source to provide an accurate answer.\n",
      "Retrieved Documents: 5\n",
      "Reflection Score: 9.0/10\n",
      "Citations: ['books/declaration_of_independence_of_the_united_states.txt', 'books/olande.txt']\n",
      "\n",
      "Answer:\n",
      "Chris Olande is a dynamic and intellectually curious student of Statistics and Programming at Kenyatta University, as described in the document \"olande.txt\". According to this document, he has a growing portfolio of sophisticated projects that blend statistical rigor with cutting-edge machine learning techniques, demonstrating a mastery of foundational principles in data science and programming.\n",
      "\n",
      "As noted in \"olande.txt\", Chris's academic background and technical expertise include a robust understanding of programming, especially in Python, with proficiency in libraries such as NumPy, pandas, matplotlib, seaborn, scikit-learn, PyTorch, and Hugging Face Transformers. He has also undertaken projects that demonstrate applied knowledge of exploratory factor analysis (EFA), hypothesis testing, regression modeling, and time series analysis.\n",
      "\n",
      "The document \"olande.txt\" further highlights Chris's leadership in education, where he has organized a field study for Grade 7 students to visit Impala Glass Industries in Nairobi, showcasing his commitment to nurturing curiosity and STEM literacy among younger learners. His work in this domain underscores a broader commitment to education as a vehicle for empowerment and societal progress.\n",
      "\n",
      "In terms of his professional traits and work ethic, \"olande.txt\" describes Chris as methodical, results-oriented, and deeply inquisitive, with a willingness to experiment with new frameworks, keep up with industry trends, and learn from feedback. His code is clean, well-documented, and adheres to best practices in software development.\n",
      "\n",
      "The document \"olande.txt\" also outlines Chris's career aspirations and potential, suggesting that he is well-positioned for roles in data science, machine learning engineering, research, or AI systems development, with a current focus on agentic RAGs that could lead to a future in intelligent information systems and AI product development.\n",
      "\n",
      "It is worth noting that the document \"declaration_of_independence_of_the_united_states.txt\" does not provide any information about Chris Olande, as it appears to be a historical text unrelated to his profile.\n",
      "\n",
      "In summary, based on the information provided in \"olande.txt\", Chris Olande is a promising young professional with a strong foundation in statistics and programming, a passion for innovative applications in real-world contexts, and a commitment to education and community engagement.\n",
      "\n",
      "============================================================\n",
      "Query: What has Chris Olande studied at Kenyatta University?\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:langchain_cohere.utils:Retrying langchain_cohere.embeddings.CohereEmbeddings.aembed_with_retry.<locals>._embed_with_retry in 4.0 seconds as it raised ConnectError: All connection attempts failed.\n",
      "INFO:httpx:HTTP Request: POST https://api.cohere.com/v1/embed \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Needs Retrieval: True\n",
      "Reasoning: This query asks for specific facts about Chris Olande's studies at Kenyatta University, which suggests it requires domain-specific information. The answer to this question would likely benefit from external documents or a knowledge base, as it is not general knowledge that can be answered without retrieval. Additionally, the query requires specific information about an individual's academic background, which may not be readily available without accessing external sources. The information needed is also specialized, as it pertains to a particular person and institution.\n",
      "Retrieved Documents: 5\n",
      "Reflection Score: 9.0/10\n",
      "Citations: ['books/declaration_of_independence_of_the_united_states.txt', 'books/olande.txt']\n",
      "\n",
      "Answer:\n",
      "According to the provided context, specifically in the document \"olande.txt\", Chris Olande has studied Statistics and Programming at Kenyatta University. This is mentioned in the introduction of the document, which describes him as \"a dynamic and intellectually curious student of Statistics and Programming at Kenyatta University\" (olande.txt). \n",
      "\n",
      "Additionally, the document highlights his academic foundation in statistics, complemented by a robust understanding of programming, especially in Python, and his proficiency in various libraries such as NumPy, pandas, matplotlib, seaborn, scikit-learn, PyTorch, and Hugging Face Transformers (olande.txt). This suggests that his studies at Kenyatta University have equipped him with a strong foundation in both statistical analysis and programming skills.\n",
      "\n",
      "============================================================\n",
      "Query: What is the central conflict in Romeo and Juliet?\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:langchain_cohere.utils:Retrying langchain_cohere.embeddings.CohereEmbeddings.aembed_with_retry.<locals>._embed_with_retry in 4.0 seconds as it raised ConnectError: All connection attempts failed.\n",
      "INFO:httpx:HTTP Request: POST https://api.cohere.com/v1/embed \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Needs Retrieval: True\n",
      "Reasoning: The query is specific to literary analysis, benefits from referencing external sources for accuracy, and while not requiring recent information, it does involve specialized knowledge about a classic work of literature.\n",
      "Retrieved Documents: 5\n",
      "Reflection Score: 9.0/10\n",
      "Citations: ['books/romeo_and_juliet.txt']\n",
      "\n",
      "Answer:\n",
      "The central conflict in Romeo and Juliet is the long-standing feud between the two prominent families of Verona, the Montagues and the Capulets, which ultimately leads to the tragic demise of the two young lovers, Romeo and Juliet. This conflict is evident throughout the provided context, particularly in the opening scenes of the play.\n",
      "\n",
      "As stated in the prologue of the play (Document: romeo_and_juliet.txt), \"Two households, both alike in dignity, / In fair Verona, where we lay our scene, / From ancient grudge break to new mutiny, / Where civil blood makes civil hands unclean.\" This sets the tone for the rest of the play, highlighting the deep-seated hatred between the two families.\n",
      "\n",
      "The conflict is further emphasized in Act I, Scene I, where Sampson and Gregory, servants of the Capulets, discuss their hatred for the Montagues (Document: romeo_and_juliet.txt). Sampson says, \"Gregory, on my word, weâ€™ll not carry coals,\" implying that they will not back down from a fight with the Montagues.\n",
      "\n",
      "The feud between the two families also leads to the tragic events that unfold, including the banishment of Romeo and the eventual death of Tybalt, Juliet's cousin (Document: romeo_and_juliet.txt). Juliet's nurse lamenting the death of Tybalt and the banishment of Romeo, saying \"Tybalt is gone, and Romeo banished, / Romeo that kill'd him, he is banished\" (Document: romeo_and_juliet.txt).\n",
      "\n",
      "Furthermore, the conflict between the two families is so intense that it even affects the way the characters perceive each other. For example, when Juliet learns that Romeo is a Montague, she exclaims, \"My only love sprung from my only hate! / Too early seen unknown, and known too late! / Prodigious birth of love it is to me, / That I must love a loathed enemy\" (Document: romeo_and_juliet.txt).\n",
      "\n",
      "In conclusion, based on the provided context, the central conflict in Romeo and Juliet is the long-standing feud between the Montagues and the Capulets, which ultimately leads to the tragic demise of the two young lovers. This conflict is evident throughout the play and is a major driving force behind the events that unfold. \n",
      "\n",
      "Note: The analysis is based primarily on the provided context, with no additional information from general knowledge required to answer the question. The answer cites specific documents to support the analysis, providing a detailed and accurate explanation of the central conflict in Romeo and Juliet.\n",
      "\n",
      "============================================================\n",
      "Query: How does Shakespeare portray love and fate in Romeo and Juliet?\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.cohere.com/v1/embed \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Needs Retrieval: True\n",
      "Reasoning: This query requires an analysis of Shakespeare's portrayal of love and fate in Romeo and Juliet, which is a literary work. To answer this question, one would need to have knowledge of the play, its characters, plot, and themes. The query asks for an interpretation of the author's intent and the ways in which he explores these themes, rather than specific facts or data. While general knowledge of the play is necessary, a more detailed and nuanced understanding would benefit from external documents or a knowledge base, such as literary critiques, analyses, or scholarly articles. The answer would likely require a deeper understanding of the play's context, characters, and literary devices, which may not be readily available without referencing external sources. Additionally, the query does not require recent information, as the play is a classic work of literature, but it does require specialized knowledge of literary analysis.\n",
      "Retrieved Documents: 5\n",
      "Reflection Score: 9.0/10\n",
      "Citations: ['books/romeo_and_juliet.txt']\n",
      "\n",
      "Answer:\n",
      "In the provided context of Romeo and Juliet, Shakespeare portrays love and fate as intertwined and ultimately, tragically, inseparable. The love between Romeo and Juliet is depicted as intense, passionate, and all-consuming, yet it is also shown to be doomed from the start due to the feud between their families.\n",
      "\n",
      "The chorus in Act II (document: romeo_and_juliet.txt) sets the tone for the tragic fate of the lovers, stating that \"old desire doth in his deathbed lie, / And young affection gapes to be his heir\" and that Romeo \"is belov'd, and loves again, / Alike bewitched by the charm of looks\" (CHORUS). This suggests that the love between Romeo and Juliet is strong, but also that it is subject to the whims of fate.\n",
      "\n",
      "Juliet's soliloquy in Scene II (document: romeo_and_juliet.txt) further emphasizes the intensity of her love for Romeo, as she implores the night to \"give me my Romeo, and when I shall die, / Take him and cut him out in little stars, / And he will make the face of heaven so fine / That all the world will be in love with night\" (JULIET). This passage highlights the all-consuming nature of Juliet's love and her desire to be with Romeo, even in death.\n",
      "\n",
      "However, the fate that awaits the lovers is also foreshadowed in the context. The Nurse's announcement of Tybalt's death and Romeo's banishment (document: romeo_and_juliet.txt) serves as a turning point in the play, highlighting the tragic consequences of the feud between the Montagues and Capulets. Juliet's reaction to the news, in which she laments the loss of her \"dearest cousin, and my dearer lord\" (JULIET), underscores the devastating impact of fate on the lovers.\n",
      "\n",
      "The portrayal of love and fate in Romeo and Juliet is also influenced by the characters' perceptions of each other. Juliet's description of Romeo as a \"beautiful tyrant, fiend angelical, / Dove-feather'd raven, wolvish-ravening lamb!\" (JULIET) suggests that she is aware of the contradictions and dangers inherent in their love, yet she is drawn to him nonetheless.\n",
      "\n",
      "While the provided context does not contain the entirety of the play, it is clear that Shakespeare portrays love and fate as intertwined and ultimately tragic. The love between Romeo and Juliet is intense and all-consuming, but it is also subject to the whims of fate and the feud between their families. As the play progresses, the tragic consequences of their love become increasingly apparent, ultimately leading to the devastating conclusion that fate has in store for the star-crossed lovers.\n",
      "\n",
      "References:\n",
      "- document: romeo_and_juliet.txt (multiple scenes and characters)\n",
      "- General knowledge of the play's plot and themes is used to supplement the context, but all specific references are based on the provided documents.\n",
      "\n",
      "============================================================\n",
      "Query: What role does Friar Laurence play in the tragedy of Romeo and Juliet?\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "async def main():\n",
    "    \n",
    "    \n",
    "    # Initialize the system\n",
    "    rag_system = RAGSystem(\n",
    "        cohere_api_key=os.getenv(\"COHERE_API_KEY\"),\n",
    "        openrouter_api_key=os.getenv(\"OPENROUTER_API_KEY\"),\n",
    "        kb_folder=\"books\",\n",
    "        max_concurrent_requests=5\n",
    "    )\n",
    "    \n",
    "    # Initialize the system\n",
    "    await rag_system.initialize()\n",
    "    \n",
    "    \n",
    "    queries = [\n",
    "    \"Who is Chris Olande?\",\n",
    "    \"What has Chris Olande studied at Kenyatta University?\",\n",
    "    \n",
    "    # Romeo and Juliet\n",
    "    \"What is the central conflict in Romeo and Juliet?\",\n",
    "    \"How does Shakespeare portray love and fate in Romeo and Juliet?\",\n",
    "    \"What role does Friar Laurence play in the tragedy of Romeo and Juliet?\",\n",
    "    \"Compare the characters of Romeo and Tybalt\",\n",
    "    \"How does Juliet's character evolve throughout the play?\",\n",
    "\n",
    "    # Declaration of Independence\n",
    "    \"What are the main ideas expressed in the Declaration of Independence?\",\n",
    "    \"Who wrote the Declaration of Independence and why?\",\n",
    "    \"How does the Declaration justify the colonies break from Britain?\",\n",
    "    \"What Enlightenment principles are reflected in the Declaration?\",\n",
    "    \"What impact did the Declaration of Independence have on global democratic movements?\"\n",
    "]\n",
    "    \n",
    "    # Process queries individually\n",
    "    for query in queries:\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Query: {query}\")\n",
    "        print('='*60)\n",
    "        \n",
    "        try:\n",
    "            response = await rag_system.query(query)\n",
    "            \n",
    "            print(f\"Needs Retrieval: {response.needs_retrieval}\")\n",
    "            print(f\"Reasoning: {response.retrieval_decision_reasoning}\")\n",
    "            print(f\"Retrieved Documents: {len(response.retrieved_docs)}\")\n",
    "            print(f\"Reflection Score: {response.reflection_score}/10\")\n",
    "            \n",
    "            if response.citations:\n",
    "                print(f\"Citations: {response.citations}\")\n",
    "            \n",
    "            print(f\"\\nAnswer:\\n{response.answer}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing query: {e}\")\n",
    "    \n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"BATCH PROCESSING EXAMPLE\")\n",
    "    print('='*60)\n",
    "    \n",
    "    batch_processor = BatchProcessor(rag_system, max_concurrent=5)\n",
    "    batch_queries = queries[:3]  # Process first 3 queries in batch\n",
    "    \n",
    "    batch_results = await batch_processor.process_batch(batch_queries)\n",
    "    \n",
    "    for query, response in batch_results.items():\n",
    "        print(f\"\\nBatch Query: {query}\")\n",
    "        print(f\"Reflection Score: {response.reflection_score}/10\")\n",
    "        print(f\"Answer Length: {len(response.answer)} characters\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    await main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d647ae61",
   "metadata": {
    "cellUniqueIdByVincent": "b3971"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  },
  "vincent": {
   "sessionId": "3f2153659d5b596a4189cf1d_2025-05-30T15-07-59-793Z"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
