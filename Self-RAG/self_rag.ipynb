{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ab6155cf",
   "metadata": {
    "cellUniqueIdByVincent": "9eee7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Standard library imports\n",
    "import os\n",
    "import asyncio\n",
    "import logging\n",
    "from pathlib import Path\n",
    "from typing import Tuple, Any, List, Dict, Optional\n",
    "from dataclasses import dataclass\n",
    "from asyncio import sleep, Semaphore\n",
    "from tenacity import retry, stop_after_attempt, wait_exponential, retry_if_exception_type # To implement exponential backoff\n",
    "import httpx\n",
    "import numpy as np\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Embeddings and LLM imports\n",
    "from langchain_cohere import CohereEmbeddings\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# Vector store and text splitters imports \n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.docstore import InMemoryDocstore\n",
    "import faiss\n",
    "\n",
    "# Document Loader Imports\n",
    "from langchain_community.document_loaders import (\n",
    "    TextLoader, \n",
    "    UnstructuredMarkdownLoader,\n",
    "    JSONLoader,\n",
    "    UnstructuredHTMLLoader,\n",
    "    PyPDFLoader\n",
    ")\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "# Prompt and template imports\n",
    "from langchain.prompts import ChatPromptTemplate, PromptTemplate\n",
    "\n",
    "# Langchain  runnables and pipeline imports\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "from langchain_core.runnables import RunnableParallel, RunnableLambda\n",
    "\n",
    "# Callbacks and logging imports\n",
    "from langchain.callbacks.base import AsyncCallbackHandler\n",
    "from langchain.schema import LLMResult\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Configure environment variables\n",
    "load_dotenv()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "20293f49",
   "metadata": {
    "cellUniqueIdByVincent": "2ee0a"
   },
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class SelfRAGResponse:\n",
    "    \"\"\"Complete self rag with reflection\"\"\"\n",
    "    answer: str\n",
    "    retrieved_docs: List[Document]\n",
    "    reflection_score: float\n",
    "    needs_retrieval: bool\n",
    "    citations: List[str]\n",
    "    retrieval_decision_reasoning: str\n",
    "\n",
    "class RateLimitCallback(AsyncCallbackHandler):\n",
    "    \"\"\"Callback handler to manage API rate limiting with semaphores\"\"\"\n",
    "    \n",
    "    def __init__(self, semaphore: asyncio.Semaphore):\n",
    "        self.semaphore = semaphore\n",
    "        \n",
    "    async def on_llm_start(self, serialized: Dict[str, Any], prompts: List[str], **kwargs: Any) -> None:\n",
    "        await self.semaphore.acquire()\n",
    "        \n",
    "    async def on_llm_end(self, response: LLMResult, **kwargs: Any) -> None:\n",
    "        self.semaphore.release()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0e1ab11",
   "metadata": {
    "cellUniqueIdByVincent": "948ec"
   },
   "outputs": [],
   "source": [
    "class RateLimitedCohereEmbeddings:\n",
    "    \"\"\"Wrapper for Cohere embeddings with rate limiting and retry logic\"\"\"\n",
    "    \n",
    "    def __init__(self, model: str, cohere_api_key: str, max_concurrent: int = 2, delay_between_calls: float = 0.5, batch_size: int = 30):\n",
    "        self.base_embeddings = CohereEmbeddings(\n",
    "            model=model,\n",
    "            cohere_api_key=cohere_api_key\n",
    "        )\n",
    "        self.semaphore = Semaphore(max_concurrent)\n",
    "        self.delay_between_calls = delay_between_calls\n",
    "        self.batch_size = batch_size\n",
    "        self.last_call_time = 0\n",
    "    \n",
    "    @retry(\n",
    "        stop=stop_after_attempt(5),\n",
    "        wait=wait_exponential(multiplier=1, min=1, max=60),\n",
    "        retry=retry_if_exception_type((httpx.HTTPStatusError, Exception))\n",
    "    )\n",
    "    async def _embed_with_retry(self, texts: List[str]) -> List[List[float]]:\n",
    "        \"\"\"Embed texts with retry logic\"\"\"\n",
    "        async with self.semaphore:\n",
    "            # Ensure minimum delay between calls\n",
    "            current_time = asyncio.get_event_loop().time()\n",
    "            time_since_last_call = current_time - self.last_call_time\n",
    "            if time_since_last_call < self.delay_between_calls:\n",
    "                await sleep(self.delay_between_calls - time_since_last_call)\n",
    "            \n",
    "            try:\n",
    "                logger.debug(f\"Embedding batch of {len(texts)} texts\")\n",
    "                result = await self.base_embeddings.aembed_documents(texts)\n",
    "                self.last_call_time = asyncio.get_event_loop().time()\n",
    "                return result\n",
    "                \n",
    "            except Exception as e:\n",
    "                if \"429\" in str(e) or \"Too Many Requests\" in str(e):\n",
    "                    logger.warning(f\"Rate limit hit, retrying after delay...\")\n",
    "                    await sleep(2)  # Additional delay for rate limits\n",
    "                    raise\n",
    "                else:\n",
    "                    logger.error(f\"Embedding error: {e}\")\n",
    "                    raise\n",
    "    \n",
    "    async def aembed_documents(self, texts: List[str]) -> List[List[float]]:\n",
    "        \"\"\"Embed documents with batching and rate limiting\"\"\"\n",
    "        if not texts:\n",
    "            return []\n",
    "        \n",
    "        # Use configurable batch size\n",
    "        all_embeddings = []\n",
    "        total_batches = (len(texts) + self.batch_size - 1) // self.batch_size\n",
    "        \n",
    "        logger.info(f\"Processing {len(texts)} texts in {total_batches} batches of {self.batch_size}\")\n",
    "        \n",
    "        for i in range(0, len(texts), self.batch_size):\n",
    "            batch_num = (i // self.batch_size) + 1\n",
    "            batch = texts[i:i + self.batch_size]\n",
    "            \n",
    "            logger.info(f\"Processing batch {batch_num}/{total_batches} ({len(batch)} texts)\")\n",
    "            \n",
    "            batch_embeddings = await self._embed_with_retry(batch)\n",
    "            all_embeddings.extend(batch_embeddings)\n",
    "            \n",
    "            # Delay between batches (except for the last one)\n",
    "            if i + self.batch_size < len(texts):\n",
    "                await sleep(self.delay_between_calls)\n",
    "        \n",
    "        return all_embeddings\n",
    "    \n",
    "    async def aembed_query(self, text: str) -> List[float]:\n",
    "        \"\"\"Embed a single query\"\"\"\n",
    "        async with self.semaphore:\n",
    "            current_time = asyncio.get_event_loop().time()\n",
    "            time_since_last_call = current_time - self.last_call_time\n",
    "            if time_since_last_call < self.delay_between_calls:\n",
    "                await sleep(self.delay_between_calls - time_since_last_call)\n",
    "            \n",
    "            result = await self.base_embeddings.aembed_query(text)\n",
    "            self.last_call_time = asyncio.get_event_loop().time()\n",
    "            return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "843e8cb6",
   "metadata": {
    "cellUniqueIdByVincent": "52d9a"
   },
   "outputs": [],
   "source": [
    "class DocumentLoader:\n",
    "    def __init__(self):\n",
    "        self.loaders = {\n",
    "            \".txt\": TextLoader,\n",
    "            \".md\": UnstructuredMarkdownLoader,\n",
    "            \".json\": self._create_json_loader,\n",
    "            \".pdf\": PyPDFLoader,\n",
    "            \".html\": UnstructuredHTMLLoader,\n",
    "            \".py\": TextLoader,\n",
    "            \".js\": TextLoader,\n",
    "            \".css\": TextLoader\n",
    "        }\n",
    "\n",
    "    def _create_json_loader(self, file_path: str):\n",
    "        \"\"\"Create JSON loader with custom jq_schema\"\"\"\n",
    "        return JSONLoader(\n",
    "            file_path=file_path,\n",
    "            jq_schema='.[]',\n",
    "            text_content=False\n",
    "        )\n",
    "\n",
    "    async def load_documents(self, kb_folder: str) -> List[Document]:\n",
    "        \"\"\"Load all documents from the knowledge base folder\"\"\"\n",
    "        documents = []\n",
    "        kb_path = Path(kb_folder)\n",
    "\n",
    "        if not kb_path.exists():\n",
    "            raise FileNotFoundError(f\"Knowledge base folder not found: {kb_path}\")\n",
    "\n",
    "        for file_path in kb_path.glob(\"**/*\"):\n",
    "            if file_path.is_file() and file_path.suffix.lower() in self.loaders:\n",
    "                try:\n",
    "                    loader_class = self.loaders[file_path.suffix.lower()]\n",
    "\n",
    "                    if file_path.suffix.lower() == \".json\":\n",
    "                        loader = loader_class(str(file_path))\n",
    "                    else:\n",
    "                        loader = loader_class(str(file_path))\n",
    "\n",
    "                    docs = loader.load()\n",
    "\n",
    "                    # Add metadata\n",
    "                    for doc in docs:\n",
    "                        doc.metadata.update({\n",
    "                            'file_path': str(file_path),\n",
    "                            'file_type': file_path.suffix,\n",
    "                            'file_name': file_path.name\n",
    "                        })\n",
    "\n",
    "                    documents.extend(docs)\n",
    "                \n",
    "                except Exception as e:\n",
    "                    logger.warning(f\"There was an error loading the knowledge base: {str(e)}\")\n",
    "                    # Fallback to TextLoader for unknown formats\n",
    "                    try:\n",
    "                        loader = TextLoader(str(file_path))\n",
    "                        docs = loader.load()\n",
    "                        # Add metadata\n",
    "                        for doc in docs:\n",
    "                            doc.metadata.update({\n",
    "                                'file_path': str(file_path),\n",
    "                                'file_type': file_path.suffix,\n",
    "                                'file_name': file_path.name\n",
    "                            })\n",
    "\n",
    "                        documents.extend(docs)\n",
    "\n",
    "                    except Exception as fallback_error:\n",
    "                        logger.error(f\"Failed to load {file_path} with fallback: {fallback_error}\")\n",
    "        \n",
    "        logger.info(f\"Loaded {len(documents)} documents from {kb_folder}\")\n",
    "        return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b5fb11cd",
   "metadata": {
    "cellUniqueIdByVincent": "a8e95"
   },
   "outputs": [],
   "source": [
    "\n",
    "@dataclass\n",
    "class RAGSystem:\n",
    "    cohere_api_key: str\n",
    "    openrouter_api_key: str\n",
    "    kb_folder: str\n",
    "    vector_store_path: str = None\n",
    "    max_concurrent_requests: int = 3  \n",
    "    max_concurrent_embeddings: int = 5  \n",
    "    embedding_delay: float = 0.2  \n",
    "    embedding_batch_size: int = 96  \n",
    "    chunk_size: int = 2000\n",
    "    chunk_overlap: int = 200\n",
    "    auto_save_vector_store: bool = True\n",
    "\n",
    "    def __post_init__(self):\n",
    "        # Initialize with rate-limited embeddings\n",
    "        self.embeddings = RateLimitedCohereEmbeddings(\n",
    "            model=\"embed-v4.0\",\n",
    "            cohere_api_key=self.cohere_api_key,\n",
    "            max_concurrent=self.max_concurrent_embeddings,\n",
    "            delay_between_calls=self.embedding_delay,\n",
    "            batch_size=self.embedding_batch_size\n",
    "        )\n",
    "\n",
    "        self.llm = ChatOpenAI(\n",
    "            model=\"meta-llama/llama-3.3-70b-instruct\",\n",
    "            openai_api_key=self.openrouter_api_key,\n",
    "            openai_api_base=\"https://openrouter.ai/api/v1\",\n",
    "            temperature=0.6,\n",
    "            max_tokens=1500\n",
    "        )\n",
    "\n",
    "        # Text Splitter\n",
    "        self.text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=self.chunk_size,\n",
    "            chunk_overlap=self.chunk_overlap,\n",
    "            length_function=len\n",
    "        )\n",
    "\n",
    "        self.document_loader = DocumentLoader()\n",
    "\n",
    "        # Vector store\n",
    "        self.vector_store: Optional[FAISS] = None\n",
    "        \n",
    "        # Set default vector store path if not provided\n",
    "        if self.vector_store_path is None:\n",
    "            self.vector_store_path = os.path.join(self.kb_folder, \"vector_store\")\n",
    "\n",
    "        # Semaphores for rate limiting\n",
    "        self.llm_semaphore = Semaphore(self.max_concurrent_requests)\n",
    "\n",
    "        self.rate_limit_callback = RateLimitCallback(self.llm_semaphore)\n",
    "\n",
    "        self.is_initialized = False\n",
    "\n",
    "        # Set up prompts\n",
    "        self._setup_prompts()\n",
    "\n",
    "    def _setup_prompts(self):\n",
    "        \"\"\"Set up prompts for different stages\"\"\"\n",
    "\n",
    "        # Retrieval decision prompt\n",
    "        self.retrieval_decision_prompt = PromptTemplate(\n",
    "            input_variables=[\"query\"],\n",
    "            template=\"\"\"\n",
    "            Analyze the following query to determine if it requires external knowledge retrieval.\n",
    "            \n",
    "            Query: \"{query}\"\n",
    "            \n",
    "            Consider:\n",
    "            1. Does this query ask for specific facts, data, or domain-specific information?\n",
    "            2. Would the answer benefit from external documents or knowledge base?\n",
    "            3. Is this asking about general knowledge that can be answered without retrieval?\n",
    "            4. Does it require recent or specialized information?\n",
    "            \n",
    "            Provide your reasoning and then answer with either \"RETRIEVE\" or \"NO_RETRIEVE\".\n",
    "            \n",
    "            Reasoning: [Explain your decision]\n",
    "            Decision: [RETRIEVE or NO_RETRIEVE]\n",
    "            \"\"\"\n",
    "        )\n",
    "\n",
    "        # Answer generation with retrieval prompt\n",
    "        self.rag_prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "            You are a helpful AI assistant. Use the following context documents to answer the user's question accurately and comprehensively.\n",
    "            \n",
    "            Context Documents:\n",
    "            {context}\n",
    "            \n",
    "            Question: {question}\n",
    "            \n",
    "            Instructions:\n",
    "            - Base your answer primarily on the provided context\n",
    "            - If the context doesn't contain sufficient information, acknowledge this\n",
    "            - Cite specific documents when referencing information\n",
    "            - Be accurate, detailed, and helpful\n",
    "            - If you need to use general knowledge to supplement the context, clearly indicate this\n",
    "            \n",
    "            Answer:\n",
    "        \"\"\")\n",
    "\n",
    "        # Answer generation without retrieval prompt\n",
    "        self.no_retrieval_prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "            You are a helpful AI assistant. Answer the following question using your general knowledge.\n",
    "            \n",
    "            Question: {question}\n",
    "            \n",
    "            Provide a comprehensive and accurate answer based on your training knowledge.\n",
    "            \n",
    "            Answer:\n",
    "        \"\"\")\n",
    "\n",
    "        # Reflection prompt\n",
    "        self.reflection_prompt = PromptTemplate(\n",
    "            input_variables=[\"query\", \"answer\", \"context\"],\n",
    "            template=\"\"\"\n",
    "            Evaluate the quality of the following answer based on the query and available context.\n",
    "            \n",
    "            Query: {query}\n",
    "            \n",
    "            Context:\n",
    "            {context}\n",
    "            \n",
    "            Answer: {answer}\n",
    "            \n",
    "            Rate the answer on a scale of 0-10 considering:\n",
    "            - Accuracy and factual correctness\n",
    "            - Completeness and comprehensiveness\n",
    "            - Relevance to the query\n",
    "            - Proper use of available context\n",
    "            - Clarity and helpfulness\n",
    "            \n",
    "            Provide only a single number between 0 and 10 as your rating.\n",
    "            \n",
    "            Rating:\n",
    "            \"\"\"\n",
    "        )\n",
    "\n",
    "    async def initialize(self, force_rebuild: bool = False):\n",
    "        \"\"\"Initialize the RAG System with option to force rebuild\"\"\"\n",
    "        if self.is_initialized and not force_rebuild:\n",
    "            return\n",
    "        \n",
    "        logger.info(\"Initializing the RAG system ...\")\n",
    "\n",
    "        # Try to load existing vector store first (unless force rebuild)\n",
    "        if not force_rebuild and os.path.exists(self.vector_store_path):\n",
    "            try:\n",
    "                await self.load_vector_store(self.vector_store_path)\n",
    "                logger.info(\"Loaded existing vector store successfully\")\n",
    "                return\n",
    "            except Exception as e:\n",
    "                logger.warning(f\"Failed to load existing vector store: {e}. Building new one...\")\n",
    "\n",
    "        # Load the documents\n",
    "        documents = await self.document_loader.load_documents(self.kb_folder)\n",
    "\n",
    "        if not documents:\n",
    "            logger.warning(\"No documents were found in the knowledge base ...\")\n",
    "            self.vector_store = None\n",
    "            self.is_initialized = True\n",
    "            return\n",
    "\n",
    "        # Split the documents into chunks\n",
    "        split_docs = self.text_splitter.split_documents(documents)\n",
    "        logger.info(f\"Split {len(documents)} documents into {len(split_docs)} chunks\")\n",
    "\n",
    "        # Create the vector store with progress tracking\n",
    "        logger.info(\"Creating embeddings (this may take a while due to rate limiting)...\")\n",
    "        \n",
    "        try:\n",
    "            self.vector_store = await self._create_vector_store_with_progress(split_docs)\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to create vector store: {e}\")\n",
    "            raise\n",
    "\n",
    "        # ALWAYS SAVE THE VECTOR STORE AFTER CREATION\n",
    "        if self.auto_save_vector_store:\n",
    "            await self.save_vector_store(self.vector_store_path)\n",
    "            logger.info(f\"Vector store automatically saved to {self.vector_store_path}\")\n",
    "\n",
    "        self.is_initialized = True\n",
    "        logger.info(\"Self-RAG system initialized successfully\")\n",
    "\n",
    "    async def _create_vector_store_with_progress(self, documents: List[Document]) -> FAISS:\n",
    "        \n",
    "        logger.info(f\"Creating embeddings for {len(documents)} documents...\")\n",
    "        texts = [doc.page_content for doc in documents]\n",
    "        embeddings_list = await self.embeddings.aembed_documents(texts)\n",
    "        \n",
    "        # Convert embeddings to numpy array\n",
    "        embedding_matrix = np.array(embeddings_list).astype('float32')\n",
    "        \n",
    "        # Create FAISS index\n",
    "        dimension = embedding_matrix.shape[1]\n",
    "        index = faiss.IndexFlatL2(dimension)\n",
    "        index.add(embedding_matrix)\n",
    "        \n",
    "        # Create docstore and index mapping\n",
    "        docstore = InMemoryDocstore({str(i): doc for i, doc in enumerate(documents)})\n",
    "        index_to_docstore_id = {i: str(i) for i in range(len(documents))}\n",
    "        \n",
    "        # Create FAISS vector store\n",
    "        vector_store = FAISS(\n",
    "            embedding_function=self.embeddings.base_embeddings,\n",
    "            index=index,\n",
    "            docstore=docstore,\n",
    "            index_to_docstore_id=index_to_docstore_id\n",
    "        )\n",
    "        \n",
    "        logger.info(\"Vector store created successfully\")\n",
    "        return vector_store\n",
    "\n",
    "    async def _should_retrieve(self, query: str) -> Tuple[bool, str]:\n",
    "        \"\"\" Determine if the retrieval is needed and get reasoning\"\"\"\n",
    "        chain = self.retrieval_decision_prompt | self.llm.with_config(callbacks=[self.rate_limit_callback])\n",
    "\n",
    "        result = await chain.ainvoke({\"query\": query})\n",
    "\n",
    "        # Parse the result\n",
    "        lines = result.content.strip().split('\\n')\n",
    "        reasoning = \"\"\n",
    "        decision = False\n",
    "\n",
    "        for line in lines:\n",
    "            if line.startswith(\"Reasoning:\"):\n",
    "                reasoning = line.replace(\"Reasoning:\", \"\").strip()\n",
    "            elif line.startswith(\"Decision:\"):\n",
    "                decision_text = line.replace(\"Decision:\", \"\").strip()\n",
    "                decision = \"RETRIEVE\" in decision_text.upper()\n",
    "        \n",
    "        return decision, reasoning\n",
    "\n",
    "    async def _retrieve_documents(self, query: str, k:int = 5) -> List[Document]:\n",
    "        \"\"\"Retrieve relevant documents\"\"\"\n",
    "        if not self.vector_store:\n",
    "            return []\n",
    "\n",
    "        # Use similarity search with scores\n",
    "        doc_with_scores = await self.vector_store.asimilarity_search_with_score(query, k = k)\n",
    "        # Filter by relevance score\n",
    "        relevant_docs = [doc for doc, score in doc_with_scores if score > 0.8]\n",
    "\n",
    "        return relevant_docs\n",
    "\n",
    "    async def _generate_answer_with_retrieval(self, query: str, documents: List[Document]) -> str:\n",
    "        \"\"\"Generate answers using retrieved documents\"\"\"\n",
    "        context = \"\\n\\n\".join([\n",
    "            f\"Document: {doc.metadata.get('file_name', 'Unknown')}\\n{doc.page_content}\"\n",
    "            for doc in documents\n",
    "        ])\n",
    "        \n",
    "        chain = self.rag_prompt | self.llm | StrOutputParser()\n",
    "        \n",
    "        result = await chain.ainvoke({\n",
    "            \"context\": context,\n",
    "            \"question\": query\n",
    "        })\n",
    "        \n",
    "        return result\n",
    "        \n",
    "    async def _generate_answer_without_retrieval(self, query: str) -> str:\n",
    "        \"\"\"Generate answer without retrieval\"\"\"\n",
    "        chain = self.no_retrieval_prompt | self.llm | StrOutputParser()\n",
    "        \n",
    "        result = await chain.ainvoke({\"question\": query})\n",
    "        return result\n",
    "\n",
    "    async def _reflect_on_answer(self, query: str, answer: str, documents: List[Document]) -> float:\n",
    "        \"\"\"Reflect on answer quality\"\"\"\n",
    "        context = \"\\n\\n\".join([doc.page_content for doc in documents]) if documents else \"No context provided\"\n",
    "        \n",
    "        chain = self.reflection_prompt | self.llm.with_config(callbacks=[self.rate_limit_callback])\n",
    "        \n",
    "        result = await chain.ainvoke({\n",
    "            \"query\": query,\n",
    "            \"answer\": answer,\n",
    "            \"context\": context\n",
    "        })\n",
    "        \n",
    "        try:\n",
    "            content = result.content if hasattr(result, 'content') else str(result)\n",
    "            score_str = content.strip().split('\\n')[-1]\n",
    "            \n",
    "            # Extract digits and decimal point from the score string\n",
    "            score_chars = ''.join(c for c in score_str if c.isdigit() or c == '.')\n",
    "            \n",
    "            if score_chars:\n",
    "                score = float(score_chars)\n",
    "                return min(10.0, max(0.0, score))  # Clamp between 0 and 10\n",
    "            else:\n",
    "                logger.warning(f\"No numeric score found in: {score_str}\")\n",
    "                return 5.0\n",
    "                \n",
    "        except (ValueError, IndexError) as e:\n",
    "            logger.warning(f\"Could not parse reflection score: {result}. Error: {e}\")\n",
    "            return 5.0\n",
    "\n",
    "    async def query(self, query: str, force_retrieval: bool = False) -> SelfRAGResponse:\n",
    "        \"\"\"Process query using the SelfRAG\"\"\"\n",
    "        if not self.is_initialized:\n",
    "            await self.initialize()\n",
    "\n",
    "        # Decide whether we need retrieval\n",
    "        if force_retrieval:\n",
    "            needs_retrieval = True\n",
    "            reasoning = \"Forced retrieval requested\"\n",
    "        else:\n",
    "            needs_retrieval, reasoning = await self._should_retrieve(query)\n",
    "\n",
    "        retrieved_docs = []\n",
    "        citations = []\n",
    "\n",
    "        # Retrieve documents if needed\n",
    "        if needs_retrieval and self.vector_store:\n",
    "            retrieved_docs = await self._retrieve_documents(query)\n",
    "            citations = [\n",
    "                doc.metadata.get('file_path', f\"Document {i}\")\n",
    "                for i, doc in enumerate(retrieved_docs)\n",
    "            ]\n",
    "\n",
    "        # Generate answer\n",
    "        if retrieved_docs:\n",
    "            answer = await self._generate_answer_with_retrieval(query, retrieved_docs)\n",
    "        else:\n",
    "            answer = await self._generate_answer_without_retrieval(query)\n",
    "\n",
    "        # Reflect on answer quality\n",
    "        reflection_score = await self._reflect_on_answer(query, answer, retrieved_docs)\n",
    "\n",
    "        return SelfRAGResponse(\n",
    "            answer=answer,\n",
    "            retrieved_docs=retrieved_docs,\n",
    "            reflection_score=reflection_score,\n",
    "            needs_retrieval=needs_retrieval,\n",
    "            citations=list(set(citations)),  # Remove duplicates\n",
    "            retrieval_decision_reasoning=reasoning\n",
    "        )\n",
    "\n",
    "    async def save_vector_store(self, path: str = None):\n",
    "        \"\"\"Save vector store to disk\"\"\"\n",
    "        if not self.vector_store:\n",
    "            logger.warning(\"No vector store to save\")\n",
    "            return\n",
    "            \n",
    "        save_path = path or self.vector_store_path\n",
    "        \n",
    "        # Create directory if it doesn't exist\n",
    "        os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "        \n",
    "        try:\n",
    "            self.vector_store.save_local(save_path)\n",
    "            logger.info(f\"Vector store saved to {save_path}\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to save vector store: {e}\")\n",
    "            raise\n",
    "    \n",
    "    async def load_vector_store(self, path: str = None):\n",
    "        from langchain_community.vectorstores import FAISS\n",
    "        load_path = path or self.vector_store_path\n",
    "        try:\n",
    "            self.vector_store = FAISS.load_local(\n",
    "                load_path,\n",
    "                self.embeddings.base_embeddings,\n",
    "                allow_dangerous_deserialization=True\n",
    "            )\n",
    "            if not isinstance(self.vector_store, FAISS):\n",
    "                raise ValueError(\"Loaded vector store is not a valid FAISS object\")\n",
    "            self.is_initialized = True\n",
    "            logger.info(f\"Vector store loaded from {load_path}\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to load vector store from {load_path}: {e}\")\n",
    "            self.vector_store = None\n",
    "            raise\n",
    "\n",
    "    async def rebuild_vector_store(self):\n",
    "        logger.info(\"Force rebuilding vector store...\")\n",
    "        self.vector_store = None\n",
    "        await self.initialize(force_rebuild=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7bab832c",
   "metadata": {
    "cellUniqueIdByVincent": "83e4e"
   },
   "outputs": [],
   "source": [
    "class BatchProcessor:\n",
    "    \"\"\"Process multiple queries in batch with concurrency control\"\"\"\n",
    "    \n",
    "    def __init__(self, rag_system: RAGSystem, max_concurrent: int = 3):\n",
    "        self.rag_system = rag_system\n",
    "        self.semaphore = asyncio.Semaphore(max_concurrent)\n",
    "    \n",
    "    async def process_query(self, query: str) -> Tuple[str, SelfRAGResponse]:\n",
    "        \"\"\"Process a single query with semaphore\"\"\"\n",
    "        async with self.semaphore:\n",
    "            response = await self.rag_system.query(query)\n",
    "            return query, response\n",
    "    \n",
    "    async def process_batch(self, queries: List[str]) -> Dict[str, SelfRAGResponse]:\n",
    "        \"\"\"Process multiple queries concurrently\"\"\"\n",
    "        tasks = [self.process_query(query) for query in queries]\n",
    "        results = await asyncio.gather(*tasks, return_exceptions=True)\n",
    "        \n",
    "        output = {}\n",
    "        for result in results:\n",
    "            if isinstance(result, Exception):\n",
    "                logger.error(f\"Error processing query: {result}\")\n",
    "            else:\n",
    "                query, response = result\n",
    "                output[query] = response\n",
    "        \n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "be260d94",
   "metadata": {
    "cellUniqueIdByVincent": "4beb4"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Initializing the RAG system ...\n",
      "INFO:__main__:Loaded 3 documents from books\n",
      "INFO:__main__:Split 3 documents into 170 chunks\n",
      "INFO:__main__:Creating embeddings (this may take a while due to rate limiting)...\n",
      "INFO:__main__:Creating embeddings for 170 documents...\n",
      "INFO:__main__:Processing 170 texts in 2 batches of 96\n",
      "INFO:__main__:Processing batch 1/2 (96 texts)\n",
      "ERROR:__main__:Failed to create vector store: RetryError[<Future at 0x7f93ccee8180 state=finished raised AttributeError>]\n"
     ]
    },
    {
     "ename": "RetryError",
     "evalue": "RetryError[<Future at 0x7f93ccee8180 state=finished raised AttributeError>]",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Langchain/.venv/lib/python3.13/site-packages/tenacity/asyncio/__init__.py:114\u001b[39m, in \u001b[36mAsyncRetrying.__call__\u001b[39m\u001b[34m(self, fn, *args, **kwargs)\u001b[39m\n\u001b[32m    113\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m114\u001b[39m     result = \u001b[38;5;28;01mawait\u001b[39;00m fn(*args, **kwargs)\n\u001b[32m    115\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m:  \u001b[38;5;66;03m# noqa: B902\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 25\u001b[39m, in \u001b[36mRateLimitedCohereEmbeddings._embed_with_retry\u001b[39m\u001b[34m(self, texts)\u001b[39m\n\u001b[32m     24\u001b[39m current_time = asyncio.get_event_loop().time()\n\u001b[32m---> \u001b[39m\u001b[32m25\u001b[39m time_since_last_call = current_time - \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlast_call_time\u001b[49m\n\u001b[32m     26\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m time_since_last_call < \u001b[38;5;28mself\u001b[39m.delay_between_calls:\n",
      "\u001b[31mAttributeError\u001b[39m: 'RateLimitedCohereEmbeddings' object has no attribute 'last_call_time'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mRetryError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 63\u001b[39m\n\u001b[32m     60\u001b[39m         \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mAnswer Length: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(response.answer)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m characters\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     62\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m63\u001b[39m     \u001b[38;5;28;01mawait\u001b[39;00m main()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 9\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m      2\u001b[39m     rag_system = RAGSystem(\n\u001b[32m      3\u001b[39m     cohere_api_key=os.getenv(\u001b[33m\"\u001b[39m\u001b[33mCOHERE_API_KEY\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m      4\u001b[39m     openrouter_api_key=os.getenv(\u001b[33m\"\u001b[39m\u001b[33mOPENROUTER_API_KEY\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m      5\u001b[39m     kb_folder=\u001b[33m\"\u001b[39m\u001b[33mbooks\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      6\u001b[39m     )\n\u001b[32m      8\u001b[39m     \u001b[38;5;66;03m# Initialize the system\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m     \u001b[38;5;28;01mawait\u001b[39;00m rag_system.initialize()\n\u001b[32m     12\u001b[39m     queries = [\n\u001b[32m     13\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mWho is Chris Olande?\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     14\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mWhat has Chris Olande studied at Kenyatta University?\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     22\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mHow does the Declaration justify the colonies break from Britain?\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     23\u001b[39m ]\n\u001b[32m     25\u001b[39m     \u001b[38;5;66;03m# Process queries individually\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 172\u001b[39m, in \u001b[36mRAGSystem.initialize\u001b[39m\u001b[34m(self, force_rebuild)\u001b[39m\n\u001b[32m    169\u001b[39m logger.info(\u001b[33m\"\u001b[39m\u001b[33mCreating embeddings (this may take a while due to rate limiting)...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    171\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m172\u001b[39m     \u001b[38;5;28mself\u001b[39m.vector_store = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._create_vector_store_with_progress(split_docs)\n\u001b[32m    173\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    174\u001b[39m     logger.error(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFailed to create vector store: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 189\u001b[39m, in \u001b[36mRAGSystem._create_vector_store_with_progress\u001b[39m\u001b[34m(self, documents)\u001b[39m\n\u001b[32m    187\u001b[39m logger.info(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCreating embeddings for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(documents)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m documents...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    188\u001b[39m texts = [doc.page_content \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m documents]\n\u001b[32m--> \u001b[39m\u001b[32m189\u001b[39m embeddings_list = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.embeddings.aembed_documents(texts)\n\u001b[32m    191\u001b[39m \u001b[38;5;66;03m# Convert embeddings to numpy array\u001b[39;00m\n\u001b[32m    192\u001b[39m embedding_matrix = np.array(embeddings_list).astype(\u001b[33m'\u001b[39m\u001b[33mfloat32\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 61\u001b[39m, in \u001b[36mRateLimitedCohereEmbeddings.aembed_documents\u001b[39m\u001b[34m(self, texts)\u001b[39m\n\u001b[32m     57\u001b[39m batch = texts[i:i + \u001b[38;5;28mself\u001b[39m.batch_size]\n\u001b[32m     59\u001b[39m logger.info(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mProcessing batch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbatch_num\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtotal_batches\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(batch)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m texts)\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m61\u001b[39m batch_embeddings = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._embed_with_retry(batch)\n\u001b[32m     62\u001b[39m all_embeddings.extend(batch_embeddings)\n\u001b[32m     64\u001b[39m \u001b[38;5;66;03m# Delay between batches (except for the last one)\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Langchain/.venv/lib/python3.13/site-packages/tenacity/asyncio/__init__.py:189\u001b[39m, in \u001b[36mAsyncRetrying.wraps.<locals>.async_wrapped\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    187\u001b[39m copy = \u001b[38;5;28mself\u001b[39m.copy()\n\u001b[32m    188\u001b[39m async_wrapped.statistics = copy.statistics  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m189\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m copy(fn, *args, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Langchain/.venv/lib/python3.13/site-packages/tenacity/asyncio/__init__.py:111\u001b[39m, in \u001b[36mAsyncRetrying.__call__\u001b[39m\u001b[34m(self, fn, *args, **kwargs)\u001b[39m\n\u001b[32m    109\u001b[39m retry_state = RetryCallState(retry_object=\u001b[38;5;28mself\u001b[39m, fn=fn, args=args, kwargs=kwargs)\n\u001b[32m    110\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m111\u001b[39m     do = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.iter(retry_state=retry_state)\n\u001b[32m    112\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(do, DoAttempt):\n\u001b[32m    113\u001b[39m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Langchain/.venv/lib/python3.13/site-packages/tenacity/asyncio/__init__.py:153\u001b[39m, in \u001b[36mAsyncRetrying.iter\u001b[39m\u001b[34m(self, retry_state)\u001b[39m\n\u001b[32m    151\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    152\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m action \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.iter_state.actions:\n\u001b[32m--> \u001b[39m\u001b[32m153\u001b[39m     result = \u001b[38;5;28;01mawait\u001b[39;00m action(retry_state)\n\u001b[32m    154\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Langchain/.venv/lib/python3.13/site-packages/tenacity/_utils.py:99\u001b[39m, in \u001b[36mwrap_to_async_func.<locals>.inner\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     98\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34minner\u001b[39m(*args: typing.Any, **kwargs: typing.Any) -> typing.Any:\n\u001b[32m---> \u001b[39m\u001b[32m99\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Langchain/.venv/lib/python3.13/site-packages/tenacity/__init__.py:419\u001b[39m, in \u001b[36mBaseRetrying._post_stop_check_actions.<locals>.exc_check\u001b[39m\u001b[34m(rs)\u001b[39m\n\u001b[32m    417\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.reraise:\n\u001b[32m    418\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m retry_exc.reraise()\n\u001b[32m--> \u001b[39m\u001b[32m419\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m retry_exc \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mfut\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mexception\u001b[39;00m()\n",
      "\u001b[31mRetryError\u001b[39m: RetryError[<Future at 0x7f93ccee8180 state=finished raised AttributeError>]"
     ]
    }
   ],
   "source": [
    "async def main():\n",
    "    rag_system = RAGSystem(\n",
    "    cohere_api_key=os.getenv(\"COHERE_API_KEY\"),\n",
    "    openrouter_api_key=os.getenv(\"OPENROUTER_API_KEY\"),\n",
    "    kb_folder=\"books\"\n",
    "    )\n",
    "    \n",
    "    # Initialize the system\n",
    "    await rag_system.initialize()\n",
    "    \n",
    "    \n",
    "    queries = [\n",
    "    \"Who is Chris Olande?\",\n",
    "    \"What has Chris Olande studied at Kenyatta University?\",\n",
    "    \n",
    "    # Romeo and Juliet\n",
    "    \"What is the central conflict in Romeo and Juliet?\",\n",
    "    \"How does Shakespeare portray love and fate in Romeo and Juliet?\",\n",
    "\n",
    "    # Declaration of Independence\n",
    "    \"What are the main ideas expressed in the Declaration of Independence?\",\n",
    "    \"How does the Declaration justify the colonies break from Britain?\"\n",
    "]\n",
    "    \n",
    "    # Process queries individually\n",
    "    for query in queries:\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"Query: {query}\")\n",
    "        print('='*80)\n",
    "        \n",
    "        try:\n",
    "            response = await rag_system.query(query)\n",
    "            \n",
    "            print(f\"Needs Retrieval: {response.needs_retrieval}\")\n",
    "            print(f\"Reasoning: {response.retrieval_decision_reasoning}\")\n",
    "            print(f\"Retrieved Documents: {len(response.retrieved_docs)}\")\n",
    "            print(f\"Reflection Score: {response.reflection_score}/10\")\n",
    "            \n",
    "            if response.citations:\n",
    "                print(f\"Citations: {response.citations}\")\n",
    "            \n",
    "            print(f\"\\nAnswer:\\n{response.answer}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing query: {e}\")\n",
    "    \n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"BATCH PROCESSING EXAMPLE\")\n",
    "    print('='*60)\n",
    "    \n",
    "    batch_processor = BatchProcessor(rag_system, max_concurrent=3)\n",
    "    batch_queries = queries[:4]  # Process first 4 queries in batch\n",
    "    \n",
    "    batch_results = await batch_processor.process_batch(batch_queries)\n",
    "    \n",
    "    for query, response in batch_results.items():\n",
    "        print(f\"\\nBatch Query: {query}\")\n",
    "        print(f\"Reflection Score: {response.reflection_score}/10\")\n",
    "        print(f\"Answer Length: {len(response.answer)} characters\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    await main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "118052ee",
   "metadata": {
    "cellUniqueIdByVincent": "6d452"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  },
  "vincent": {
   "sessionId": "3f2153659d5b596a4189cf1d_2025-05-30T15-07-59-793Z"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
