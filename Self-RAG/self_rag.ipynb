{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ab6155cf",
   "metadata": {
    "cellUniqueIdByVincent": "9eee7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Standard library imports\n",
    "import os\n",
    "import asyncio\n",
    "import logging\n",
    "from pathlib import Path\n",
    "from typing import Tuple, Any, List, Dict, Optional\n",
    "from dataclasses import dataclass\n",
    "from asyncio import sleep, Semaphore\n",
    "from tenacity import retry, stop_after_attempt, wait_exponential, retry_if_exception_type # To implement exponential backoff\n",
    "import httpx\n",
    "import numpy as np\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Embeddings and LLM imports\n",
    "from langchain_cohere import CohereEmbeddings\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# Vector store and text splitters imports \n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.docstore import InMemoryDocstore\n",
    "import faiss\n",
    "\n",
    "# Document Loader Imports\n",
    "from langchain_community.document_loaders import (\n",
    "    TextLoader, \n",
    "    UnstructuredMarkdownLoader,\n",
    "    JSONLoader,\n",
    "    UnstructuredHTMLLoader,\n",
    "    PyPDFLoader\n",
    ")\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "# Prompt and template imports\n",
    "from langchain.prompts import ChatPromptTemplate, PromptTemplate\n",
    "\n",
    "# Langchain  runnables and pipeline imports\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "from langchain_core.runnables import RunnableParallel, RunnableLambda\n",
    "\n",
    "# Callbacks and logging imports\n",
    "from langchain.callbacks.base import AsyncCallbackHandler\n",
    "from langchain.schema import LLMResult\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Configure environment variables\n",
    "load_dotenv()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "20293f49",
   "metadata": {
    "cellUniqueIdByVincent": "2ee0a"
   },
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class SelfRAGResponse:\n",
    "    \"\"\"Complete self rag with reflection\"\"\"\n",
    "    answer: str\n",
    "    retrieved_docs: List[Document]\n",
    "    reflection_score: float\n",
    "    needs_retrieval: bool\n",
    "    citations: List[str]\n",
    "    retrieval_decision_reasoning: str\n",
    "\n",
    "class RateLimitCallback(AsyncCallbackHandler):\n",
    "    \"\"\"Callback handler to manage API rate limiting with semaphores\"\"\"\n",
    "    \n",
    "    def __init__(self, semaphore: asyncio.Semaphore):\n",
    "        self.semaphore = semaphore\n",
    "        \n",
    "    async def on_llm_start(self, serialized: Dict[str, Any], prompts: List[str], **kwargs: Any) -> None:\n",
    "        await self.semaphore.acquire()\n",
    "        \n",
    "    async def on_llm_end(self, response: LLMResult, **kwargs: Any) -> None:\n",
    "        self.semaphore.release()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f0e1ab11",
   "metadata": {
    "cellUniqueIdByVincent": "948ec"
   },
   "outputs": [],
   "source": [
    "class RateLimitedCohereEmbeddings:\n",
    "    \"\"\"Wrapper for Cohere embeddings with rate limiting and retry logic\"\"\"\n",
    "    \n",
    "    def __init__(self, model: str, cohere_api_key: str, max_concurrent: int = 2, delay_between_calls: float = 0.5, batch_size: int = 30):\n",
    "        self.base_embeddings = CohereEmbeddings(\n",
    "            model=model,\n",
    "            cohere_api_key=cohere_api_key\n",
    "        )\n",
    "        self.semaphore = Semaphore(max_concurrent)\n",
    "        self.delay_between_calls = delay_between_calls\n",
    "        self.batch_size = batch_size\n",
    "        self.last_call_time = 0\n",
    "    \n",
    "    @retry(\n",
    "        stop=stop_after_attempt(5),\n",
    "        wait=wait_exponential(multiplier=1, min=1, max=60),\n",
    "        retry=retry_if_exception_type((httpx.HTTPStatusError, Exception))\n",
    "    )\n",
    "    async def _embed_with_retry(self, texts: List[str]) -> List[List[float]]:\n",
    "        \"\"\"Embed texts with retry logic\"\"\"\n",
    "        async with self.semaphore:\n",
    "            # Ensure minimum delay between calls\n",
    "            current_time = asyncio.get_event_loop().time()\n",
    "            time_since_last_call = current_time - self.last_call_time\n",
    "            if time_since_last_call < self.delay_between_calls:\n",
    "                await sleep(self.delay_between_calls - time_since_last_call)\n",
    "            \n",
    "            try:\n",
    "                logger.debug(f\"Embedding batch of {len(texts)} texts\")\n",
    "                result = await self.base_embeddings.aembed_documents(texts)\n",
    "                self.last_call_time = asyncio.get_event_loop().time()\n",
    "                return result\n",
    "                \n",
    "            except Exception as e:\n",
    "                if \"429\" in str(e) or \"Too Many Requests\" in str(e):\n",
    "                    logger.warning(f\"Rate limit hit, retrying after delay...\")\n",
    "                    await sleep(2)  # Additional delay for rate limits\n",
    "                    raise\n",
    "                else:\n",
    "                    logger.error(f\"Embedding error: {e}\")\n",
    "                    raise\n",
    "    \n",
    "    async def aembed_documents(self, texts: List[str]) -> List[List[float]]:\n",
    "        \"\"\"Embed documents with batching and rate limiting\"\"\"\n",
    "        if not texts:\n",
    "            return []\n",
    "        \n",
    "        # Use configurable batch size\n",
    "        all_embeddings = []\n",
    "        total_batches = (len(texts) + self.batch_size - 1) // self.batch_size\n",
    "        \n",
    "        logger.info(f\"Processing {len(texts)} texts in {total_batches} batches of {self.batch_size}\")\n",
    "        \n",
    "        for i in range(0, len(texts), self.batch_size):\n",
    "            batch_num = (i // self.batch_size) + 1\n",
    "            batch = texts[i:i + self.batch_size]\n",
    "            \n",
    "            logger.info(f\"Processing batch {batch_num}/{total_batches} ({len(batch)} texts)\")\n",
    "            \n",
    "            batch_embeddings = await self._embed_with_retry(batch)\n",
    "            all_embeddings.extend(batch_embeddings)\n",
    "            \n",
    "            # Delay between batches (except for the last one)\n",
    "            if i + self.batch_size < len(texts):\n",
    "                await sleep(self.delay_between_calls)\n",
    "        \n",
    "        return all_embeddings\n",
    "    \n",
    "    async def aembed_query(self, text: str) -> List[float]:\n",
    "        \"\"\"Embed a single query\"\"\"\n",
    "        async with self.semaphore:\n",
    "            current_time = asyncio.get_event_loop().time()\n",
    "            time_since_last_call = current_time - self.last_call_time\n",
    "            if time_since_last_call < self.delay_between_calls:\n",
    "                await sleep(self.delay_between_calls - time_since_last_call)\n",
    "            \n",
    "            result = await self.base_embeddings.aembed_query(text)\n",
    "            self.last_call_time = asyncio.get_event_loop().time()\n",
    "            return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "843e8cb6",
   "metadata": {
    "cellUniqueIdByVincent": "52d9a"
   },
   "outputs": [],
   "source": [
    "class DocumentLoader:\n",
    "    def __init__(self):\n",
    "        self.loaders = {\n",
    "            \".txt\": TextLoader,\n",
    "            \".md\": UnstructuredMarkdownLoader,\n",
    "            \".json\": self._create_json_loader,\n",
    "            \".pdf\": PyPDFLoader,\n",
    "            \".html\": UnstructuredHTMLLoader,\n",
    "            \".py\": TextLoader,\n",
    "            \".js\": TextLoader,\n",
    "            \".css\": TextLoader\n",
    "        }\n",
    "\n",
    "    def _create_json_loader(self, file_path: str):\n",
    "        \"\"\"Create JSON loader with custom jq_schema\"\"\"\n",
    "        return JSONLoader(\n",
    "            file_path=file_path,\n",
    "            jq_schema='.[]',\n",
    "            text_content=False\n",
    "        )\n",
    "\n",
    "    async def load_documents(self, kb_folder: str) -> List[Document]:\n",
    "        \"\"\"Load all documents from the knowledge base folder\"\"\"\n",
    "        documents = []\n",
    "        kb_path = Path(kb_folder)\n",
    "\n",
    "        if not kb_path.exists():\n",
    "            raise FileNotFoundError(f\"Knowledge base folder not found: {kb_path}\")\n",
    "\n",
    "        for file_path in kb_path.glob(\"**/*\"):\n",
    "            if file_path.is_file() and file_path.suffix.lower() in self.loaders:\n",
    "                try:\n",
    "                    loader_class = self.loaders[file_path.suffix.lower()]\n",
    "\n",
    "                    if file_path.suffix.lower() == \".json\":\n",
    "                        loader = loader_class(str(file_path))\n",
    "                    else:\n",
    "                        loader = loader_class(str(file_path))\n",
    "\n",
    "                    docs = loader.load()\n",
    "\n",
    "                    # Add metadata\n",
    "                    for doc in docs:\n",
    "                        doc.metadata.update({\n",
    "                            'file_path': str(file_path),\n",
    "                            'file_type': file_path.suffix,\n",
    "                            'file_name': file_path.name\n",
    "                        })\n",
    "\n",
    "                    documents.extend(docs)\n",
    "                \n",
    "                except Exception as e:\n",
    "                    logger.warning(f\"There was an error loading the knowledge base: {str(e)}\")\n",
    "                    # Fallback to TextLoader for unknown formats\n",
    "                    try:\n",
    "                        loader = TextLoader(str(file_path))\n",
    "                        docs = loader.load()\n",
    "                        # Add metadata\n",
    "                        for doc in docs:\n",
    "                            doc.metadata.update({\n",
    "                                'file_path': str(file_path),\n",
    "                                'file_type': file_path.suffix,\n",
    "                                'file_name': file_path.name\n",
    "                            })\n",
    "\n",
    "                        documents.extend(docs)\n",
    "\n",
    "                    except Exception as fallback_error:\n",
    "                        logger.error(f\"Failed to load {file_path} with fallback: {fallback_error}\")\n",
    "        \n",
    "        logger.info(f\"Loaded {len(documents)} documents from {kb_folder}\")\n",
    "        return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b5fb11cd",
   "metadata": {
    "cellUniqueIdByVincent": "a8e95"
   },
   "outputs": [],
   "source": [
    "\n",
    "@dataclass\n",
    "class RAGSystem:\n",
    "    cohere_api_key: str\n",
    "    openrouter_api_key: str\n",
    "    kb_folder: str\n",
    "    vector_store_path: str = None\n",
    "    max_concurrent_requests: int = 3  \n",
    "    max_concurrent_embeddings: int = 5  \n",
    "    embedding_delay: float = 0.2  \n",
    "    embedding_batch_size: int = 96  \n",
    "    chunk_size: int = 2000\n",
    "    chunk_overlap: int = 200\n",
    "    auto_save_vector_store: bool = True\n",
    "\n",
    "    def __post_init__(self):\n",
    "        # Initialize with rate-limited embeddings\n",
    "        self.embeddings = RateLimitedCohereEmbeddings(\n",
    "            model=\"embed-v4.0\",\n",
    "            cohere_api_key=self.cohere_api_key,\n",
    "            max_concurrent=self.max_concurrent_embeddings,\n",
    "            delay_between_calls=self.embedding_delay,\n",
    "            batch_size=self.embedding_batch_size\n",
    "        )\n",
    "\n",
    "        self.llm = ChatOpenAI(\n",
    "            model=\"meta-llama/llama-3.3-70b-instruct\",\n",
    "            openai_api_key=self.openrouter_api_key,\n",
    "            openai_api_base=\"https://openrouter.ai/api/v1\",\n",
    "            temperature=0.6,\n",
    "            max_tokens=1500\n",
    "        )\n",
    "\n",
    "        # Text Splitter\n",
    "        self.text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=self.chunk_size,\n",
    "            chunk_overlap=self.chunk_overlap,\n",
    "            length_function=len\n",
    "        )\n",
    "\n",
    "        self.document_loader = DocumentLoader()\n",
    "\n",
    "        # Vector store\n",
    "        self.vector_store: Optional[FAISS] = None\n",
    "        \n",
    "        # Set default vector store path if not provided\n",
    "        if self.vector_store_path is None:\n",
    "            self.vector_store_path = os.path.join(self.kb_folder, \"vector_store\")\n",
    "\n",
    "        # Semaphores for rate limiting\n",
    "        self.llm_semaphore = Semaphore(self.max_concurrent_requests)\n",
    "\n",
    "        self.rate_limit_callback = RateLimitCallback(self.llm_semaphore)\n",
    "\n",
    "        self.is_initialized = False\n",
    "\n",
    "        # Set up prompts\n",
    "        self._setup_prompts()\n",
    "\n",
    "    def _setup_prompts(self):\n",
    "        \"\"\"Set up prompts for different stages\"\"\"\n",
    "\n",
    "        # Retrieval decision prompt\n",
    "        self.retrieval_decision_prompt = PromptTemplate(\n",
    "            input_variables=[\"query\"],\n",
    "            template=\"\"\"\n",
    "            Analyze the following query to determine if it requires external knowledge retrieval.\n",
    "            \n",
    "            Query: \"{query}\"\n",
    "            \n",
    "            Consider:\n",
    "            1. Does this query ask for specific facts, data, or domain-specific information?\n",
    "            2. Would the answer benefit from external documents or knowledge base?\n",
    "            3. Is this asking about general knowledge that can be answered without retrieval?\n",
    "            4. Does it require recent or specialized information?\n",
    "            \n",
    "            Provide your reasoning and then answer with either \"RETRIEVE\" or \"NO_RETRIEVE\".\n",
    "            \n",
    "            Reasoning: [Explain your decision]\n",
    "            Decision: [RETRIEVE or NO_RETRIEVE]\n",
    "            \"\"\"\n",
    "        )\n",
    "\n",
    "        # Answer generation with retrieval prompt\n",
    "        self.rag_prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "            You are a helpful AI assistant. Use the following context documents to answer the user's question accurately and comprehensively.\n",
    "            \n",
    "            Context Documents:\n",
    "            {context}\n",
    "            \n",
    "            Question: {question}\n",
    "            \n",
    "            Instructions:\n",
    "            - Base your answer primarily on the provided context\n",
    "            - If the context doesn't contain sufficient information, acknowledge this\n",
    "            - Cite specific documents when referencing information\n",
    "            - Be accurate, detailed, and helpful\n",
    "            - If you need to use general knowledge to supplement the context, clearly indicate this\n",
    "            \n",
    "            Answer:\n",
    "        \"\"\")\n",
    "\n",
    "        # Answer generation without retrieval prompt\n",
    "        self.no_retrieval_prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "            You are a helpful AI assistant. Answer the following question using your general knowledge.\n",
    "            \n",
    "            Question: {question}\n",
    "            \n",
    "            Provide a comprehensive and accurate answer based on your training knowledge.\n",
    "            \n",
    "            Answer:\n",
    "        \"\"\")\n",
    "\n",
    "        # Reflection prompt\n",
    "        self.reflection_prompt = PromptTemplate(\n",
    "            input_variables=[\"query\", \"answer\", \"context\"],\n",
    "            template=\"\"\"\n",
    "            Evaluate the quality of the following answer based on the query and available context.\n",
    "            \n",
    "            Query: {query}\n",
    "            \n",
    "            Context:\n",
    "            {context}\n",
    "            \n",
    "            Answer: {answer}\n",
    "            \n",
    "            Rate the answer on a scale of 0-10 considering:\n",
    "            - Accuracy and factual correctness\n",
    "            - Completeness and comprehensiveness\n",
    "            - Relevance to the query\n",
    "            - Proper use of available context\n",
    "            - Clarity and helpfulness\n",
    "            \n",
    "            Provide only a single number between 0 and 10 as your rating.\n",
    "            \n",
    "            Rating:\n",
    "            \"\"\"\n",
    "        )\n",
    "\n",
    "    async def initialize(self, force_rebuild: bool = False):\n",
    "        \"\"\"Initialize the RAG System with option to force rebuild\"\"\"\n",
    "        if self.is_initialized and not force_rebuild:\n",
    "            return\n",
    "        \n",
    "        logger.info(\"Initializing the RAG system ...\")\n",
    "\n",
    "        # Try to load existing vector store first (unless force rebuild)\n",
    "        if not force_rebuild and os.path.exists(self.vector_store_path):\n",
    "            try:\n",
    "                await self.load_vector_store(self.vector_store_path)\n",
    "                logger.info(\"Loaded existing vector store successfully\")\n",
    "                return\n",
    "            except Exception as e:\n",
    "                logger.warning(f\"Failed to load existing vector store: {e}. Building new one...\")\n",
    "\n",
    "        # Load the documents\n",
    "        documents = await self.document_loader.load_documents(self.kb_folder)\n",
    "\n",
    "        if not documents:\n",
    "            logger.warning(\"No documents were found in the knowledge base ...\")\n",
    "            self.vector_store = None\n",
    "            self.is_initialized = True\n",
    "            return\n",
    "\n",
    "        # Split the documents into chunks\n",
    "        split_docs = self.text_splitter.split_documents(documents)\n",
    "        logger.info(f\"Split {len(documents)} documents into {len(split_docs)} chunks\")\n",
    "\n",
    "        # Create the vector store with progress tracking\n",
    "        logger.info(\"Creating embeddings (this may take a while due to rate limiting)...\")\n",
    "        \n",
    "        try:\n",
    "            self.vector_store = await self._create_vector_store_with_progress(split_docs)\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to create vector store: {e}\")\n",
    "            raise\n",
    "\n",
    "        # ALWAYS SAVE THE VECTOR STORE AFTER CREATION\n",
    "        if self.auto_save_vector_store:\n",
    "            await self.save_vector_store(self.vector_store_path)\n",
    "            logger.info(f\"Vector store automatically saved to {self.vector_store_path}\")\n",
    "\n",
    "        self.is_initialized = True\n",
    "        logger.info(\"Self-RAG system initialized successfully\")\n",
    "\n",
    "    async def _create_vector_store_with_progress(self, documents: List[Document]) -> FAISS:\n",
    "        \n",
    "        logger.info(f\"Creating embeddings for {len(documents)} documents...\")\n",
    "        texts = [doc.page_content for doc in documents]\n",
    "        embeddings_list = await self.embeddings.aembed_documents(texts)\n",
    "        \n",
    "        # Convert embeddings to numpy array\n",
    "        embedding_matrix = np.array(embeddings_list).astype('float32')\n",
    "        \n",
    "        # Create FAISS index\n",
    "        dimension = embedding_matrix.shape[1]\n",
    "        index = faiss.IndexFlatL2(dimension)\n",
    "        index.add(embedding_matrix)\n",
    "        \n",
    "        # Create docstore and index mapping\n",
    "        docstore = InMemoryDocstore({str(i): doc for i, doc in enumerate(documents)})\n",
    "        index_to_docstore_id = {i: str(i) for i in range(len(documents))}\n",
    "        \n",
    "        # Create FAISS vector store\n",
    "        vector_store = FAISS(\n",
    "            embedding_function=self.embeddings.base_embeddings,\n",
    "            index=index,\n",
    "            docstore=docstore,\n",
    "            index_to_docstore_id=index_to_docstore_id\n",
    "        )\n",
    "        \n",
    "        logger.info(\"Vector store created successfully\")\n",
    "        return vector_store\n",
    "\n",
    "    async def _should_retrieve(self, query: str) -> Tuple[bool, str]:\n",
    "        \"\"\" Determine if the retrieval is needed and get reasoning\"\"\"\n",
    "        chain = self.retrieval_decision_prompt | self.llm.with_config(callbacks=[self.rate_limit_callback])\n",
    "\n",
    "        result = await chain.ainvoke({\"query\": query})\n",
    "\n",
    "        # Parse the result\n",
    "        lines = result.content.strip().split('\\n')\n",
    "        reasoning = \"\"\n",
    "        decision = False\n",
    "\n",
    "        for line in lines:\n",
    "            if line.startswith(\"Reasoning:\"):\n",
    "                reasoning = line.replace(\"Reasoning:\", \"\").strip()\n",
    "            elif line.startswith(\"Decision:\"):\n",
    "                decision_text = line.replace(\"Decision:\", \"\").strip()\n",
    "                decision = \"RETRIEVE\" in decision_text.upper()\n",
    "        \n",
    "        return decision, reasoning\n",
    "\n",
    "    async def _retrieve_documents(self, query: str, k:int = 5) -> List[Document]:\n",
    "        \"\"\"Retrieve relevant documents\"\"\"\n",
    "        if not self.vector_store:\n",
    "            return []\n",
    "\n",
    "        # Use similarity search with scores\n",
    "        doc_with_scores = await self.vector_store.asimilarity_search_with_score(query, k = k)\n",
    "        # Filter by relevance score\n",
    "        relevant_docs = [doc for doc, score in doc_with_scores if score > 0.8]\n",
    "\n",
    "        return relevant_docs\n",
    "\n",
    "    async def _generate_answer_with_retrieval(self, query: str, documents: List[Document]) -> str:\n",
    "        \"\"\"Generate answers using retrieved documents\"\"\"\n",
    "        context = \"\\n\\n\".join([\n",
    "            f\"Document: {doc.metadata.get('file_name', 'Unknown')}\\n{doc.page_content}\"\n",
    "            for doc in documents\n",
    "        ])\n",
    "        \n",
    "        chain = self.rag_prompt | self.llm | StrOutputParser()\n",
    "        \n",
    "        result = await chain.ainvoke({\n",
    "            \"context\": context,\n",
    "            \"question\": query\n",
    "        })\n",
    "        \n",
    "        return result\n",
    "        \n",
    "    async def _generate_answer_without_retrieval(self, query: str) -> str:\n",
    "        \"\"\"Generate answer without retrieval\"\"\"\n",
    "        chain = self.no_retrieval_prompt | self.llm | StrOutputParser()\n",
    "        \n",
    "        result = await chain.ainvoke({\"question\": query})\n",
    "        return result\n",
    "\n",
    "    async def _reflect_on_answer(self, query: str, answer: str, documents: List[Document]) -> float:\n",
    "        \"\"\"Reflect on answer quality\"\"\"\n",
    "        context = \"\\n\\n\".join([doc.page_content for doc in documents]) if documents else \"No context provided\"\n",
    "        \n",
    "        chain = self.reflection_prompt | self.llm.with_config(callbacks=[self.rate_limit_callback])\n",
    "        \n",
    "        result = await chain.ainvoke({\n",
    "            \"query\": query,\n",
    "            \"answer\": answer,\n",
    "            \"context\": context\n",
    "        })\n",
    "        \n",
    "        try:\n",
    "            content = result.content if hasattr(result, 'content') else str(result)\n",
    "            score_str = content.strip().split('\\n')[-1]\n",
    "            \n",
    "            # Extract digits and decimal point from the score string\n",
    "            score_chars = ''.join(c for c in score_str if c.isdigit() or c == '.')\n",
    "            \n",
    "            if score_chars:\n",
    "                score = float(score_chars)\n",
    "                return min(10.0, max(0.0, score))  # Clamp between 0 and 10\n",
    "            else:\n",
    "                logger.warning(f\"No numeric score found in: {score_str}\")\n",
    "                return 5.0\n",
    "                \n",
    "        except (ValueError, IndexError) as e:\n",
    "            logger.warning(f\"Could not parse reflection score: {result}. Error: {e}\")\n",
    "            return 5.0\n",
    "\n",
    "    async def query(self, query: str, force_retrieval: bool = False) -> SelfRAGResponse:\n",
    "        \"\"\"Process query using the SelfRAG\"\"\"\n",
    "        if not self.is_initialized:\n",
    "            await self.initialize()\n",
    "\n",
    "        # Decide whether we need retrieval\n",
    "        if force_retrieval:\n",
    "            needs_retrieval = True\n",
    "            reasoning = \"Forced retrieval requested\"\n",
    "        else:\n",
    "            needs_retrieval, reasoning = await self._should_retrieve(query)\n",
    "\n",
    "        retrieved_docs = []\n",
    "        citations = []\n",
    "\n",
    "        # Retrieve documents if needed\n",
    "        if needs_retrieval and self.vector_store:\n",
    "            retrieved_docs = await self._retrieve_documents(query)\n",
    "            citations = [\n",
    "                doc.metadata.get('file_path', f\"Document {i}\")\n",
    "                for i, doc in enumerate(retrieved_docs)\n",
    "            ]\n",
    "\n",
    "        # Generate answer\n",
    "        if retrieved_docs:\n",
    "            answer = await self._generate_answer_with_retrieval(query, retrieved_docs)\n",
    "        else:\n",
    "            answer = await self._generate_answer_without_retrieval(query)\n",
    "\n",
    "        # Reflect on answer quality\n",
    "        reflection_score = await self._reflect_on_answer(query, answer, retrieved_docs)\n",
    "\n",
    "        return SelfRAGResponse(\n",
    "            answer=answer,\n",
    "            retrieved_docs=retrieved_docs,\n",
    "            reflection_score=reflection_score,\n",
    "            needs_retrieval=needs_retrieval,\n",
    "            citations=list(set(citations)),  # Remove duplicates\n",
    "            retrieval_decision_reasoning=reasoning\n",
    "        )\n",
    "\n",
    "    async def save_vector_store(self, path: str = None):\n",
    "        \"\"\"Save vector store to disk\"\"\"\n",
    "        if not self.vector_store:\n",
    "            logger.warning(\"No vector store to save\")\n",
    "            return\n",
    "            \n",
    "        save_path = path or self.vector_store_path\n",
    "        \n",
    "        # Create directory if it doesn't exist\n",
    "        os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "        \n",
    "        try:\n",
    "            self.vector_store.save_local(save_path)\n",
    "            logger.info(f\"Vector store saved to {save_path}\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to save vector store: {e}\")\n",
    "            raise\n",
    "    \n",
    "    async def load_vector_store(self, path: str = None):\n",
    "        from langchain_community.vectorstores import FAISS\n",
    "        load_path = path or self.vector_store_path\n",
    "        try:\n",
    "            self.vector_store = FAISS.load_local(\n",
    "                load_path,\n",
    "                self.embeddings.base_embeddings,\n",
    "                allow_dangerous_deserialization=True\n",
    "            )\n",
    "            if not isinstance(self.vector_store, FAISS):\n",
    "                raise ValueError(\"Loaded vector store is not a valid FAISS object\")\n",
    "            self.is_initialized = True\n",
    "            logger.info(f\"Vector store loaded from {load_path}\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to load vector store from {load_path}: {e}\")\n",
    "            self.vector_store = None\n",
    "            raise\n",
    "\n",
    "    async def rebuild_vector_store(self):\n",
    "        logger.info(\"Force rebuilding vector store...\")\n",
    "        self.vector_store = None\n",
    "        await self.initialize(force_rebuild=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7bab832c",
   "metadata": {
    "cellUniqueIdByVincent": "83e4e"
   },
   "outputs": [],
   "source": [
    "class BatchProcessor:\n",
    "    \"\"\"Process multiple queries in batch with concurrency control\"\"\"\n",
    "    \n",
    "    def __init__(self, rag_system: RAGSystem, max_concurrent: int = 3):\n",
    "        self.rag_system = rag_system\n",
    "        self.semaphore = asyncio.Semaphore(max_concurrent)\n",
    "    \n",
    "    async def process_query(self, query: str) -> Tuple[str, SelfRAGResponse]:\n",
    "        \"\"\"Process a single query with semaphore\"\"\"\n",
    "        async with self.semaphore:\n",
    "            response = await self.rag_system.query(query)\n",
    "            return query, response\n",
    "    \n",
    "    async def process_batch(self, queries: List[str]) -> Dict[str, SelfRAGResponse]:\n",
    "        \"\"\"Process multiple queries concurrently\"\"\"\n",
    "        tasks = [self.process_query(query) for query in queries]\n",
    "        results = await asyncio.gather(*tasks, return_exceptions=True)\n",
    "        \n",
    "        output = {}\n",
    "        for result in results:\n",
    "            if isinstance(result, Exception):\n",
    "                logger.error(f\"Error processing query: {result}\")\n",
    "            else:\n",
    "                query, response = result\n",
    "                output[query] = response\n",
    "        \n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "be260d94",
   "metadata": {
    "cellUniqueIdByVincent": "4beb4"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Initializing the RAG system ...\n",
      "INFO:__main__:Loaded 3 documents from books\n",
      "INFO:__main__:Split 3 documents into 170 chunks\n",
      "INFO:__main__:Creating embeddings (this may take a while due to rate limiting)...\n",
      "INFO:__main__:Creating embeddings for 170 documents...\n",
      "INFO:__main__:Processing 170 texts in 2 batches of 96\n",
      "INFO:__main__:Processing batch 1/2 (96 texts)\n",
      "INFO:httpx:HTTP Request: POST https://api.cohere.com/v1/embed \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Processing batch 2/2 (74 texts)\n",
      "INFO:httpx:HTTP Request: POST https://api.cohere.com/v1/embed \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Vector store created successfully\n",
      "INFO:__main__:Vector store saved to books/vector_store\n",
      "INFO:__main__:Vector store automatically saved to books/vector_store\n",
      "INFO:__main__:Self-RAG system initialized successfully\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "Query: Who is Chris Olande?\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.cohere.com/v1/embed \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Needs Retrieval: True\n",
      "Reasoning: The query seeks specific information about an individual that is not commonly known, suggesting the need for external sources to provide an accurate answer.\n",
      "Retrieved Documents: 5\n",
      "Reflection Score: 9.0/10\n",
      "Citations: ['books/declaration_of_independence_of_the_united_states.txt', 'books/olande.txt']\n",
      "\n",
      "Answer:\n",
      "According to the provided context, specifically the document \"olande.txt\", Chris Olande is a dynamic and intellectually curious student of Statistics and Programming at Kenyatta University. He is described as having a growing portfolio of sophisticated projects that blend statistical rigor with cutting-edge machine learning techniques (olande.txt).\n",
      "\n",
      "As a student, Chris has demonstrated a strong foundation in statistics, complemented by a robust understanding of programming, especially in Python. His technical expertise includes proficiency in various libraries such as NumPy, pandas, matplotlib, seaborn, scikit-learn, PyTorch, and Hugging Face Transformers (olande.txt). He has applied his knowledge in projects involving exploratory factor analysis, hypothesis testing, regression modeling, and time series analysis, showcasing his ability to work across the full data science pipeline (olande.txt).\n",
      "\n",
      "Chris has also demonstrated leadership in education by organizing a field study for Grade 7 students, reflecting his commitment to nurturing curiosity and STEM literacy among younger learners (olande.txt). His work in this domain underscores a broader commitment to education as a vehicle for empowerment and societal progress.\n",
      "\n",
      "In terms of his professional traits and work ethic, Chris is described as methodical, results-oriented, and deeply inquisitive, with a strong ability to approach problems analytically and balance statistical rigor with creative thinking (olande.txt). His code is clean, well-documented, and adheres to best practices in software development, making him a reliable and collaborative team player (olande.txt).\n",
      "\n",
      "The context suggests that Chris is well-positioned for roles in data science, machine learning engineering, research, or AI systems development, with a potential future in intelligent information systems and AI product development (olande.txt). Overall, Chris Olande is portrayed as a promising young professional with a unique blend of technical competence, educational leadership, and forward-thinking innovation (olande.txt).\n",
      "\n",
      "It's worth noting that the document \"declaration_of_independence_of_the_united_states.txt\" does not provide any relevant information about Chris Olande, as it appears to be a historical document unrelated to his profile. \n",
      "\n",
      "In summary, based on the provided context, Chris Olande is a talented and ambitious student with a strong foundation in statistics and programming, a passion for machine learning and education, and a promising career trajectory in the tech industry.\n",
      "\n",
      "================================================================================\n",
      "Query: What has Chris Olande studied at Kenyatta University?\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.cohere.com/v1/embed \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Needs Retrieval: True\n",
      "Reasoning: The query is specific to an individual's educational background, which typically requires consulting external sources such as university records, personal profiles, or other documents that list educational achievements. This information is not usually part of general knowledge and necessitates retrieval from specialized or up-to-date sources.\n",
      "Retrieved Documents: 5\n",
      "Reflection Score: 9.0/10\n",
      "Citations: ['books/declaration_of_independence_of_the_united_states.txt', 'books/olande.txt']\n",
      "\n",
      "Answer:\n",
      "Based on the provided context, specifically the document \"olande.txt\", Chris Olande has studied Statistics and Programming at Kenyatta University. This information is mentioned in the introduction of the document, which states: \"Chris Olande is a dynamic and intellectually curious student of Statistics and Programming at Kenyatta University...\" (olande.txt). \n",
      "\n",
      "Additionally, the document highlights his academic background and technical expertise in areas such as Python programming, data science, and machine learning, which are likely part of his studies at the university (olande.txt). However, the context does not provide a detailed list of specific courses or subjects he has studied beyond these general areas. \n",
      "\n",
      "It is also worth noting that the other provided document, \"declaration_of_independence_of_the_united_states.txt\", does not contain any relevant information about Chris Olande's studies.\n",
      "\n",
      "================================================================================\n",
      "Query: What is the central conflict in Romeo and Juliet?\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.cohere.com/v1/embed \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Needs Retrieval: True\n",
      "Reasoning: The query \"What is the central conflict in Romeo and Juliet?\" asks for specific information about a well-known literary work. This information is generally considered part of common knowledge or can be found within the text itself, but it is also domain-specific (literature) and could benefit from external documents or a knowledge base for a more detailed or accurate explanation. However, the central conflict in Romeo and Juliet is a basic aspect of the play that is widely known and taught, suggesting it doesn't necessarily require recent or highly specialized information to answer. The question seeks an understanding or summary of the play's core theme, which can be answered based on general knowledge without needing to retrieve the most current or external information.\n",
      "Retrieved Documents: 5\n",
      "Reflection Score: 9.0/10\n",
      "Citations: ['books/romeo_and_juliet.txt']\n",
      "\n",
      "Answer:\n",
      "The central conflict in Romeo and Juliet is the feud between the two rival families, the Montagues and the Capulets, which ultimately leads to the tragic demise of the two lovers. This conflict is evident throughout the provided context, particularly in the Dramatis Personæ section of the document \"romeo_and_juliet.txt\", which lists the characters and their affiliations, highlighting the division between the two families.\n",
      "\n",
      "The Prologue in the same document sets the tone for the conflict, stating that \"Two households, both alike in dignity, / In fair Verona, where we lay our scene, / From ancient grudge break to new mutiny\" (Document: romeo_and_juliet.txt). This ancient grudge between the Montagues and Capulets is the root of the central conflict, which drives the plot and leads to the tragic events that unfold.\n",
      "\n",
      "The conflict is further emphasized in Act 1, Scene 1, where Sampson and Gregory, servants of the Capulets, discuss their hatred for the Montagues, saying \"I mean, if we be in choler, we’ll draw\" (Document: romeo_and_juliet.txt). This scene showcases the tension and hostility between the two families, which ultimately leads to the tragic confrontation between Tybalt and Romeo.\n",
      "\n",
      "The star-crossed lovers, Romeo and Juliet, find themselves caught in the midst of this conflict, and their love becomes a victim of the feud. As Juliet says, \"My only love sprung from my only hate! / Too early seen unknown, and known too late! / Prodigious birth of love it is to me, / That I must love a loathed enemy\" (Document: romeo_and_juliet.txt). This quote highlights the tragic nature of their love, which is forbidden due to the conflict between their families.\n",
      "\n",
      "While the provided context does contain sufficient information to identify the central conflict, it is worth noting that the full tragic consequences of this conflict are not fully explored within the given documents. However, based on the information available, it is clear that the feud between the Montagues and Capulets is the central conflict that drives the plot of Romeo and Juliet.\n",
      "\n",
      "In summary, the central conflict in Romeo and Juliet is the long-standing feud between the Montagues and Capulets, which leads to the tragic demise of the two lovers. This conflict is evident throughout the provided context, particularly in the Dramatis Personæ, Prologue, and Act 1, Scene 1, of the document \"romeo_and_juliet.txt\".\n",
      "\n",
      "================================================================================\n",
      "Query: How does Shakespeare portray love and fate in Romeo and Juliet?\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.cohere.com/v1/embed \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Needs Retrieval: True\n",
      "Reasoning: The query necessitates a deep dive into literary analysis and critique, which typically involves referencing specific texts, scholarly articles, or study guides. This goes beyond general knowledge and requires specialized information that is best retrieved from external sources.\n",
      "Retrieved Documents: 5\n",
      "Reflection Score: 8.0/10\n",
      "Citations: ['books/romeo_and_juliet.txt']\n",
      "\n",
      "Answer:\n",
      "In the provided context of Romeo and Juliet, Shakespeare portrays love and fate as intertwined and inevitable forces that drive the plot and characters' actions. \n",
      "\n",
      "Love is depicted as a powerful and all-consuming emotion, as seen in Juliet's passionate speech in romeo_and_juliet.txt, where she exclaims, \"Gallop apace, you fiery-footed steeds, / Towards Phoebus' lodging... Come, civil night, / Thou sober-suited matron, all in black, / And learn me how to lose a winning match, / Play'd for a pair of stainless maidenhoods\" (Document: romeo_and_juliet.txt). This passage showcases Juliet's intense longing for Romeo and her desire to be with him, highlighting the overwhelming nature of their love.\n",
      "\n",
      "Fate, on the other hand, is portrayed as a force that is beyond the characters' control. In the same document, Friar Lawrence's plan to help Juliet escape her arranged marriage to Paris is revealed, where he says, \"Take thou this vial, being then in bed, / And this distilled liquor drink thou off, / When presently through all thy veins shall run / A cold and drowsy humour; for no pulse / Shall keep his native progress, but surcease\" (Document: romeo_and_juliet.txt). This plan ultimately leads to a series of tragic events, suggesting that the characters' attempts to defy fate are doomed to fail.\n",
      "\n",
      "The Chorus's speech in ACT II of romeo_and_juliet.txt also highlights the role of fate in the story, stating, \"Now old desire doth in his deathbed lie, / And young affection gapes to be his heir; / That fair for which love groan'd for and would die, / With tender Juliet match'd, is now not fair\" (Document: romeo_and_juliet.txt). This passage implies that the characters' love is destined for tragedy, and that their fate is sealed from the beginning.\n",
      "\n",
      "Furthermore, the conversation between Juliet and her Nurse in romeo_and_juliet.txt, where the Nurse reveals Romeo's banishment and Tybalt's death, showcases the devastating consequences of the characters' actions and the inevitability of their fate. Juliet's reaction to the news, where she exclaims, \"O, break, my heart. Poor bankrout, break at once. / To prison, eyes; ne'er look on liberty\" (Document: romeo_and_juliet.txt), highlights the tragic nature of their love and the fate that awaits them.\n",
      "\n",
      "While the provided context offers valuable insights into Shakespeare's portrayal of love and fate in Romeo and Juliet, it is essential to acknowledge that the context is limited to specific scenes and characters. A more comprehensive analysis of the play would require additional context and general knowledge of the story. Nevertheless, based on the provided documents, it is clear that Shakespeare depicts love and fate as intertwined forces that drive the plot and characters' actions, ultimately leading to a tragic conclusion.\n",
      "\n",
      "================================================================================\n",
      "Query: What are the main ideas expressed in the Declaration of Independence?\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.cohere.com/v1/embed \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:__main__:No numeric score found in: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Needs Retrieval: True\n",
      "Reasoning: The query requires detailed information about a historical document that, while widely known, necessitates access to the document itself or scholarly interpretations to accurately identify its main ideas. This goes beyond general knowledge and suggests the need for external knowledge retrieval to provide a comprehensive answer.\n",
      "Retrieved Documents: 5\n",
      "Reflection Score: 5.0/10\n",
      "Citations: ['books/declaration_of_independence_of_the_united_states.txt']\n",
      "\n",
      "Answer:\n",
      "The main ideas expressed in the Declaration of Independence, as outlined in the provided context documents, particularly in \"declaration_of_independence_of_the_united_states.txt,\" can be summarized as follows:\n",
      "\n",
      "1. **Necessity of Separation**: The document begins by stating that when a people find it necessary to dissolve the political bands connecting them to another, they should declare the causes for this separation (Document: declaration_of_independence_of_the_united_states.txt). This introduction sets the stage for the colonies' decision to separate from Great Britain.\n",
      "\n",
      "2. **Petitions for Redress and Repeated Injuries**: The colonies have petitioned for redress in humble terms, but these petitions were met with repeated injuries (Document: declaration_of_independence_of_the_united_states.txt). This indicates that the colonies attempted to resolve their grievances peacefully but were ignored or mistreated.\n",
      "\n",
      "3. **Unfitness of the Prince**: The character of the Prince (referring to the British monarch) is marked by acts that define a tyrant, making him unfit to rule a free people (Document: declaration_of_independence_of_the_united_states.txt). This critique of the monarch's behavior is a key justification for the separation.\n",
      "\n",
      "4. **Attention to British Brethren**: The colonies have warned their British brethren about attempts to extend unwarrantable jurisdiction over them and have appealed to their sense of justice and common kindred (Document: declaration_of_independence_of_the_united_states.txt). Despite these efforts, the British have been \"deaf to the voice of justice and of consanguinity,\" leading to the necessity of separation.\n",
      "\n",
      "5. **Declaration of Independence**: The Representatives of the United States of America, in General Congress, Assembled, declare that the United Colonies are, and of right ought to be, free and independent states (Document: declaration_of_independence_of_the_united_states.txt). This declaration is the central statement of the document, asserting the colonies' right to independence.\n",
      "\n",
      "6. **Absolution from Allegiance**: The colonies are absolved from all allegiance to the British Crown, and all political connection between them and the State of Great Britain is to be totally dissolved (Document: declaration_of_independence_of_the_united_states.txt). This marks a complete break with Great Britain.\n",
      "\n",
      "7. **Powers of Independent States**: As free and independent states, the colonies have the full power to levy war, conclude peace, contract alliances, establish commerce, and perform all other acts that independent states may do (Document: declaration_of_independence_of_the_united_states.txt). This outlines the scope of their newfound independence.\n",
      "\n",
      "8. **Mutual Pledge**: The signers mutually pledge to each other their lives, fortunes, and sacred honor in support of this declaration, with a firm reliance on the protection of Divine Providence (Document: declaration_of_independence_of_the_united_states.txt). This pledge underscores the seriousness and commitment of the signers to the cause of independence.\n",
      "\n",
      "These ideas, as expressed in the Declaration of Independence, form the foundation of the United States' assertion of independence from Great Britain and its establishment as a sovereign nation. The context provided does not require supplementation with general knowledge to understand the main ideas, as the documents themselves clearly outline the rationale and declaration of independence.\n",
      "\n",
      "================================================================================\n",
      "Query: How does the Declaration justify the colonies break from Britain?\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.cohere.com/v1/embed \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Needs Retrieval: True\n",
      "Reasoning: The query \"How does the Declaration justify the colonies break from Britain?\" requires an understanding of historical context and the content of the Declaration of Independence. To answer this question, one needs to have knowledge of the document's purpose, its key points, and the reasoning behind the colonies' decision to separate from Britain. This query asks for specific, domain-specific information related to history and political documents.\n",
      "Retrieved Documents: 5\n",
      "Reflection Score: 9.0/10\n",
      "Citations: ['books/declaration_of_independence_of_the_united_states.txt']\n",
      "\n",
      "Answer:\n",
      "The Declaration of Independence justifies the colonies' break from Britain by citing the principles of equality, liberty, and the pursuit of happiness, as well as the colonies' right to self-governance. According to the document \"declaration_of_independence_of_the_united_states.txt\" (multiple versions), the Declaration states that \"all men are created equal\" and are endowed with \"certain unalienable Rights\" such as \"Life, Liberty, and the pursuit of Happiness\" (Document: declaration_of_independence_of_the_united_states.txt).\n",
      "\n",
      "The Declaration argues that governments derive their power from the consent of the governed and that when a government becomes destructive of these ends, it is the right of the people to alter or abolish it (Document: declaration_of_independence_of_the_united_states.txt). The document claims that the British government has become tyrannical, with the King of Great Britain having a history of \"repeated injuries and usurpations\" aimed at establishing \"an absolute Tyranny over these States\" (Document: declaration_of_independence_of_the_united_states.txt).\n",
      "\n",
      "The Declaration lists specific grievances against the King, including his refusal to assent to laws necessary for the public good, forbidding governors from passing laws without his consent, and neglecting to attend to suspended laws (Document: declaration_of_independence_of_the_united_states.txt). These grievances demonstrate a pattern of abuse and usurpation, which the Declaration argues justifies the colonies' separation from Britain.\n",
      "\n",
      "Furthermore, the Declaration asserts that the colonies have petitioned for redress in humble terms, but their repeated petitions have been met with repeated injury, demonstrating the British government's unwillingness to listen to their concerns (Document: declaration_of_independence_of_the_united_states.txt). The Declaration concludes that the colonies must \"acquiesce in the necessity, which denounces our Separation\" and hold the British as \"Enemies in War, in Peace Friends\" (Document: declaration_of_independence_of_the_united_states.txt).\n",
      "\n",
      "In summary, the Declaration of Independence justifies the colonies' break from Britain by arguing that the British government has become tyrannical, abusive, and unresponsive to the colonies' concerns, and that the colonies have the right to self-governance and to establish a new government based on the principles of equality, liberty, and the pursuit of happiness.\n",
      "\n",
      "Note: The provided context documents are multiple versions of the Declaration of Independence, which contain similar information. The answer is based primarily on these documents, and general knowledge is not required to supplement the context.\n",
      "\n",
      "============================================================\n",
      "BATCH PROCESSING EXAMPLE\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.cohere.com/v1/embed \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:langchain_cohere.utils:Retrying langchain_cohere.embeddings.CohereEmbeddings.aembed_with_retry.<locals>._embed_with_retry in 4.0 seconds as it raised RemoteProtocolError: Server disconnected without sending a response..\n",
      "INFO:httpx:HTTP Request: POST https://api.cohere.com/v1/embed \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.cohere.com/v1/embed \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.cohere.com/v1/embed \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Batch Query: Who is Chris Olande?\n",
      "Reflection Score: 9.0/10\n",
      "Answer Length: 2128 characters\n",
      "\n",
      "Batch Query: What has Chris Olande studied at Kenyatta University?\n",
      "Reflection Score: 9.0/10\n",
      "Answer Length: 1026 characters\n",
      "\n",
      "Batch Query: What is the central conflict in Romeo and Juliet?\n",
      "Reflection Score: 9.0/10\n",
      "Answer Length: 2229 characters\n",
      "\n",
      "Batch Query: How does Shakespeare portray love and fate in Romeo and Juliet?\n",
      "Reflection Score: 8.0/10\n",
      "Answer Length: 2705 characters\n"
     ]
    }
   ],
   "source": [
    "async def main():\n",
    "    rag_system = RAGSystem(\n",
    "    cohere_api_key=os.getenv(\"COHERE_API_KEY\"),\n",
    "    openrouter_api_key=os.getenv(\"OPENROUTER_API_KEY\"),\n",
    "    kb_folder=\"books\"\n",
    "    )\n",
    "    \n",
    "    # Initialize the system\n",
    "    await rag_system.initialize()\n",
    "    \n",
    "    \n",
    "    queries = [\n",
    "    \"Who is Chris Olande?\",\n",
    "    \"What has Chris Olande studied at Kenyatta University?\",\n",
    "    \n",
    "    # Romeo and Juliet\n",
    "    \"What is the central conflict in Romeo and Juliet?\",\n",
    "    \"How does Shakespeare portray love and fate in Romeo and Juliet?\",\n",
    "\n",
    "    # Declaration of Independence\n",
    "    \"What are the main ideas expressed in the Declaration of Independence?\",\n",
    "    \"How does the Declaration justify the colonies break from Britain?\"\n",
    "]\n",
    "    \n",
    "    # Process queries individually\n",
    "    for query in queries:\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"Query: {query}\")\n",
    "        print('='*80)\n",
    "        \n",
    "        try:\n",
    "            response = await rag_system.query(query)\n",
    "            \n",
    "            print(f\"Needs Retrieval: {response.needs_retrieval}\")\n",
    "            print(f\"Reasoning: {response.retrieval_decision_reasoning}\")\n",
    "            print(f\"Retrieved Documents: {len(response.retrieved_docs)}\")\n",
    "            print(f\"Reflection Score: {response.reflection_score}/10\")\n",
    "            \n",
    "            if response.citations:\n",
    "                print(f\"Citations: {response.citations}\")\n",
    "            \n",
    "            print(f\"\\nAnswer:\\n{response.answer}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing query: {e}\")\n",
    "    \n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"BATCH PROCESSING EXAMPLE\")\n",
    "    print('='*60)\n",
    "    \n",
    "    batch_processor = BatchProcessor(rag_system, max_concurrent=3)\n",
    "    batch_queries = queries[:4]  # Process first 4 queries in batch\n",
    "    \n",
    "    batch_results = await batch_processor.process_batch(batch_queries)\n",
    "    \n",
    "    for query, response in batch_results.items():\n",
    "        print(f\"\\nBatch Query: {query}\")\n",
    "        print(f\"Reflection Score: {response.reflection_score}/10\")\n",
    "        print(f\"Answer Length: {len(response.answer)} characters\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    await main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "118052ee",
   "metadata": {
    "cellUniqueIdByVincent": "6d452"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  },
  "vincent": {
   "sessionId": "3f2153659d5b596a4189cf1d_2025-05-30T15-07-59-793Z"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
