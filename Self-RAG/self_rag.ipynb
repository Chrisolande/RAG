{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellUniqueIdByVincent": "db184"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import asyncio\n",
    "import logging \n",
    "from pathlib import Path\n",
    "from typing import Tuple, Any, List, Dict, Optional\n",
    "from dataclasses import dataclass\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from langchain_cohere import CohereEmbeddings\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.document_loaders import (\n",
    "    TextLoader, \n",
    "    UnstructuredMarkdownLoader,\n",
    "    JSONLoader,\n",
    "    UnstructuredHTMLLoader,\n",
    "    PyPDFLoader\n",
    ")\n",
    "\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.schema import Document\n",
    "from langchain.prompts import ChatPromptTemplate, PromptTemplate\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "from langchain_core.runnables import RunnableParallel, RunnableLambda\n",
    "from langchain.callbacks.base import AsyncCallbackHandler\n",
    "from langchain.schema import LLMResult\n",
    "\n",
    "load_dotenv()\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cellUniqueIdByVincent": "dbac3"
   },
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class SelfRAGResponse:\n",
    "    \"\"\"Complete self rag with reflection\"\"\"\n",
    "    answer: str\n",
    "    retrieved_docs: List[Document]\n",
    "    reflection_score: float\n",
    "    needs_retrieval: bool\n",
    "    citations: List[str]\n",
    "    retrieval_decision_reasoning: str\n",
    "\n",
    "\n",
    "class RateLimitCallback(AsyncCallbackHandler):\n",
    "    \"\"\"Callback handler to manage API rate limiting with semaphores\"\"\"\n",
    "    \n",
    "    def __init__(self, semaphore: asyncio.Semaphore):\n",
    "        self.semaphore = semaphore\n",
    "        \n",
    "    async def on_llm_start(self, serialized: Dict[str, Any], prompts: List[str], **kwargs: Any) -> None:\n",
    "        await self.semaphore.acquire()\n",
    "        \n",
    "    async def on_llm_end(self, response: LLMResult, **kwargs: Any) -> None:\n",
    "        self.semaphore.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cellUniqueIdByVincent": "a00b3"
   },
   "outputs": [],
   "source": [
    "class DocumentLoader:\n",
    "    def __init__(self):\n",
    "        self.loaders = {\n",
    "            '.txt': TextLoader,\n",
    "            '.md': UnstructuredMarkdownLoader,\n",
    "            '.json': self._create_json_loader,\n",
    "            '.html': UnstructuredHTMLLoader,\n",
    "            '.py': TextLoader,\n",
    "            '.js': TextLoader,\n",
    "            '.css': TextLoader,\n",
    "            '.pdf': PyPDFLoader\n",
    "        }\n",
    "\n",
    "    def _create_json_loader(self, file_path: str):\n",
    "        \"\"\"Create JSON loader with custom jq_schema\"\"\"\n",
    "        return JSONLoader(\n",
    "            file_path=file_path,\n",
    "            jq_schema='.[]',\n",
    "            text_content=False\n",
    "        )\n",
    "\n",
    "    async def load_documents(self, kb_folder: str) -> List[Document]:\n",
    "        \"\"\"Load all documents from the knowledge base folder\"\"\"\n",
    "        documents = []\n",
    "        kb_path = Path(kb_folder)\n",
    "\n",
    "        if not kb_path.exists():\n",
    "            raise FileNotFoundError(f\"Knowledge base folder not found: {kb_path}\")\n",
    "\n",
    "        for file_path in kb_path.glob(\"**/*\"):\n",
    "            if file_path.is_file() and file_path.suffix.lower() in self.loaders:\n",
    "                try:\n",
    "                    loader_class = self.loaders[file_path.suffix.lower()]\n",
    "\n",
    "                    if file_path.suffix.lower() == \".json\":\n",
    "                        loader = loader_class(str(file_path))\n",
    "                    else:\n",
    "                        loader = loader_class(str(file_path))\n",
    "\n",
    "                    docs = loader.load()\n",
    "\n",
    "                    # Add metadata\n",
    "                    for doc in docs:\n",
    "                        doc.metadata.update({\n",
    "                            'file_path': str(file_path),\n",
    "                            'file_type': file_path.suffix,\n",
    "                            'file_name': file_path.name\n",
    "                        })\n",
    "\n",
    "                    documents.extend(docs)\n",
    "                \n",
    "                except Exception as e:\n",
    "                    logger.warning(f\"There was an error loading the knowledge base: {str(e)}\")\n",
    "                    # Fallback to TextLoader for unknown formats\n",
    "                    try:\n",
    "                        loader = TextLoader(str(file_path))\n",
    "                        docs = loader.load()\n",
    "                        # Add metadata\n",
    "                        for doc in docs:\n",
    "                            doc.metadata.update({\n",
    "                                'file_path': str(file_path),\n",
    "                                'file_type': file_path.suffix,\n",
    "                                'file_name': file_path.name\n",
    "                            })\n",
    "\n",
    "                        documents.extend(docs)\n",
    "\n",
    "                    except Exception as fallback_error:\n",
    "                        logger.error(f\"Failed to load {file_path} with fallback: {fallback_error}\")\n",
    "        \n",
    "        logger.info(f\"Loaded {len(documents)} documents from {kb_folder}\")\n",
    "        return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellUniqueIdByVincent": "fb77a"
   },
   "outputs": [],
   "source": [
    "from asyncio import Semaphore\n",
    "@dataclass\n",
    "class RAGSystem:\n",
    "    cohere_api_key: str\n",
    "    openrouter_api_key: str\n",
    "    kb_folder: str\n",
    "    max_concurrent_requests: int = 5\n",
    "    chunk_size: int = 2000\n",
    "    chunk_overlap: int = 200\n",
    "\n",
    "    def __post_init__(self):\n",
    "        # Initialize the components\n",
    "        self.embeddings = CohereEmbeddings(model = \"embed-v4.0\",\n",
    "                                         cohere_api_key = os.getenv(\"COHERE_API_KEY\"))\n",
    "\n",
    "        self.llm = ChatOpenAI(\n",
    "            model=\"meta-llama/llama-3.3-70b-instruct\",\n",
    "            openai_api_key=self.openrouter_api_key,\n",
    "            openai_api_base=\"https://openrouter.ai/api/v1\",\n",
    "            temperature=0.6,\n",
    "            max_tokens=1500\n",
    "        )\n",
    "\n",
    "        # Text Splitter\n",
    "        self.text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size = self.chunk_size,\n",
    "            chunk_overlap = self.chunk_overlap,\n",
    "            length_function = len\n",
    "        )\n",
    "\n",
    "        self.document_loader = DocumentLoader()\n",
    "\n",
    "        # Vector store\n",
    "        self.vector_store: Optional[FAISS] = None\n",
    "        #self.kb_folder = kb_folder\n",
    "\n",
    "        # Semaphores for rate limiting\n",
    "        self.llm_semaphore = Semaphore(self.max_concurrent_requests)\n",
    "        self.embeddings_semaphore = Semaphore(self.max_concurrent_requests)\n",
    "\n",
    "        self.is_initialized = False\n",
    "\n",
    "        # Set up prompts\n",
    "        self._setup_prompts()\n",
    "\n",
    "    def _setup_prompts(self):\n",
    "        \"\"\"Set up prompts for different stages\"\"\"\n",
    "\n",
    "        # Retrieval decision prompt\n",
    "        self.retrieval_decision_prompt = PromptTemplate(\n",
    "            input_variables=[\"query\"],\n",
    "            template=\"\"\"\n",
    "            Analyze the following query to determine if it requires external knowledge retrieval.\n",
    "            \n",
    "            Query: \"{query}\"\n",
    "            \n",
    "            Consider:\n",
    "            1. Does this query ask for specific facts, data, or domain-specific information?\n",
    "            2. Would the answer benefit from external documents or knowledge base?\n",
    "            3. Is this asking about general knowledge that can be answered without retrieval?\n",
    "            4. Does it require recent or specialized information?\n",
    "            \n",
    "            Provide your reasoning and then answer with either \"RETRIEVE\" or \"NO_RETRIEVE\".\n",
    "            \n",
    "            Reasoning: [Explain your decision]\n",
    "            Decision: [RETRIEVE or NO_RETRIEVE]\n",
    "            \"\"\"\n",
    "        )\n",
    "\n",
    "        # Answer generation with retrieval prompt\n",
    "        self.rag_prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "            You are a helpful AI assistant. Use the following context documents to answer the user's question accurately and comprehensively.\n",
    "            \n",
    "            Context Documents:\n",
    "            {context}\n",
    "            \n",
    "            Question: {question}\n",
    "            \n",
    "            Instructions:\n",
    "            - Base your answer primarily on the provided context\n",
    "            - If the context doesn't contain sufficient information, acknowledge this\n",
    "            - Cite specific documents when referencing information\n",
    "            - Be accurate, detailed, and helpful\n",
    "            - If you need to use general knowledge to supplement the context, clearly indicate this\n",
    "            \n",
    "            Answer:\n",
    "        \"\"\")\n",
    "\n",
    "        # Answer generation without retrieval prompt\n",
    "        self.no_retrieval_prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "            You are a helpful AI assistant. Answer the following question using your general knowledge.\n",
    "            \n",
    "            Question: {question}\n",
    "            \n",
    "            Provide a comprehensive and accurate answer based on your training knowledge.\n",
    "            \n",
    "            Answer:\n",
    "        \"\"\")\n",
    "\n",
    "        # Reflection prompt\n",
    "        self.reflection_prompt = PromptTemplate(\n",
    "            input_variables=[\"query\", \"answer\", \"context\"],\n",
    "            template=\"\"\"\n",
    "            Evaluate the quality of the following answer based on the query and available context.\n",
    "            \n",
    "            Query: {query}\n",
    "            \n",
    "            Context:\n",
    "            {context}\n",
    "            \n",
    "            Answer: {answer}\n",
    "            \n",
    "            Rate the answer on a scale of 0-10 considering:\n",
    "            - Accuracy and factual correctness\n",
    "            - Completeness and comprehensiveness\n",
    "            - Relevance to the query\n",
    "            - Proper use of available context\n",
    "            - Clarity and helpfulness\n",
    "            \n",
    "            Provide only a single number between 0 and 10 as your rating.\n",
    "            \n",
    "            Rating:\n",
    "            \"\"\"\n",
    "        )\n",
    "\n",
    "    async def initialize(self):\n",
    "        \"\"\"Initialize the RAG System \"\"\"\n",
    "        if self.is_initialized:\n",
    "            return\n",
    "        \n",
    "        logger.info(\"Initializing the RAG system ...\")\n",
    "\n",
    "        # Load the document\n",
    "        documents = await self.document_loader.load_documents(self.kb_folder)\n",
    "\n",
    "        if not documents:\n",
    "            logger.warning(\"No documents were found in the knowledge base ...\")\n",
    "            self.vector_store = None\n",
    "            self.is_initialized = True\n",
    "            return\n",
    "\n",
    "        # Split the documents into chunks\n",
    "        split_docs = self.text_splitter.split_documents(documents)\n",
    "        logger.info(f\"Split {len(documents)} documents into {len(split_docs)} chunks\")\n",
    "\n",
    "        # Create the vector store\n",
    "        self.vector_store = await FAISS.from_documents(\n",
    "            split_docs,\n",
    "            self.embeddings\n",
    "        )\n",
    "\n",
    "        self.is_initialized = True\n",
    "        logger.info(\"Self-RAG system initialized successfully\")\n",
    "\n",
    "    async def _should_retrieve(self, query: str) -> Tuple[bool, str]:\n",
    "        \"\"\" Determine if the retrieval is needed and get reasoning\"\"\"\n",
    "        chain = self.retrieval_decision_prompt | self.llm.with_config(callbacks=[self.rate_limit_callback])\n",
    "\n",
    "        result = await chain.arun(query = query)\n",
    "\n",
    "        # Parse the result\n",
    "        lines = result.strip().split('\\n')\n",
    "        reasoning = \"\"\n",
    "        decision = False\n",
    "\n",
    "        for line in lines:\n",
    "            if line.startswith(\"Reasoning:\"):\n",
    "                reasoning = line.replace(\"Reasoning:\", \"\").strip()\n",
    "            elif line.startswith(\"Decision:\"):\n",
    "                decision_text = line.replace(\"Decision:\", \"\").strip()\n",
    "                decision = \"RETRIEVE\" in decision_text.upper()\n",
    "        \n",
    "        return decision, reasoning\n",
    "\n",
    "    async def _retrieve_documents(self, query: str, k:int = 5) -> List[Document]:\n",
    "        \"\"\"Retrieve relevant documents\"\"\"\n",
    "        if not self.vector_store:\n",
    "            return []\n",
    "\n",
    "        # Use similarity search with scores\n",
    "        doc_with_scores = await self.vector_store.asimilarity_search_with_score(query, k = k)\n",
    "        # Filter by relevance score\n",
    "        relevant_docs = [doc for doc, score in doc_with_scores if score > 0.8]\n",
    "\n",
    "        return relevant_docs\n",
    "\n",
    "    async def _generate_answer_with_retrieval(self, query: str, documents: Documents) -> str:\n",
    "        \"\"\"Generate answers using retrieved documents\"\"\"\n",
    "        context = \"\\n\\n\".join([\n",
    "            f\"Document: {doc.metadata.get('file_name', 'Unknown')}\\n{doc.page_content}\"\n",
    "            for doc in documents\n",
    "        ])\n",
    "        \n",
    "        chain = self.rag_prompt | self.llm | StrOutputParser()\n",
    "        \n",
    "        result = await chain.ainvoke({\n",
    "            \"context\": context,\n",
    "            \"question\": query\n",
    "        })\n",
    "        \n",
    "        return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e84635bd",
   "metadata": {
    "cellUniqueIdByVincent": "6ebcf"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  },
  "vincent": {
   "sessionId": "3f2153659d5b596a4189cf1d_2025-05-30T15-07-59-793Z"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
