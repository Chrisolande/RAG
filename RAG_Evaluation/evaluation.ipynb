{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellUniqueIdByVincent": "0a411"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from typing import List, Any, Tuple, Dict, Optional\n",
    "from dataclasses import dataclass\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# Your existing classes\n",
    "@dataclass\n",
    "class RAGEvaluationResult:\n",
    "    \"\"\"Structure to hold evaluation results for a single question\"\"\"\n",
    "    question: str\n",
    "    context: str\n",
    "    generated_answer: str\n",
    "    relevance_score: float\n",
    "    correctness_score: float\n",
    "    faithfulness_score: float\n",
    "    hallucination_score: float\n",
    "    detailed_feedback: Dict[str, str]\n",
    "    overall_score: float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cellUniqueIdByVincent": "d50d5"
   },
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class OpenRouterModel:\n",
    "    api_key: Optional[str] = os.getenv(\"OPENROUTER_API_KEY\")\n",
    "\n",
    "    def __post_init__(self):\n",
    "        if not self.api_key:\n",
    "            raise ValueError(\"Please set a valid api key in the environment variables\")\n",
    "        self.client = OpenAI(\n",
    "            base_url=\"https://openrouter.ai/api/v1\",\n",
    "            api_key=self.api_key,\n",
    "        )\n",
    "        \n",
    "        self.model = \"meta-llama/llama-3.3-70b-instruct\"\n",
    "\n",
    "    def invoke(self, prompt: str, temperature: float = 0.0, max_tokens: int = 2000) -> str:\n",
    "        \"\"\"Generate response using Llama 3.3 70B Instruct\"\"\"\n",
    "        try:\n",
    "            response = self.client.chat.completions.create(\n",
    "                model=self.model,\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                temperature=temperature,\n",
    "                max_tokens=max_tokens\n",
    "            )\n",
    "            return response.choices[0].message.content\n",
    "        except Exception as e:\n",
    "            print(f\"Error calling OpenRouter API: {e}\")\n",
    "            return \"\"\n",
    "        \n",
    "    def invoke_with_json_output(self, prompt_template, input_variables):\n",
    "        \"\"\"Generate structured JSON output from the model\"\"\"\n",
    "        \n",
    "        formatted_prompt = prompt_template.format(**input_variables)\n",
    "        \n",
    "        # Get the raw response from the model\n",
    "        raw_response = self.invoke(formatted_prompt)\n",
    "        \n",
    "        # Extract JSON from the response\n",
    "        try:\n",
    "            # Find JSON content within the response\n",
    "            json_start = raw_response.find('{')\n",
    "            json_end = raw_response.rfind('}') + 1\n",
    "            \n",
    "            if json_start >= 0 and json_end > json_start:\n",
    "                json_str = raw_response[json_start:json_end]\n",
    "                result = json.loads(json_str)\n",
    "                return result\n",
    "            else:\n",
    "                # If no JSON found, try to parse the whole response\n",
    "                return json.loads(raw_response)\n",
    "        except json.JSONDecodeError:\n",
    "            print(\"Failed to parse JSON from response. Raw response:\")\n",
    "            print(raw_response)\n",
    "            # Return a default structure if parsing fails\n",
    "            return {\"score\": 0, \"reasoning\": \"Failed to parse response\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cellUniqueIdByVincent": "2a1cc"
   },
   "outputs": [],
   "source": [
    "class ResultScore(BaseModel):\n",
    "    score: float = Field(..., description=\"The score of the result, ranging from 0 to 1 where 1 is the best possible score.\")\n",
    "    reasoning: str = Field(..., description=\"An extensive explanation of the score.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellUniqueIdByVincent": "a10db"
   },
   "outputs": [],
   "source": [
    "def evaluate_relevance(question: str, context: str, generated_answer: str, llm: OpenRouterModel):\n",
    "    prompt = PromptTemplate(\n",
    "        input_variables=[\"question\", \"context\", \"generated_answer\"],\n",
    "        template = \"\"\"\n",
    "        Evaluate the RELEVANCE of the retrieved context to the given question.\n",
    "        \n",
    "        Question: {question}\n",
    "        \n",
    "        Retrieved Context: {context}\n",
    "\n",
    "        Generated Answer: {generated_answer}\n",
    "        \n",
    "        Rate the relevance on a scale of 1-5 where:\n",
    "        - 1: Completely irrelevant, no connection to the question\n",
    "        - 2: Minimally relevant, tangential connection\n",
    "        - 3: Somewhat relevant, partial connection\n",
    "        - 4: Highly relevant, strong connection\n",
    "        - 5: Perfectly relevant, directly addresses the question\n",
    "        \n",
    "        Provide your response in this format:\n",
    "        {{\n",
    "            \"score\": <number>,\n",
    "            \"reasoning\": \"<detailed explanation of why you gave this score>\"\n",
    "        }}\n",
    "        \n",
    "        Ensure your response is valid JSON.\n",
    "        \"\"\"\n",
    "    )\n",
    "\n",
    "    # Use the custom method for structured output\n",
    "    result = llm.invoke_with_json_output(\n",
    "        prompt_template=prompt.template,\n",
    "        input_variables={\n",
    "            \"question\": question,\n",
    "            \"context\": context,\n",
    "            \"generated_answer\": generated_answer\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    # Maps 1–5 to 0.0–1.0\n",
    "    if \"score\" in result:\n",
    "        result[\"score\"] = (result[\"score\"] - 1) / 4.0 \n",
    "        \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellUniqueIdByVincent": "f88c0"
   },
   "outputs": [],
   "source": [
    "def evaluate_correctness(question: str, generated_answer: str, llm: OpenRouterModel, context: Optional[str] = None):\n",
    "    prompt_template = \"\"\"\n",
    "    Evaluate the FACTUAL CORRECTNESS of the given answer to the question.\n",
    "    \n",
    "    Question: {question}\n",
    "    \n",
    "    Answer: {generated_answer}\n",
    "    \"\"\"\n",
    "    \n",
    "    # Add context if provided\n",
    "    if context:\n",
    "        prompt_template += f\"\\nContext (for reference): {context}\\n\"\n",
    "    else:\n",
    "        prompt_template += \"\\n\"\n",
    "        \n",
    "    prompt_template += \"\"\"\n",
    "    Rate the correctness on a scale of 1-5 where:\n",
    "    - 1: Completely incorrect, contains major factual errors\n",
    "    - 2: Mostly incorrect, some facts but significant errors\n",
    "    - 3: Partially correct, mix of correct and incorrect information\n",
    "    - 4: Mostly correct, minor errors or omissions\n",
    "    - 5: Completely correct, factually accurate and comprehensive\n",
    "    \n",
    "    Focus on:\n",
    "    - Factual accuracy of claims made\n",
    "    - Logical consistency\n",
    "    - Completeness of the answer\n",
    "    \n",
    "    Provide your response in this exact JSON format:\n",
    "    {{\n",
    "        \"score\": <number>,\n",
    "        \"reasoning\": \"<detailed explanation focusing on specific facts>\"\n",
    "    }}\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create input variables dict based on whether context is provided\n",
    "    input_variables = {\n",
    "        \"question\": question,\n",
    "        \"generated_answer\": generated_answer\n",
    "    }\n",
    "    \n",
    "    # Only add context to input variables if it's provided\n",
    "    if context:\n",
    "        input_variables[\"context\"] = context\n",
    "    \n",
    "    result = llm.invoke_with_json_output(\n",
    "        prompt_template=prompt_template,\n",
    "        input_variables=input_variables\n",
    "    )\n",
    "    \n",
    "    # Maps 1–5 to 0.0–1.0\n",
    "    if \"score\" in result:\n",
    "        result[\"score\"] = (result[\"score\"] - 1) / 4.0 \n",
    "        \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "cellUniqueIdByVincent": "28105"
   },
   "outputs": [],
   "source": [
    "def evaluate_faithfulness(question: str, generated_answer: str, llm: OpenRouterModel, context: Optional[str] = None):\n",
    "    prompt = PromptTemplate(\n",
    "        input_variables = [\"question\", \"context\", \"generated_answer\"],\n",
    "        template = \"\"\"\n",
    "        Evaluate the FAITHFULNESS of the answer to the provided context. The answer should only contain information that can be supported by or reasonably inferred from the context.\n",
    "        \n",
    "        Question: {question}\n",
    "        \n",
    "        Context: {context}\n",
    "        \n",
    "        Answer: {generated_answer}\n",
    "        \n",
    "        Rate the faithfulness on a scale of 1-5 where:\n",
    "        - 1: Completely unfaithful, answer contradicts or ignores context\n",
    "        - 2: Mostly unfaithful, some alignment but significant deviations\n",
    "        - 3: Partially faithful, generally aligned but some unsupported claims\n",
    "        - 4: Mostly faithful, well-grounded with minor unsupported details\n",
    "        - 5: Completely faithful, all claims supported by or inferable from context\n",
    "        \n",
    "        Check for:\n",
    "        - Claims that go beyond what's stated in the context\n",
    "        - Information that contradicts the context\n",
    "        - Proper grounding of all assertions\n",
    "        \n",
    "        Provide your response in this exact JSON format:\n",
    "        {{\n",
    "            \"score\": <number>,\n",
    "            \"reasoning\": \"<detailed explanation with specific examples>\"\n",
    "        }}\n",
    "        \"\"\"\n",
    "    )\n",
    "    # Use the custom method for structured output\n",
    "    result = llm.invoke_with_json_output(\n",
    "        prompt_template=prompt.template,\n",
    "        input_variables={\n",
    "            \"question\": question,\n",
    "            \"context\": context,\n",
    "            \"generated_answer\": generated_answer\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    # Maps 1–5 to 0.0–1.0\n",
    "    if \"score\" in result:\n",
    "        result[\"score\"] = (result[\"score\"] - 1) / 4.0 \n",
    "        \n",
    "    return result\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "cellUniqueIdByVincent": "d99c6"
   },
   "outputs": [],
   "source": [
    "def evaluate_hallucination(question: str, generated_answer: str, llm: OpenRouterModel, context: Optional[str] = None):\n",
    "    prompt = PromptTemplate(\n",
    "        input_variables = [\"question\", \"context\", \"generated_answer\"],\n",
    "        template = \"\"\"\n",
    "        Evaluate whether the answer contains HALLUCINATED information - facts, claims, or details that are NOT present in the provided context.\n",
    "        \n",
    "        Question: {question}\n",
    "        \n",
    "        Context: {context}\n",
    "        \n",
    "        Answer: {generated_answer}\n",
    "        \n",
    "        Rate hallucination on a scale of 1-5 where:\n",
    "        - 1: Severe hallucination, answer contains mostly fabricated information\n",
    "        - 2: Significant hallucination, multiple unsupported claims\n",
    "        - 3: Moderate hallucination, some fabricated details\n",
    "        - 4: Minor hallucination, mostly grounded with few unsupported claims\n",
    "        - 5: No hallucination, all information comes from or is inferable from context\n",
    "        \n",
    "        Specifically look for:\n",
    "        - Facts mentioned in answer but not in context\n",
    "        - Specific numbers, dates, names not in context\n",
    "        - Detailed explanations beyond what context provides\n",
    "        - Claims that go beyond reasonable inference\n",
    "        \n",
    "        Provide your response in this exact JSON format:\n",
    "        {{\n",
    "            \"score\": <number>,\n",
    "            \"reasoning\": \"<detailed explanation identifying specific hallucinations if any>\"\n",
    "        }}\n",
    "        \"\"\"\n",
    "    )\n",
    "    # Use the custom method for structured output\n",
    "    result = llm.invoke_with_json_output(\n",
    "        prompt_template=prompt.template,\n",
    "        input_variables={\n",
    "            \"question\": question,\n",
    "            \"context\": context,\n",
    "            \"generated_answer\": generated_answer\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    # Maps 1–5 to 0.0–1.0\n",
    "    if \"score\" in result:\n",
    "        result[\"score\"] = (result[\"score\"] - 1) / 4.0 \n",
    "        \n",
    "    return result\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "cellUniqueIdByVincent": "1f302"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Relevance Score: 1.0\n",
      "Reasoning: The retrieved context directly addresses the question by stating that climate change is primarily caused by human activities such as burning fossil fuels, deforestation, industrial processes, and agriculture, which increase greenhouse gas concentrations in the atmosphere. The generated answer also aligns closely with this context, mentioning both human factors (burning fossil fuels, deforestation, industrial processes, and transportation) and briefly acknowledging natural factors. The information provided in the context and the generated answer comprehensively covers the causes of climate change as requested by the question, making the retrieved context perfectly relevant to the given question.\n",
      "Correctness Score: 1.0\n",
      "Reasoning: The answer provided accurately identifies both natural and human factors contributing to climate change. It correctly highlights the significant role of human activities such as burning fossil fuels and deforestation in the current climate change scenario. The mention of increased greenhouse gases from industrial processes and transportation further supports the factual correctness of the answer. The context provided for reference aligns with the information given in the answer, emphasizing the primary causes of climate change as human activities, particularly the burning of fossil fuels, deforestation, and industrial processes. The answer demonstrates logical consistency by acknowledging the long-term impacts of natural factors like volcanic eruptions and variations in solar radiation while emphasizing the predominant influence of human activities on current climate change. The completeness of the answer is also noteworthy, as it covers the main contributors to climate change without omitting significant factors or including incorrect information.\n",
      "Faithfulness Score: 0.75\n",
      "Reasoning: The answer is mostly faithful to the context as it correctly identifies human activities such as burning fossil fuels and deforestation as primary causes of climate change, which is directly supported by the context. It also mentions industrial processes, which is in line with the context. However, the answer introduces 'volcanic eruptions and variations in solar radiation' as natural factors contributing to climate change, which is not mentioned in the provided context. This introduction of additional, unsupported information prevents the answer from being completely faithful. Nonetheless, the core of the answer remains well-grounded in the context, with the majority of the claims being supported or inferable from it.\n",
      "Hallucination Score: 0.75\n",
      "Reasoning: The answer mentions both natural and human factors contributing to climate change, including volcanic eruptions and variations in solar radiation, which are not mentioned in the provided context. However, these additions are not entirely unfounded or fabricated, as they are generally recognized factors in the broader discussion of climate change. The answer also stays largely true to the context by highlighting human activities such as burning fossil fuels, deforestation, industrial processes, and transportation as significant contributors. The introduction of volcanic eruptions and solar radiation variations as natural factors is the primary deviation from the context, but it does not significantly distort the overall message and is a reasonable, albeit not directly supported, expansion of the topic. Therefore, the hallucination is considered minor, as the answer is mostly grounded in the provided context with only a few unsupported claims that are still within the realm of general knowledge on the subject.\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "llm = OpenRouterModel()\n",
    "\n",
    "# Example data\n",
    "question = \"What causes climate change?\"\n",
    "context = \"Climate change is primarily caused by human activities that increase greenhouse gas concentrations in the atmosphere. The main contributors include burning fossil fuels (coal, oil, gas) for electricity, heat, and transportation, which releases carbon dioxide. Deforestation reduces the Earth's capacity to absorb CO2. Industrial processes and agriculture also contribute significantly.\"\n",
    "generated_answer = \"\"\"\n",
    "Climate change is caused by both natural and human factors. While volcanic eruptions and variations in solar radiation have long-term impacts, the current changes are largely due to human activities such as burning fossil fuels and deforestation. Increased greenhouse gases from industrial processes and transportation play a significant role.\"\"\"\n",
    "\n",
    "# Evaluate relevance\n",
    "relevance_result = evaluate_relevance(\n",
    "    question=question, \n",
    "    context=context, \n",
    "    generated_answer=generated_answer, \n",
    "    llm=llm\n",
    ")\n",
    "print(f\"Relevance Score: {relevance_result['score']}\")\n",
    "print(f\"Reasoning: {relevance_result['reasoning']}\")\n",
    "\n",
    "# Evaluate correctness\n",
    "correctness_result = evaluate_correctness(\n",
    "    question=question, \n",
    "    generated_answer=generated_answer, \n",
    "    llm=llm, \n",
    "    context=context\n",
    ")\n",
    "print(f\"Correctness Score: {correctness_result['score']}\")\n",
    "print(f\"Reasoning: {correctness_result['reasoning']}\")\n",
    "\n",
    "# Evaluate faithfulness\n",
    "faithfulness_result = evaluate_faithfulness(\n",
    "    question=question, \n",
    "    generated_answer=generated_answer, \n",
    "    llm=llm, \n",
    "    context=context\n",
    ")\n",
    "print(f\"Faithfulness Score: {faithfulness_result['score']}\")\n",
    "print(f\"Reasoning: {faithfulness_result['reasoning']}\")\n",
    "\n",
    "# Evaluate faithfulness\n",
    "hallucination_result = evaluate_hallucination(\n",
    "    question=question, \n",
    "    generated_answer=generated_answer, \n",
    "    llm=llm, \n",
    "    context=context\n",
    ")\n",
    "print(f\"Hallucination Score: {hallucination_result['score']}\")\n",
    "print(f\"Reasoning: {hallucination_result['reasoning']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellUniqueIdByVincent": "6865c"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  },
  "vincent": {
   "sessionId": "b75523e7856d7a763194bb99_2025-05-29T13-09-17-251Z"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
