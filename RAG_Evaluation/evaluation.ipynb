{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cellUniqueIdByVincent": "0a411"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from typing import List, Any, Tuple, Dict, Optional\n",
    "from dataclasses import dataclass\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "@dataclass\n",
    "class RAGEvaluationResult:\n",
    "    \"\"\"Structure to hold evaluation results for a single question\"\"\"\n",
    "    question: str\n",
    "    context: str\n",
    "    generated_answer: str\n",
    "    relevance_score: float\n",
    "    correctness_score: float\n",
    "    faithfulness_score: float\n",
    "    hallucination_score: float\n",
    "    detailed_feedback: Dict[str, str]\n",
    "    overall_score: float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "cellUniqueIdByVincent": "e2e6b"
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "def score(func):\n",
    "    @wraps(func)\n",
    "    def wrapper(*args, **kwargs):\n",
    "        result = func(*args, **kwargs)\n",
    "\n",
    "        # Handle JSON string\n",
    "        if isinstance(result, str):\n",
    "            result = json.loads(result)\n",
    "\n",
    "        if \"score\" in result:\n",
    "            raw_score = result[\"score\"]\n",
    "            normalized_score = (raw_score - 1) / 4.0\n",
    "            result[\"score\"] = normalized_score\n",
    "\n",
    "            # Log the transformation\n",
    "            logging.info(f\"Raw score: {raw_score}, Normalized: {normalized_score}\")\n",
    "        else:\n",
    "            logging.warning(\"Score key not found in result.\")\n",
    "\n",
    "        return result\n",
    "    return wrapper\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "cellUniqueIdByVincent": "d50d5"
   },
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class OpenRouterModel:\n",
    "    api_key: Optional[str] = os.getenv(\"OPENROUTER_API_KEY\")\n",
    "    model: str = \"meta-llama/llama-3.3-70b-instruct\"\n",
    "\n",
    "    def __post_init__(self):\n",
    "        if not self.api_key:\n",
    "            raise ValueError(\"Please set a valid api key in the environment variables\")\n",
    "        self.client = OpenAI(\n",
    "            base_url=\"https://openrouter.ai/api/v1\",\n",
    "            api_key=self.api_key,\n",
    "        )\n",
    "        \n",
    "\n",
    "    def invoke(self, prompt: str, temperature: float = 0.1, max_tokens: int = 2000) -> str:\n",
    "        \"\"\"Generate response using Llama 3.3 70B Instruct\"\"\"\n",
    "        try:\n",
    "            response = self.client.chat.completions.create(\n",
    "                model=self.model,\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                temperature=temperature,\n",
    "                max_tokens=max_tokens\n",
    "            )\n",
    "            return response.choices[0].message.content\n",
    "        except Exception as e:\n",
    "            print(f\"Error calling OpenRouter API: {e}\")\n",
    "            return \"\"\n",
    "        \n",
    "    def invoke_with_json_output(self, prompt_template, input_variables):\n",
    "        \"\"\"Generate structured JSON output from the model\"\"\"\n",
    "        \n",
    "        formatted_prompt = prompt_template.format(**input_variables)\n",
    "        \n",
    "        # Get the raw response from the model\n",
    "        raw_response = self.invoke(formatted_prompt)\n",
    "        \n",
    "        # Extract JSON from the response\n",
    "        try:\n",
    "            # Find JSON content within the response\n",
    "            json_start = raw_response.find('{')\n",
    "            json_end = raw_response.rfind('}') + 1\n",
    "            \n",
    "            if json_start >= 0 and json_end > json_start:\n",
    "                json_str = raw_response[json_start:json_end]\n",
    "                result = json.loads(json_str)\n",
    "                return result\n",
    "            else:\n",
    "                # If no JSON found, try to parse the whole response\n",
    "                return json.loads(raw_response)\n",
    "        except json.JSONDecodeError:\n",
    "            print(\"Failed to parse JSON from response. Raw response:\")\n",
    "            print(raw_response)\n",
    "            # Return a default structure if parsing fails\n",
    "            return {\"score\": 0, \"reasoning\": \"Failed to parse response\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "cellUniqueIdByVincent": "89d38"
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "non-default argument 'generated_answer' follows default argument 'context'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mfunctools\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m wraps\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;129;43m@dataclass\u001b[39;49m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;43;01mclass\u001b[39;49;00m\u001b[38;5;250;43m \u001b[39;49m\u001b[34;43;01mEvaluation\u001b[39;49;00m\u001b[43m:\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[43mquestion\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mOptional\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.13/dataclasses.py:1305\u001b[39m, in \u001b[36mdataclass\u001b[39m\u001b[34m(cls, init, repr, eq, order, unsafe_hash, frozen, match_args, kw_only, slots, weakref_slot)\u001b[39m\n\u001b[32m   1302\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m wrap\n\u001b[32m   1304\u001b[39m \u001b[38;5;66;03m# We're called as @dataclass without parens.\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1305\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrap\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.13/dataclasses.py:1295\u001b[39m, in \u001b[36mdataclass.<locals>.wrap\u001b[39m\u001b[34m(cls)\u001b[39m\n\u001b[32m   1294\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrap\u001b[39m(\u001b[38;5;28mcls\u001b[39m):\n\u001b[32m-> \u001b[39m\u001b[32m1295\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_process_class\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mrepr\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43munsafe_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1296\u001b[39m \u001b[43m                          \u001b[49m\u001b[43mfrozen\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmatch_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkw_only\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mslots\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1297\u001b[39m \u001b[43m                          \u001b[49m\u001b[43mweakref_slot\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.13/dataclasses.py:1078\u001b[39m, in \u001b[36m_process_class\u001b[39m\u001b[34m(cls, init, repr, eq, order, unsafe_hash, frozen, match_args, kw_only, slots, weakref_slot)\u001b[39m\n\u001b[32m   1074\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m init:\n\u001b[32m   1075\u001b[39m     \u001b[38;5;66;03m# Does this class have a post-init function?\u001b[39;00m\n\u001b[32m   1076\u001b[39m     has_post_init = \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mcls\u001b[39m, _POST_INIT_NAME)\n\u001b[32m-> \u001b[39m\u001b[32m1078\u001b[39m     \u001b[43m_init_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mall_init_fields\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1079\u001b[39m \u001b[43m             \u001b[49m\u001b[43mstd_init_fields\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1080\u001b[39m \u001b[43m             \u001b[49m\u001b[43mkw_only_init_fields\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1081\u001b[39m \u001b[43m             \u001b[49m\u001b[43mfrozen\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1082\u001b[39m \u001b[43m             \u001b[49m\u001b[43mhas_post_init\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1083\u001b[39m \u001b[43m             \u001b[49m\u001b[38;5;66;43;03m# The name to use for the \"self\"\u001b[39;49;00m\n\u001b[32m   1084\u001b[39m \u001b[43m             \u001b[49m\u001b[38;5;66;43;03m# param in __init__.  Use \"self\"\u001b[39;49;00m\n\u001b[32m   1085\u001b[39m \u001b[43m             \u001b[49m\u001b[38;5;66;43;03m# if possible.\u001b[39;49;00m\n\u001b[32m   1086\u001b[39m \u001b[43m             \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m__dataclass_self__\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mself\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mfields\u001b[49m\n\u001b[32m   1087\u001b[39m \u001b[43m             \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mself\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1088\u001b[39m \u001b[43m             \u001b[49m\u001b[43mfunc_builder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1089\u001b[39m \u001b[43m             \u001b[49m\u001b[43mslots\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1090\u001b[39m \u001b[43m             \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1092\u001b[39m _set_new_attribute(\u001b[38;5;28mcls\u001b[39m, \u001b[33m'\u001b[39m\u001b[33m__replace__\u001b[39m\u001b[33m'\u001b[39m, _replace)\n\u001b[32m   1094\u001b[39m \u001b[38;5;66;03m# Get the fields as a list, and include only real fields.  This is\u001b[39;00m\n\u001b[32m   1095\u001b[39m \u001b[38;5;66;03m# used in all of the following methods.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.13/dataclasses.py:627\u001b[39m, in \u001b[36m_init_fn\u001b[39m\u001b[34m(fields, std_fields, kw_only_fields, frozen, has_post_init, self_name, func_builder, slots)\u001b[39m\n\u001b[32m    625\u001b[39m             seen_default = f\n\u001b[32m    626\u001b[39m         \u001b[38;5;28;01melif\u001b[39;00m seen_default:\n\u001b[32m--> \u001b[39m\u001b[32m627\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mnon-default argument \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mf.name\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[33m \u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m    628\u001b[39m                             \u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mfollows default argument \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mseen_default.name\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[33m'\u001b[39m)\n\u001b[32m    630\u001b[39m \u001b[38;5;28mlocals\u001b[39m = {**{\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33m__dataclass_type_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mf.name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m__\u001b[39m\u001b[33m'\u001b[39m: f.type \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m fields},\n\u001b[32m    631\u001b[39m           **{\u001b[33m'\u001b[39m\u001b[33m__dataclass_HAS_DEFAULT_FACTORY__\u001b[39m\u001b[33m'\u001b[39m: _HAS_DEFAULT_FACTORY,\n\u001b[32m    632\u001b[39m              \u001b[33m'\u001b[39m\u001b[33m__dataclass_builtins_object__\u001b[39m\u001b[33m'\u001b[39m: \u001b[38;5;28mobject\u001b[39m,\n\u001b[32m    633\u001b[39m              }\n\u001b[32m    634\u001b[39m           }\n\u001b[32m    636\u001b[39m body_lines = []\n",
      "\u001b[31mTypeError\u001b[39m: non-default argument 'generated_answer' follows default argument 'context'"
     ]
    }
   ],
   "source": [
    "from functools import wraps\n",
    "@dataclass\n",
    "class Evaluation:\n",
    "    question:str\n",
    "    context: Optional[str] = None\n",
    "    generated_answer: str\n",
    "    llm: OpenRouterModel\n",
    "\n",
    "    @score\n",
    "    def evaluate_relevance(self):\n",
    "        prompt = PromptTemplate(\n",
    "            input_variables=[\"question\", \"context\", \"generated_answer\"],\n",
    "            template = \"\"\"\n",
    "            Evaluate the RELEVANCE of the retrieved context to the given question.\n",
    "            \n",
    "            Question: {question}\n",
    "            \n",
    "            Retrieved Context: {context}\n",
    "\n",
    "            Generated Answer: {generated_answer}\n",
    "            \n",
    "            Rate the relevance on a scale of 1-5 where:\n",
    "            - 1: Completely irrelevant, no connection to the question\n",
    "            - 2: Minimally relevant, tangential connection\n",
    "            - 3: Somewhat relevant, partial connection\n",
    "            - 4: Highly relevant, strong connection\n",
    "            - 5: Perfectly relevant, directly addresses the question\n",
    "            \n",
    "            Provide your response in this format:\n",
    "            {{\n",
    "                \"score\": <number>,\n",
    "                \"reasoning\": \"<detailed explanation of why you gave this score>\"\n",
    "            }}\n",
    "            \n",
    "            Ensure your response is valid JSON.\n",
    "            \"\"\"\n",
    "        )\n",
    "\n",
    "        # Use the custom method for structured output\n",
    "        return self.llm.invoke_with_json_output(\n",
    "            prompt_template=prompt.template,\n",
    "            input_variables={\n",
    "                \"question\": self.question,\n",
    "                \"context\": self.context,\n",
    "                \"generated_answer\": self.generated_answer\n",
    "            }\n",
    "        )\n",
    "\n",
    "    @score\n",
    "    def evaluate_correctness(self):\n",
    "        prompt_template = \"\"\"\n",
    "        Evaluate the FACTUAL CORRECTNESS of the given answer to the question.\n",
    "        \n",
    "        Question: {question}\n",
    "        \n",
    "        Answer: {generated_answer}\n",
    "        \"\"\"\n",
    "        \n",
    "        # Add context if provided\n",
    "        if self.context:\n",
    "            prompt_template += f\"\\nContext (for reference): {context}\\n\"\n",
    "        else:\n",
    "            prompt_template += \"\\n\"\n",
    "            \n",
    "        prompt_template += \"\"\"\n",
    "        Rate the correctness on a scale of 1-5 where:\n",
    "        - 1: Completely incorrect, contains major factual errors\n",
    "        - 2: Mostly incorrect, some facts but significant errors\n",
    "        - 3: Partially correct, mix of correct and incorrect information\n",
    "        - 4: Mostly correct, minor errors or omissions\n",
    "        - 5: Completely correct, factually accurate and comprehensive\n",
    "        \n",
    "        Focus on:\n",
    "        - Factual accuracy of claims made\n",
    "        - Logical consistency\n",
    "        - Completeness of the answer\n",
    "        \n",
    "        Provide your response in this exact JSON format:\n",
    "        {{\n",
    "            \"score\": <number>,\n",
    "            \"reasoning\": \"<detailed explanation focusing on specific facts>\"\n",
    "        }}\n",
    "        \"\"\"\n",
    "        \n",
    "        # Create input variables dict based on whether context is provided\n",
    "        input_variables = {\n",
    "            \"question\": self.question,\n",
    "            \"generated_answer\": self.generated_answer\n",
    "        }\n",
    "        \n",
    "        # Only add context to input variables if it's provided\n",
    "        if self.context:\n",
    "            input_variables[\"context\"] = self.context\n",
    "        \n",
    "        return self.llm.invoke_with_json_output(\n",
    "            prompt_template=prompt_template,\n",
    "            input_variables=input_variables\n",
    "        )\n",
    "\n",
    "    \n",
    "    @score\n",
    "    def evaluate_faithfulness(self):\n",
    "        prompt = PromptTemplate(\n",
    "            input_variables = [\"question\", \"context\", \"generated_answer\"],\n",
    "            template = \"\"\"\n",
    "            Evaluate the FAITHFULNESS of the answer to the provided context. The answer should only contain information that can be supported by or reasonably inferred from the context.\n",
    "            \n",
    "            Question: {question}\n",
    "            \n",
    "            Context: {context}\n",
    "            \n",
    "            Answer: {generated_answer}\n",
    "            \n",
    "            Rate the faithfulness on a scale of 1-5 where:\n",
    "            - 1: Completely unfaithful, answer contradicts or ignores context\n",
    "            - 2: Mostly unfaithful, some alignment but significant deviations\n",
    "            - 3: Partially faithful, generally aligned but some unsupported claims\n",
    "            - 4: Mostly faithful, well-grounded with minor unsupported details\n",
    "            - 5: Completely faithful, all claims supported by or inferable from context\n",
    "            \n",
    "            Check for:\n",
    "            - Claims that go beyond what's stated in the context\n",
    "            - Information that contradicts the context\n",
    "            - Proper grounding of all assertions\n",
    "            \n",
    "            Provide your response in this exact JSON format:\n",
    "            {{\n",
    "                \"score\": <number>,\n",
    "                \"reasoning\": \"<detailed explanation with specific examples>\"\n",
    "            }}\n",
    "            \"\"\"\n",
    "        )\n",
    "        \n",
    "        return self.llm.invoke_with_json_output(\n",
    "            prompt_template=prompt.template,\n",
    "            input_variables={\n",
    "                \"question\": self.question,\n",
    "                \"context\": self.context,\n",
    "                \"generated_answer\": self.generated_answer\n",
    "            }\n",
    "        )\n",
    "        \n",
    "    \n",
    "    @score\n",
    "    def evaluate_hallucination(self):\n",
    "        prompt = PromptTemplate(\n",
    "            input_variables = [\"question\", \"context\", \"generated_answer\"],\n",
    "            template = \"\"\"\n",
    "            Evaluate whether the answer contains HALLUCINATED information - facts, claims, or details that are NOT present in the provided context.\n",
    "            \n",
    "            Question: {question}\n",
    "            \n",
    "            Context: {context}\n",
    "            \n",
    "            Answer: {generated_answer}\n",
    "            \n",
    "            Rate hallucination on a scale of 1-5 where:\n",
    "            - 1: Severe hallucination, answer contains mostly fabricated information\n",
    "            - 2: Significant hallucination, multiple unsupported claims\n",
    "            - 3: Moderate hallucination, some fabricated details\n",
    "            - 4: Minor hallucination, mostly grounded with few unsupported claims\n",
    "            - 5: No hallucination, all information comes from or is inferable from context\n",
    "            \n",
    "            Specifically look for:\n",
    "            - Facts mentioned in answer but not in context\n",
    "            - Specific numbers, dates, names not in context\n",
    "            - Detailed explanations beyond what context provides\n",
    "            - Claims that go beyond reasonable inference\n",
    "            \n",
    "            Provide your response in this exact JSON format:\n",
    "            {{\n",
    "                \"score\": <number>,\n",
    "                \"reasoning\": \"<detailed explanation identifying specific hallucinations if any>\"\n",
    "            }}\n",
    "            \"\"\"\n",
    "        )\n",
    "        \n",
    "        return self.llm.invoke_with_json_output(\n",
    "            prompt_template=prompt.template,\n",
    "            input_variables={\n",
    "                \"question\": self.question,\n",
    "                \"context\": self.context,\n",
    "                \"generated_answer\": self.generated_answer\n",
    "            }\n",
    "        )\n",
    "    \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "cellUniqueIdByVincent": "d50ff"
   },
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Evaluation:\n",
    "    question: str\n",
    "    generated_answer: str\n",
    "    llm: OpenRouterModel\n",
    "    context: Optional[str] = None\n",
    "\n",
    "    def _evaluate_with_template(self, template: str, input_vars: Dict[str, str]) -> Dict:\n",
    "        \"\"\"Generic method to handle all evaluations with templates.\"\"\"\n",
    "        return self.llm.invoke_with_json_output(\n",
    "            prompt_template=template,\n",
    "            input_variables=input_vars\n",
    "        )\n",
    "\n",
    "    def _get_base_input_variables(self) -> Dict[str, str]:\n",
    "        \"\"\"Get the base input variables used by most evaluation methods.\"\"\"\n",
    "        return {\n",
    "            \"question\": self.question,\n",
    "            \"context\": self.context,\n",
    "            \"generated_answer\": self.generated_answer\n",
    "        }\n",
    "\n",
    "    @score\n",
    "    def evaluate_relevance(self) -> Dict:\n",
    "        template = \"\"\"\n",
    "        Evaluate the RELEVANCE of the retrieved context to the given question.\n",
    "        \n",
    "        Question: {question}\n",
    "        \n",
    "        Retrieved Context: {context}\n",
    "\n",
    "        Generated Answer: {generated_answer}\n",
    "        \n",
    "        Rate the relevance on a scale of 1-5 where:\n",
    "        - 1: Completely irrelevant, no connection to the question\n",
    "        - 2: Minimally relevant, tangential connection\n",
    "        - 3: Somewhat relevant, partial connection\n",
    "        - 4: Highly relevant, strong connection\n",
    "        - 5: Perfectly relevant, directly addresses the question\n",
    "        \n",
    "        Provide your response in this format:\n",
    "        {{\n",
    "            \"score\": <number>,\n",
    "            \"reasoning\": \"<detailed explanation of why you gave this score>\"\n",
    "        }}\n",
    "        \n",
    "        Ensure your response is valid JSON.\n",
    "        \"\"\"\n",
    "        \n",
    "        return self._evaluate_with_template(template, self._get_base_input_variables())\n",
    "\n",
    "    @score\n",
    "    def evaluate_correctness(self) -> Dict:\n",
    "        template = \"\"\"\n",
    "        Evaluate the FACTUAL CORRECTNESS of the given answer to the question.\n",
    "        \n",
    "        Question: {question}\n",
    "        \n",
    "        Answer: {generated_answer}\n",
    "        \"\"\"\n",
    "        \n",
    "        # Add context if provided\n",
    "        if self.context:\n",
    "            template += f\"\\nContext (for reference): {{context}}\\n\"\n",
    "        else:\n",
    "            template += \"\\n\"\n",
    "            \n",
    "        template += \"\"\"\n",
    "        Rate the correctness on a scale of 1-5 where:\n",
    "        - 1: Completely incorrect, contains major factual errors\n",
    "        - 2: Mostly incorrect, some facts but significant errors\n",
    "        - 3: Partially correct, mix of correct and incorrect information\n",
    "        - 4: Mostly correct, minor errors or omissions\n",
    "        - 5: Completely correct, factually accurate and comprehensive\n",
    "        \n",
    "        Focus on:\n",
    "        - Factual accuracy of claims made\n",
    "        - Logical consistency\n",
    "        - Completeness of the answer\n",
    "        \n",
    "        Provide your response in this exact JSON format:\n",
    "        {{\n",
    "            \"score\": <number>,\n",
    "            \"reasoning\": \"<detailed explanation focusing on specific facts>\"\n",
    "        }}\n",
    "        \"\"\"\n",
    "        \n",
    "        # Create input variables dict based on whether context is provided\n",
    "        input_variables = {\n",
    "            \"question\": self.question,\n",
    "            \"generated_answer\": self.generated_answer\n",
    "        }\n",
    "        \n",
    "        # Only add context to input variables if it's provided\n",
    "        if self.context:\n",
    "            input_variables[\"context\"] = self.context\n",
    "        \n",
    "        return self._evaluate_with_template(template, input_variables)\n",
    "\n",
    "    @score\n",
    "    def evaluate_faithfulness(self) -> Dict:\n",
    "        template = \"\"\"\n",
    "        Evaluate the FAITHFULNESS of the answer to the provided context. The answer should only contain information that can be supported by or reasonably inferred from the context.\n",
    "        \n",
    "        Question: {question}\n",
    "        \n",
    "        Context: {context}\n",
    "        \n",
    "        Answer: {generated_answer}\n",
    "        \n",
    "        Rate the faithfulness on a scale of 1-5 where:\n",
    "        - 1: Completely unfaithful, answer contradicts or ignores context\n",
    "        - 2: Mostly unfaithful, some alignment but significant deviations\n",
    "        - 3: Partially faithful, generally aligned but some unsupported claims\n",
    "        - 4: Mostly faithful, well-grounded with minor unsupported details\n",
    "        - 5: Completely faithful, all claims supported by or inferable from context\n",
    "        \n",
    "        Check for:\n",
    "        - Claims that go beyond what's stated in the context\n",
    "        - Information that contradicts the context\n",
    "        - Proper grounding of all assertions\n",
    "        \n",
    "        Provide your response in this exact JSON format:\n",
    "        {{\n",
    "            \"score\": <number>,\n",
    "            \"reasoning\": \"<detailed explanation with specific examples>\"\n",
    "        }}\n",
    "        \"\"\"\n",
    "        \n",
    "        return self._evaluate_with_template(template, self._get_base_input_variables())\n",
    "\n",
    "    @score\n",
    "    def evaluate_hallucination(self) -> Dict:\n",
    "        template = \"\"\"\n",
    "        Evaluate whether the answer contains HALLUCINATED information - facts, claims, or details that are NOT present in the provided context.\n",
    "        \n",
    "        Question: {question}\n",
    "        \n",
    "        Context: {context}\n",
    "        \n",
    "        Answer: {generated_answer}\n",
    "        \n",
    "        Rate hallucination on a scale of 1-5 where:\n",
    "        - 1: Severe hallucination, answer contains mostly fabricated information\n",
    "        - 2: Significant hallucination, multiple unsupported claims\n",
    "        - 3: Moderate hallucination, some fabricated details\n",
    "        - 4: Minor hallucination, mostly grounded with few unsupported claims\n",
    "        - 5: No hallucination, all information comes from or is inferable from context\n",
    "        \n",
    "        Specifically look for:\n",
    "        - Facts mentioned in answer but not in context\n",
    "        - Specific numbers, dates, names not in context\n",
    "        - Detailed explanations beyond what context provides\n",
    "        - Claims that go beyond reasonable inference\n",
    "        \n",
    "        Provide your response in this exact JSON format:\n",
    "        {{\n",
    "            \"score\": <number>,\n",
    "            \"reasoning\": \"<detailed explanation identifying specific hallucinations if any>\"\n",
    "        }}\n",
    "        \"\"\"\n",
    "        \n",
    "        return self._evaluate_with_template(template, self._get_base_input_variables())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "cellUniqueIdByVincent": "ed4ab"
   },
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "llm = OpenRouterModel()\n",
    "question = \"What causes climate change?\"\n",
    "context = \"Climate change is primarily caused by human activities that increase greenhouse gas concentrations in the atmosphere. The main contributors include burning fossil fuels (coal, oil, gas) for electricity, heat, and transportation, which releases carbon dioxide. Deforestation reduces the Earth's capacity to absorb CO2. Industrial processes and agriculture also contribute significantly.\"\n",
    "generated_answer = \"\"\"\n",
    "Climate change is caused by both natural and human factors. While volcanic eruptions and variations in solar radiation have long-term impacts, the current changes are largely due to human activities such as burning fossil fuels and deforestation. Increased greenhouse gases from industrial processes and transportation play a significant role.\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "cellUniqueIdByVincent": "c5e07"
   },
   "outputs": [],
   "source": [
    "eval1 = Evaluation(question = question, context = context, generated_answer = generated_answer, llm = llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "cellUniqueIdByVincent": "2fd6d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:root:Raw score: 5, Normalized: 1.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'score': 1.0,\n",
       " 'reasoning': 'The answer provided is factually accurate, logically consistent, and comprehensive. It correctly identifies both natural and human factors as contributors to climate change, with a focus on the dominant role of human activities such as burning fossil fuels and deforestation. The mention of volcanic eruptions and variations in solar radiation as natural factors is also accurate, as these can influence climate patterns. The answer highlights the significant role of increased greenhouse gases from industrial processes and transportation, which aligns with the context provided. The context emphasizes that climate change is primarily caused by human activities, particularly the burning of fossil fuels, deforestation, and industrial processes, all of which are correctly noted in the answer. Therefore, the answer is completely correct, with no major factual errors, omissions, or inconsistencies.'}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval1.evaluate_correctness()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellUniqueIdByVincent": "6865c"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  },
  "vincent": {
   "sessionId": "b75523e7856d7a763194bb99_2025-05-29T13-09-17-251Z"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
