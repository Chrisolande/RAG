{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellUniqueIdByVincent": "0a411"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from typing import List, Any, Tuple, Dict, Optional\n",
    "from dataclasses import dataclass\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# Your existing classes\n",
    "@dataclass\n",
    "class RAGEvaluationResult:\n",
    "    \"\"\"Structure to hold evaluation results for a single question\"\"\"\n",
    "    question: str\n",
    "    context: str\n",
    "    generated_answer: str\n",
    "    relevance_score: float\n",
    "    correctness_score: float\n",
    "    faithfulness_score: float\n",
    "    hallucination_score: float\n",
    "    detailed_feedback: Dict[str, str]\n",
    "    overall_score: float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cellUniqueIdByVincent": "d50d5"
   },
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class OpenRouterModel:\n",
    "    api_key: Optional[str] = os.getenv(\"OPENROUTER_API_KEY\")\n",
    "\n",
    "    def __post_init__(self):\n",
    "        if not self.api_key:\n",
    "            raise ValueError(\"Please set a valid api key in the environment variables\")\n",
    "        self.client = OpenAI(\n",
    "            base_url=\"https://openrouter.ai/api/v1\",\n",
    "            api_key=self.api_key,\n",
    "        )\n",
    "        \n",
    "        self.model = \"meta-llama/llama-3.3-70b-instruct\"\n",
    "\n",
    "    def invoke(self, prompt: str, temperature: float = 0.0, max_tokens: int = 2000) -> str:\n",
    "        \"\"\"Generate response using Llama 3.3 70B Instruct\"\"\"\n",
    "        try:\n",
    "            response = self.client.chat.completions.create(\n",
    "                model=self.model,\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                temperature=temperature,\n",
    "                max_tokens=max_tokens\n",
    "            )\n",
    "            return response.choices[0].message.content\n",
    "        except Exception as e:\n",
    "            print(f\"Error calling OpenRouter API: {e}\")\n",
    "            return \"\"\n",
    "        \n",
    "    def invoke_with_json_output(self, prompt_template, input_variables):\n",
    "        \"\"\"Generate structured JSON output from the model\"\"\"\n",
    "        \n",
    "        formatted_prompt = prompt_template.format(**input_variables)\n",
    "        \n",
    "        # Get the raw response from the model\n",
    "        raw_response = self.invoke(formatted_prompt)\n",
    "        \n",
    "        # Extract JSON from the response\n",
    "        try:\n",
    "            # Find JSON content within the response\n",
    "            json_start = raw_response.find('{')\n",
    "            json_end = raw_response.rfind('}') + 1\n",
    "            \n",
    "            if json_start >= 0 and json_end > json_start:\n",
    "                json_str = raw_response[json_start:json_end]\n",
    "                result = json.loads(json_str)\n",
    "                return result\n",
    "            else:\n",
    "                # If no JSON found, try to parse the whole response\n",
    "                return json.loads(raw_response)\n",
    "        except json.JSONDecodeError:\n",
    "            print(\"Failed to parse JSON from response. Raw response:\")\n",
    "            print(raw_response)\n",
    "            # Return a default structure if parsing fails\n",
    "            return {\"score\": 0, \"reasoning\": \"Failed to parse response\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cellUniqueIdByVincent": "2a1cc"
   },
   "outputs": [],
   "source": [
    "class ResultScore(BaseModel):\n",
    "    score: float = Field(..., description=\"The score of the result, ranging from 0 to 1 where 1 is the best possible score.\")\n",
    "    reasoning: str = Field(..., description=\"An extensive explanation of the score.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "cellUniqueIdByVincent": "a10db"
   },
   "outputs": [],
   "source": [
    "def evaluate_relevance(question: str, context: str, generated_answer: str, llm: OpenRouterModel):\n",
    "    prompt = PromptTemplate(\n",
    "        input_variables=[\"question\", \"context\", \"generated_answer\"],\n",
    "        template = \"\"\"\n",
    "        Evaluate the RELEVANCE of the retrieved context to the given question.\n",
    "        \n",
    "        Question: {question}\n",
    "        \n",
    "        Retrieved Context: {context}\n",
    "\n",
    "        Generated Answer: {generated_answer}\n",
    "        \n",
    "        Rate the relevance on a scale of 1-5 where:\n",
    "        - 1: Completely irrelevant, no connection to the question\n",
    "        - 2: Minimally relevant, tangential connection\n",
    "        - 3: Somewhat relevant, partial connection\n",
    "        - 4: Highly relevant, strong connection\n",
    "        - 5: Perfectly relevant, directly addresses the question\n",
    "        \n",
    "        Provide your response in this format:\n",
    "        {{\n",
    "            \"score\": <number>,\n",
    "            \"reasoning\": \"<detailed explanation of why you gave this score>\"\n",
    "        }}\n",
    "        \n",
    "        Ensure your response is valid JSON.\n",
    "        \"\"\"\n",
    "    )\n",
    "\n",
    "    # Use the custom method for structured output\n",
    "    result = llm.invoke_with_json_output(\n",
    "        prompt_template=prompt.template,\n",
    "        input_variables={\n",
    "            \"question\": question,\n",
    "            \"context\": context,\n",
    "            \"generated_answer\": generated_answer\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    # Normalize the score to 0-1 range since it's on a 1-5 scale\n",
    "    if \"score\" in result and result[\"score\"] > 1:\n",
    "        result[\"score\"] = result[\"score\"] / 5.0\n",
    "        \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "cellUniqueIdByVincent": "f88c0"
   },
   "outputs": [],
   "source": [
    "def evaluate_correctness(question: str, generated_answer: str, llm: OpenRouterModel, context: Optional[str] = None):\n",
    "    prompt_template = \"\"\"\n",
    "    Evaluate the FACTUAL CORRECTNESS of the given answer to the question.\n",
    "    \n",
    "    Question: {question}\n",
    "    \n",
    "    Answer: {generated_answer}\n",
    "    \"\"\"\n",
    "    \n",
    "    # Add context if provided\n",
    "    if context:\n",
    "        prompt_template += f\"\\nContext (for reference): {context}\\n\"\n",
    "    else:\n",
    "        prompt_template += \"\\n\"\n",
    "        \n",
    "    prompt_template += \"\"\"\n",
    "    Rate the correctness on a scale of 1-5 where:\n",
    "    - 1: Completely incorrect, contains major factual errors\n",
    "    - 2: Mostly incorrect, some facts but significant errors\n",
    "    - 3: Partially correct, mix of correct and incorrect information\n",
    "    - 4: Mostly correct, minor errors or omissions\n",
    "    - 5: Completely correct, factually accurate and comprehensive\n",
    "    \n",
    "    Focus on:\n",
    "    - Factual accuracy of claims made\n",
    "    - Logical consistency\n",
    "    - Completeness of the answer\n",
    "    \n",
    "    Provide your response in this exact JSON format:\n",
    "    {{\n",
    "        \"score\": <number>,\n",
    "        \"reasoning\": \"<detailed explanation focusing on specific facts>\"\n",
    "    }}\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create input variables dict based on whether context is provided\n",
    "    input_variables = {\n",
    "        \"question\": question,\n",
    "        \"generated_answer\": generated_answer\n",
    "    }\n",
    "    \n",
    "    # Only add context to input variables if it's provided\n",
    "    if context:\n",
    "        input_variables[\"context\"] = context\n",
    "    \n",
    "    result = llm.invoke_with_json_output(\n",
    "        prompt_template=prompt_template,\n",
    "        input_variables=input_variables\n",
    "    )\n",
    "    \n",
    "    # Normalize the score to 0-1 range since it's on a 1-5 scale\n",
    "    if \"score\" in result and result[\"score\"] > 1:\n",
    "        result[\"score\"] = result[\"score\"] / 5.0\n",
    "        \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "cellUniqueIdByVincent": "28105"
   },
   "outputs": [],
   "source": [
    "def evaluate_faithfulness(question: str, generated_answer: str, llm: OpenRouterModel, context: Optional[str] = None):\n",
    "    prompt = PromptTemplate(\n",
    "        input_variables = [\"question\", \"context\", \"generated_answer\"],\n",
    "        template = \"\"\"\n",
    "        Evaluate the FAITHFULNESS of the answer to the provided context. The answer should only contain information that can be supported by or reasonably inferred from the context.\n",
    "        \n",
    "        Question: {question}\n",
    "        \n",
    "        Context: {context}\n",
    "        \n",
    "        Answer: {generated_answer}\n",
    "        \n",
    "        Rate the faithfulness on a scale of 1-5 where:\n",
    "        - 1: Completely unfaithful, answer contradicts or ignores context\n",
    "        - 2: Mostly unfaithful, some alignment but significant deviations\n",
    "        - 3: Partially faithful, generally aligned but some unsupported claims\n",
    "        - 4: Mostly faithful, well-grounded with minor unsupported details\n",
    "        - 5: Completely faithful, all claims supported by or inferable from context\n",
    "        \n",
    "        Check for:\n",
    "        - Claims that go beyond what's stated in the context\n",
    "        - Information that contradicts the context\n",
    "        - Proper grounding of all assertions\n",
    "        \n",
    "        Provide your response in this exact JSON format:\n",
    "        {{\n",
    "            \"score\": <number>,\n",
    "            \"reasoning\": \"<detailed explanation with specific examples>\"\n",
    "        }}\n",
    "        \"\"\"\n",
    "    )\n",
    "    # Use the custom method for structured output\n",
    "    result = llm.invoke_with_json_output(\n",
    "        prompt_template=prompt.template,\n",
    "        input_variables={\n",
    "            \"question\": question,\n",
    "            \"context\": context,\n",
    "            \"generated_answer\": generated_answer\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    # Normalize the score to 0-1 range since it's on a 1-5 scale\n",
    "    if \"score\" in result and result[\"score\"] > 1:\n",
    "        result[\"score\"] = result[\"score\"] / 5.0\n",
    "        \n",
    "    return result\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "cellUniqueIdByVincent": "1f302"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Relevance Score: 1.0\n",
      "Reasoning: The retrieved context directly addresses the question 'What causes climate change?' by providing a clear and concise explanation of the primary causes of climate change, including human activities such as burning fossil fuels, deforestation, and industrial processes. The generated answer accurately summarizes the main points from the retrieved context, demonstrating a perfect connection between the question, the context, and the answer. Therefore, the relevance of the retrieved context to the given question is perfectly relevant, warranting a score of 5.\n",
      "Correctness Score: 1.0\n",
      "Reasoning: The answer provided is completely correct and factually accurate. It correctly identifies human activities, particularly burning fossil fuels for energy and transportation, as the primary cause of climate change. The answer also accurately mentions deforestation and industrial processes as contributing factors by increasing greenhouse gas concentrations. The information is consistent with the context provided, which highlights the role of human activities in increasing greenhouse gas concentrations. The answer is comprehensive, covering the main points related to the causes of climate change, and does not contain any significant errors or omissions. Therefore, the answer is rated as completely correct.\n",
      "Faithfulness Score: 1.0\n",
      "Reasoning: The answer statement aligns closely with the information provided in the context. It correctly identifies human activities, specifically burning fossil fuels for energy and transportation, as primary causes of climate change due to the release of carbon dioxide. Additionally, it mentions deforestation and industrial processes as contributors, which is also supported by the context. The answer does not introduce any claims that go beyond what is stated in the context or contradict it. All assertions in the answer are properly grounded in the provided context, making it a faithful representation of the information given.\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "llm = OpenRouterModel()\n",
    "\n",
    "# Example data\n",
    "question = \"What causes climate change?\"\n",
    "context = \"Climate change is primarily caused by human activities that increase greenhouse gas concentrations in the atmosphere. The main contributors include burning fossil fuels (coal, oil, gas) for electricity, heat, and transportation, which releases carbon dioxide. Deforestation reduces the Earth's capacity to absorb CO2. Industrial processes and agriculture also contribute significantly.\"\n",
    "generated_answer = \"Climate change is primarily caused by human activities, especially burning fossil fuels for energy and transportation, which releases carbon dioxide into the atmosphere. Deforestation and industrial processes also contribute by increasing greenhouse gas concentrations.\"\n",
    "\n",
    "# Evaluate relevance\n",
    "relevance_result = evaluate_relevance(\n",
    "    question=question, \n",
    "    context=context, \n",
    "    generated_answer=generated_answer, \n",
    "    llm=llm\n",
    ")\n",
    "print(f\"Relevance Score: {relevance_result['score']}\")\n",
    "print(f\"Reasoning: {relevance_result['reasoning']}\")\n",
    "\n",
    "# Evaluate correctness\n",
    "correctness_result = evaluate_correctness(\n",
    "    question=question, \n",
    "    generated_answer=generated_answer, \n",
    "    llm=llm, \n",
    "    context=context\n",
    ")\n",
    "print(f\"Correctness Score: {correctness_result['score']}\")\n",
    "print(f\"Reasoning: {correctness_result['reasoning']}\")\n",
    "\n",
    "# Evaluate faithfulness\n",
    "faithfulness_result = evaluate_faithfulness(\n",
    "    question=question, \n",
    "    generated_answer=generated_answer, \n",
    "    llm=llm, \n",
    "    context=context\n",
    ")\n",
    "print(f\"Faithfulness Score: {faithfulness_result['score']}\")\n",
    "print(f\"Reasoning: {faithfulness_result['reasoning']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellUniqueIdByVincent": "6865c"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  },
  "vincent": {
   "sessionId": "b75523e7856d7a763194bb99_2025-05-29T13-09-17-251Z"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
