{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellUniqueIdByVincent": "0a411"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from typing import List, Any, Tuple, Dict, Optional\n",
    "from dataclasses import dataclass\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# Your existing classes\n",
    "@dataclass\n",
    "class RAGEvaluationResult:\n",
    "    \"\"\"Structure to hold evaluation results for a single question\"\"\"\n",
    "    question: str\n",
    "    context: str\n",
    "    generated_answer: str\n",
    "    relevance_score: float\n",
    "    correctness_score: float\n",
    "    faithfulness_score: float\n",
    "    hallucination_score: float\n",
    "    detailed_feedback: Dict[str, str]\n",
    "    overall_score: float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellUniqueIdByVincent": "d50d5"
   },
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class OpenRouterModel:\n",
    "    api_key: Optional[str] = os.getenv(\"OPENROUTER_API_KEY\")\n",
    "    model: str = \"meta-llama/llama-3.3-70b-instruct\"\n",
    "\n",
    "    def __post_init__(self):\n",
    "        if not self.api_key:\n",
    "            raise ValueError(\"Please set a valid api key in the environment variables\")\n",
    "        self.client = OpenAI(\n",
    "            base_url=\"https://openrouter.ai/api/v1\",\n",
    "            api_key=self.api_key,\n",
    "        )\n",
    "        \n",
    "\n",
    "    def invoke(self, prompt: str, temperature: float = 0.0, max_tokens: int = 2000) -> str:\n",
    "        \"\"\"Generate response using Llama 3.3 70B Instruct\"\"\"\n",
    "        try:\n",
    "            response = self.client.chat.completions.create(\n",
    "                model=self.model,\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                temperature=temperature,\n",
    "                max_tokens=max_tokens\n",
    "            )\n",
    "            return response.choices[0].message.content\n",
    "        except Exception as e:\n",
    "            print(f\"Error calling OpenRouter API: {e}\")\n",
    "            return \"\"\n",
    "        \n",
    "    def invoke_with_json_output(self, prompt_template, input_variables):\n",
    "        \"\"\"Generate structured JSON output from the model\"\"\"\n",
    "        \n",
    "        formatted_prompt = prompt_template.format(**input_variables)\n",
    "        \n",
    "        # Get the raw response from the model\n",
    "        raw_response = self.invoke(formatted_prompt)\n",
    "        \n",
    "        # Extract JSON from the response\n",
    "        try:\n",
    "            # Find JSON content within the response\n",
    "            json_start = raw_response.find('{')\n",
    "            json_end = raw_response.rfind('}') + 1\n",
    "            \n",
    "            if json_start >= 0 and json_end > json_start:\n",
    "                json_str = raw_response[json_start:json_end]\n",
    "                result = json.loads(json_str)\n",
    "                return result\n",
    "            else:\n",
    "                # If no JSON found, try to parse the whole response\n",
    "                return json.loads(raw_response)\n",
    "        except json.JSONDecodeError:\n",
    "            print(\"Failed to parse JSON from response. Raw response:\")\n",
    "            print(raw_response)\n",
    "            # Return a default structure if parsing fails\n",
    "            return {\"score\": 0, \"reasoning\": \"Failed to parse response\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellUniqueIdByVincent": "89d38"
   },
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Evaluation:\n",
    "    question:str\n",
    "    context: Optional[str] = None\n",
    "    generated_answer: str\n",
    "    llm: OpenRouterModel\n",
    "\n",
    "    def evaluate_relevance(question, context, generated_answer, llm):\n",
    "        prompt = PromptTemplate(\n",
    "        input_variables=[\"question\", \"context\", \"generated_answer\"],\n",
    "        template = \"\"\"\n",
    "        Evaluate the RELEVANCE of the retrieved context to the given question.\n",
    "        \n",
    "        Question: {question}\n",
    "        \n",
    "        Retrieved Context: {context}\n",
    "\n",
    "        Generated Answer: {generated_answer}\n",
    "        \n",
    "        Rate the relevance on a scale of 1-5 where:\n",
    "        - 1: Completely irrelevant, no connection to the question\n",
    "        - 2: Minimally relevant, tangential connection\n",
    "        - 3: Somewhat relevant, partial connection\n",
    "        - 4: Highly relevant, strong connection\n",
    "        - 5: Perfectly relevant, directly addresses the question\n",
    "        \n",
    "        Provide your response in this format:\n",
    "        {{\n",
    "            \"score\": <number>,\n",
    "            \"reasoning\": \"<detailed explanation of why you gave this score>\"\n",
    "        }}\n",
    "        \n",
    "        Ensure your response is a valid JSON\n",
    "        \"\"\"\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "cellUniqueIdByVincent": "4977e"
   },
   "outputs": [],
   "source": [
    "from functools import wraps\n",
    "\n",
    "def score(func):\n",
    "    @wraps(func)\n",
    "    def wrapper(*args, **kwargs):\n",
    "        result = func(*args, **kwargs)\n",
    "        if isinstance(result, dict) and \"score\" in result:\n",
    "            try:\n",
    "                result[\"score\"] = (result[\"score\"] - 1) / 4.0\n",
    "            except Exception as e:\n",
    "                raise ValueError(f\"Normalization failed: {e}\")\n",
    "        return result\n",
    "    return wrapper\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "cellUniqueIdByVincent": "a10db"
   },
   "outputs": [],
   "source": [
    "@score\n",
    "def evaluate_relevance(question: str, context: str, generated_answer: str, llm: OpenRouterModel):\n",
    "    prompt = PromptTemplate(\n",
    "        input_variables=[\"question\", \"context\", \"generated_answer\"],\n",
    "        template = \"\"\"\n",
    "        Evaluate the RELEVANCE of the retrieved context to the given question.\n",
    "        \n",
    "        Question: {question}\n",
    "        \n",
    "        Retrieved Context: {context}\n",
    "\n",
    "        Generated Answer: {generated_answer}\n",
    "        \n",
    "        Rate the relevance on a scale of 1-5 where:\n",
    "        - 1: Completely irrelevant, no connection to the question\n",
    "        - 2: Minimally relevant, tangential connection\n",
    "        - 3: Somewhat relevant, partial connection\n",
    "        - 4: Highly relevant, strong connection\n",
    "        - 5: Perfectly relevant, directly addresses the question\n",
    "        \n",
    "        Provide your response in this format:\n",
    "        {{\n",
    "            \"score\": <number>,\n",
    "            \"reasoning\": \"<detailed explanation of why you gave this score>\"\n",
    "        }}\n",
    "        \n",
    "        Ensure your response is valid JSON.\n",
    "        \"\"\"\n",
    "    )\n",
    "\n",
    "    # Use the custom method for structured output\n",
    "    return llm.invoke_with_json_output(\n",
    "        prompt_template=prompt.template,\n",
    "        input_variables={\n",
    "            \"question\": question,\n",
    "            \"context\": context,\n",
    "            \"generated_answer\": generated_answer\n",
    "        }\n",
    "    )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "cellUniqueIdByVincent": "f88c0"
   },
   "outputs": [],
   "source": [
    "@score\n",
    "def evaluate_correctness(question: str, generated_answer: str, llm: OpenRouterModel, context: Optional[str] = None):\n",
    "    prompt_template = \"\"\"\n",
    "    Evaluate the FACTUAL CORRECTNESS of the given answer to the question.\n",
    "    \n",
    "    Question: {question}\n",
    "    \n",
    "    Answer: {generated_answer}\n",
    "    \"\"\"\n",
    "    \n",
    "    # Add context if provided\n",
    "    if context:\n",
    "        prompt_template += f\"\\nContext (for reference): {context}\\n\"\n",
    "    else:\n",
    "        prompt_template += \"\\n\"\n",
    "        \n",
    "    prompt_template += \"\"\"\n",
    "    Rate the correctness on a scale of 1-5 where:\n",
    "    - 1: Completely incorrect, contains major factual errors\n",
    "    - 2: Mostly incorrect, some facts but significant errors\n",
    "    - 3: Partially correct, mix of correct and incorrect information\n",
    "    - 4: Mostly correct, minor errors or omissions\n",
    "    - 5: Completely correct, factually accurate and comprehensive\n",
    "    \n",
    "    Focus on:\n",
    "    - Factual accuracy of claims made\n",
    "    - Logical consistency\n",
    "    - Completeness of the answer\n",
    "    \n",
    "    Provide your response in this exact JSON format:\n",
    "    {{\n",
    "        \"score\": <number>,\n",
    "        \"reasoning\": \"<detailed explanation focusing on specific facts>\"\n",
    "    }}\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create input variables dict based on whether context is provided\n",
    "    input_variables = {\n",
    "        \"question\": question,\n",
    "        \"generated_answer\": generated_answer\n",
    "    }\n",
    "    \n",
    "    # Only add context to input variables if it's provided\n",
    "    if context:\n",
    "        input_variables[\"context\"] = context\n",
    "    \n",
    "    return llm.invoke_with_json_output(\n",
    "        prompt_template=prompt_template,\n",
    "        input_variables=input_variables\n",
    "    )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "cellUniqueIdByVincent": "28105"
   },
   "outputs": [],
   "source": [
    "@score\n",
    "def evaluate_faithfulness(question: str, generated_answer: str, llm: OpenRouterModel, context: Optional[str] = None):\n",
    "    prompt = PromptTemplate(\n",
    "        input_variables = [\"question\", \"context\", \"generated_answer\"],\n",
    "        template = \"\"\"\n",
    "        Evaluate the FAITHFULNESS of the answer to the provided context. The answer should only contain information that can be supported by or reasonably inferred from the context.\n",
    "        \n",
    "        Question: {question}\n",
    "        \n",
    "        Context: {context}\n",
    "        \n",
    "        Answer: {generated_answer}\n",
    "        \n",
    "        Rate the faithfulness on a scale of 1-5 where:\n",
    "        - 1: Completely unfaithful, answer contradicts or ignores context\n",
    "        - 2: Mostly unfaithful, some alignment but significant deviations\n",
    "        - 3: Partially faithful, generally aligned but some unsupported claims\n",
    "        - 4: Mostly faithful, well-grounded with minor unsupported details\n",
    "        - 5: Completely faithful, all claims supported by or inferable from context\n",
    "        \n",
    "        Check for:\n",
    "        - Claims that go beyond what's stated in the context\n",
    "        - Information that contradicts the context\n",
    "        - Proper grounding of all assertions\n",
    "        \n",
    "        Provide your response in this exact JSON format:\n",
    "        {{\n",
    "            \"score\": <number>,\n",
    "            \"reasoning\": \"<detailed explanation with specific examples>\"\n",
    "        }}\n",
    "        \"\"\"\n",
    "    )\n",
    "    # Use the custom method for structured output\n",
    "    return llm.invoke_with_json_output(\n",
    "        prompt_template=prompt.template,\n",
    "        input_variables={\n",
    "            \"question\": question,\n",
    "            \"context\": context,\n",
    "            \"generated_answer\": generated_answer\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "cellUniqueIdByVincent": "d99c6"
   },
   "outputs": [],
   "source": [
    "@score\n",
    "def evaluate_hallucination(question: str, generated_answer: str, llm: OpenRouterModel, context: Optional[str] = None):\n",
    "    prompt = PromptTemplate(\n",
    "        input_variables = [\"question\", \"context\", \"generated_answer\"],\n",
    "        template = \"\"\"\n",
    "        Evaluate whether the answer contains HALLUCINATED information - facts, claims, or details that are NOT present in the provided context.\n",
    "        \n",
    "        Question: {question}\n",
    "        \n",
    "        Context: {context}\n",
    "        \n",
    "        Answer: {generated_answer}\n",
    "        \n",
    "        Rate hallucination on a scale of 1-5 where:\n",
    "        - 1: Severe hallucination, answer contains mostly fabricated information\n",
    "        - 2: Significant hallucination, multiple unsupported claims\n",
    "        - 3: Moderate hallucination, some fabricated details\n",
    "        - 4: Minor hallucination, mostly grounded with few unsupported claims\n",
    "        - 5: No hallucination, all information comes from or is inferable from context\n",
    "        \n",
    "        Specifically look for:\n",
    "        - Facts mentioned in answer but not in context\n",
    "        - Specific numbers, dates, names not in context\n",
    "        - Detailed explanations beyond what context provides\n",
    "        - Claims that go beyond reasonable inference\n",
    "        \n",
    "        Provide your response in this exact JSON format:\n",
    "        {{\n",
    "            \"score\": <number>,\n",
    "            \"reasoning\": \"<detailed explanation identifying specific hallucinations if any>\"\n",
    "        }}\n",
    "        \"\"\"\n",
    "    )\n",
    "    # Use the custom method for structured output\n",
    "    return llm.invoke_with_json_output(\n",
    "        prompt_template=prompt.template,\n",
    "        input_variables={\n",
    "            \"question\": question,\n",
    "            \"context\": context,\n",
    "            \"generated_answer\": generated_answer\n",
    "        }\n",
    "    )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "cellUniqueIdByVincent": "1f302"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Relevance Score: 1.0\n",
      "Reasoning: The retrieved context directly addresses the question 'What causes climate change?' by providing a clear explanation of the primary causes, including human activities such as burning fossil fuels, deforestation, industrial processes, and agriculture. The generated answer further clarifies this by mentioning both natural and human factors, with a focus on the significant role of human activities. The information provided in the retrieved context is comprehensive and directly relevant to understanding the causes of climate change, making it perfectly relevant to the given question.\n",
      "Correctness Score: 1.0\n",
      "Reasoning: The answer provided accurately identifies both natural and human factors contributing to climate change. It correctly highlights the significant role of human activities such as burning fossil fuels and deforestation in the current climate change scenario. The mention of increased greenhouse gases from industrial processes and transportation further supports the factual correctness of the answer. The context provided for reference aligns with the information given in the answer, emphasizing the primary causes of climate change, including the burning of fossil fuels, deforestation, and contributions from industrial processes and agriculture. The answer is comprehensive, logically consistent, and factually accurate, making it completely correct.\n",
      "Faithfulness Score: 0.75\n",
      "Reasoning: The answer is mostly faithful to the context as it correctly identifies human activities such as burning fossil fuels and deforestation as primary causes of climate change, which is directly supported by the context. It also mentions industrial processes, which is in line with the context. However, the answer introduces 'volcanic eruptions and variations in solar radiation' as natural factors contributing to climate change, which is not mentioned in the provided context. This introduction of additional, unsupported information prevents the answer from being completely faithful. Nonetheless, the core of the answer remains well-grounded in the context, with the majority of the claims being supported or inferable from it.\n",
      "Hallucination Score: 0.75\n",
      "Reasoning: The answer mentions both natural and human factors contributing to climate change, which is a reasonable inference from the context that primarily focuses on human activities. However, it introduces 'volcanic eruptions and variations in solar radiation' as natural factors, which are not mentioned in the provided context. This introduction of specific natural factors not covered in the context constitutes a minor hallucination, as the context does imply that human activities are the primary cause of current climate change but does not explicitly discuss natural factors. The rest of the information in the answer, such as the impact of burning fossil fuels, deforestation, industrial processes, and transportation, is directly supported by the context.\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "llm = OpenRouterModel()\n",
    "\n",
    "# Example data\n",
    "question = \"What causes climate change?\"\n",
    "context = \"Climate change is primarily caused by human activities that increase greenhouse gas concentrations in the atmosphere. The main contributors include burning fossil fuels (coal, oil, gas) for electricity, heat, and transportation, which releases carbon dioxide. Deforestation reduces the Earth's capacity to absorb CO2. Industrial processes and agriculture also contribute significantly.\"\n",
    "generated_answer = \"\"\"\n",
    "Climate change is caused by both natural and human factors. While volcanic eruptions and variations in solar radiation have long-term impacts, the current changes are largely due to human activities such as burning fossil fuels and deforestation. Increased greenhouse gases from industrial processes and transportation play a significant role.\"\"\"\n",
    "\n",
    "# Evaluate relevance\n",
    "relevance_result = evaluate_relevance(\n",
    "    question=question, \n",
    "    context=context, \n",
    "    generated_answer=generated_answer, \n",
    "    llm=llm\n",
    ")\n",
    "print(f\"Relevance Score: {relevance_result['score']}\")\n",
    "print(f\"Reasoning: {relevance_result['reasoning']}\")\n",
    "\n",
    "# Evaluate correctness\n",
    "correctness_result = evaluate_correctness(\n",
    "    question=question, \n",
    "    generated_answer=generated_answer, \n",
    "    llm=llm, \n",
    "    context=context\n",
    ")\n",
    "print(f\"Correctness Score: {correctness_result['score']}\")\n",
    "print(f\"Reasoning: {correctness_result['reasoning']}\")\n",
    "\n",
    "# Evaluate faithfulness\n",
    "faithfulness_result = evaluate_faithfulness(\n",
    "    question=question, \n",
    "    generated_answer=generated_answer, \n",
    "    llm=llm, \n",
    "    context=context\n",
    ")\n",
    "print(f\"Faithfulness Score: {faithfulness_result['score']}\")\n",
    "print(f\"Reasoning: {faithfulness_result['reasoning']}\")\n",
    "\n",
    "# Evaluate faithfulness\n",
    "hallucination_result = evaluate_hallucination(\n",
    "    question=question, \n",
    "    generated_answer=generated_answer, \n",
    "    llm=llm, \n",
    "    context=context\n",
    ")\n",
    "print(f\"Hallucination Score: {hallucination_result['score']}\")\n",
    "print(f\"Reasoning: {hallucination_result['reasoning']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellUniqueIdByVincent": "6865c"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  },
  "vincent": {
   "sessionId": "b75523e7856d7a763194bb99_2025-05-29T13-09-17-251Z"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
