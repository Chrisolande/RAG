{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cellUniqueIdByVincent": "0a411"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from typing import List, Any, Tuple, Dict, Optional\n",
    "from dataclasses import dataclass\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "@dataclass\n",
    "class RAGEvaluationResult:\n",
    "    \"\"\"Structure to hold evaluation results for a single question\"\"\"\n",
    "    question: str\n",
    "    context: str\n",
    "    generated_answer: str\n",
    "    relevance_score: float\n",
    "    correctness_score: float\n",
    "    faithfulness_score: float\n",
    "    hallucination_score: float\n",
    "    detailed_feedback: Dict[str, str]\n",
    "    overall_score: float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "cellUniqueIdByVincent": "e2e6b"
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "def score(func):\n",
    "    @wraps(func)\n",
    "    def wrapper(*args, **kwargs):\n",
    "        result = func(*args, **kwargs)\n",
    "\n",
    "        # Handle JSON string\n",
    "        if isinstance(result, str):\n",
    "            result = json.loads(result)\n",
    "\n",
    "        if \"score\" in result:\n",
    "            raw_score = result[\"score\"]\n",
    "            normalized_score = (raw_score - 1) / 4.0\n",
    "            result[\"score\"] = normalized_score\n",
    "\n",
    "            # Log the transformation\n",
    "            logging.info(f\"Raw score: {raw_score}, Normalized: {normalized_score}\")\n",
    "        else:\n",
    "            logging.warning(\"Score key not found in result.\")\n",
    "\n",
    "        return result\n",
    "    return wrapper\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "cellUniqueIdByVincent": "d50d5"
   },
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class OpenRouterModel:\n",
    "    api_key: Optional[str] = os.getenv(\"OPENROUTER_API_KEY\")\n",
    "    model: str = \"meta-llama/llama-3.3-70b-instruct\"\n",
    "\n",
    "    def __post_init__(self):\n",
    "        if not self.api_key:\n",
    "            raise ValueError(\"Please set a valid api key in the environment variables\")\n",
    "        self.client = OpenAI(\n",
    "            base_url=\"https://openrouter.ai/api/v1\",\n",
    "            api_key=self.api_key,\n",
    "        )\n",
    "        \n",
    "\n",
    "    def invoke(self, prompt: str, temperature: float = 0.1, max_tokens: int = 2000) -> str:\n",
    "        \"\"\"Generate response using Llama 3.3 70B Instruct\"\"\"\n",
    "        try:\n",
    "            response = self.client.chat.completions.create(\n",
    "                model=self.model,\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                temperature=temperature,\n",
    "                max_tokens=max_tokens\n",
    "            )\n",
    "            return response.choices[0].message.content\n",
    "        except Exception as e:\n",
    "            print(f\"Error calling OpenRouter API: {e}\")\n",
    "            return \"\"\n",
    "        \n",
    "    def invoke_with_json_output(self, prompt_template, input_variables):\n",
    "        \"\"\"Generate structured JSON output from the model\"\"\"\n",
    "        \n",
    "        formatted_prompt = prompt_template.format(**input_variables)\n",
    "        \n",
    "        # Get the raw response from the model\n",
    "        raw_response = self.invoke(formatted_prompt)\n",
    "        \n",
    "        # Extract JSON from the response\n",
    "        try:\n",
    "            # Find JSON content within the response\n",
    "            json_start = raw_response.find('{')\n",
    "            json_end = raw_response.rfind('}') + 1\n",
    "            \n",
    "            if json_start >= 0 and json_end > json_start:\n",
    "                json_str = raw_response[json_start:json_end]\n",
    "                result = json.loads(json_str)\n",
    "                return result\n",
    "            else:\n",
    "                # If no JSON found, try to parse the whole response\n",
    "                return json.loads(raw_response)\n",
    "        except json.JSONDecodeError:\n",
    "            print(\"Failed to parse JSON from response. Raw response:\")\n",
    "            print(raw_response)\n",
    "            # Return a default structure if parsing fails\n",
    "            return {\"score\": 0, \"reasoning\": \"Failed to parse response\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "cellUniqueIdByVincent": "d50ff"
   },
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Evaluation:\n",
    "    question: str\n",
    "    generated_answer: str\n",
    "    llm: OpenRouterModel\n",
    "    context: Optional[str] = None\n",
    "\n",
    "    def _evaluate_with_template(self, template: str, input_vars: Dict[str, str]) -> Dict:\n",
    "        \"\"\"Generic method to handle all evaluations with templates.\"\"\"\n",
    "        return self.llm.invoke_with_json_output(\n",
    "            prompt_template=template,\n",
    "            input_variables=input_vars\n",
    "        )\n",
    "\n",
    "    def _get_base_input_variables(self) -> Dict[str, str]:\n",
    "        \"\"\"Get the base input variables used by most evaluation methods.\"\"\"\n",
    "        return {\n",
    "            \"question\": self.question,\n",
    "            \"context\": self.context,\n",
    "            \"generated_answer\": self.generated_answer\n",
    "        }\n",
    "\n",
    "    @score\n",
    "    def evaluate_relevance(self) -> Dict:\n",
    "        template = \"\"\"\n",
    "        Evaluate the RELEVANCE of the retrieved context to the given question.\n",
    "        \n",
    "        Question: {question}\n",
    "        \n",
    "        Retrieved Context: {context}\n",
    "\n",
    "        Generated Answer: {generated_answer}\n",
    "        \n",
    "        Rate the relevance on a scale of 1-5 where:\n",
    "        - 1: Completely irrelevant, no connection to the question\n",
    "        - 2: Minimally relevant, tangential connection\n",
    "        - 3: Somewhat relevant, partial connection\n",
    "        - 4: Highly relevant, strong connection\n",
    "        - 5: Perfectly relevant, directly addresses the question\n",
    "        \n",
    "        Provide your response in this format:\n",
    "        {{\n",
    "            \"score\": <number>,\n",
    "            \"reasoning\": \"<detailed explanation of why you gave this score>\"\n",
    "        }}\n",
    "        \n",
    "        Ensure your response is valid JSON.\n",
    "        \"\"\"\n",
    "        \n",
    "        return self._evaluate_with_template(template, self._get_base_input_variables())\n",
    "\n",
    "    @score\n",
    "    def evaluate_correctness(self) -> Dict:\n",
    "        template = \"\"\"\n",
    "        Evaluate the FACTUAL CORRECTNESS of the given answer to the question.\n",
    "        \n",
    "        Question: {question}\n",
    "        \n",
    "        Answer: {generated_answer}\n",
    "        \"\"\"\n",
    "        \n",
    "        # Add context if provided\n",
    "        if self.context:\n",
    "            template += f\"\\nContext (for reference): {{context}}\\n\"\n",
    "        else:\n",
    "            template += \"\\n\"\n",
    "            \n",
    "        template += \"\"\"\n",
    "        Rate the correctness on a scale of 1-5 where:\n",
    "        - 1: Completely incorrect, contains major factual errors\n",
    "        - 2: Mostly incorrect, some facts but significant errors\n",
    "        - 3: Partially correct, mix of correct and incorrect information\n",
    "        - 4: Mostly correct, minor errors or omissions\n",
    "        - 5: Completely correct, factually accurate and comprehensive\n",
    "        \n",
    "        Focus on:\n",
    "        - Factual accuracy of claims made\n",
    "        - Logical consistency\n",
    "        - Completeness of the answer\n",
    "        \n",
    "        Provide your response in this exact JSON format:\n",
    "        {{\n",
    "            \"score\": <number>,\n",
    "            \"reasoning\": \"<detailed explanation focusing on specific facts>\"\n",
    "        }}\n",
    "        \"\"\"\n",
    "        \n",
    "        # Create input variables dict based on whether context is provided\n",
    "        input_variables = {\n",
    "            \"question\": self.question,\n",
    "            \"generated_answer\": self.generated_answer\n",
    "        }\n",
    "        \n",
    "        # Only add context to input variables if it's provided\n",
    "        if self.context:\n",
    "            input_variables[\"context\"] = self.context\n",
    "        \n",
    "        return self._evaluate_with_template(template, input_variables)\n",
    "\n",
    "    @score\n",
    "    def evaluate_faithfulness(self) -> Dict:\n",
    "        template = \"\"\"\n",
    "        Evaluate the FAITHFULNESS of the answer to the provided context. The answer should only contain information that can be supported by or reasonably inferred from the context.\n",
    "        \n",
    "        Question: {question}\n",
    "        \n",
    "        Context: {context}\n",
    "        \n",
    "        Answer: {generated_answer}\n",
    "        \n",
    "        Rate the faithfulness on a scale of 1-5 where:\n",
    "        - 1: Completely unfaithful, answer contradicts or ignores context\n",
    "        - 2: Mostly unfaithful, some alignment but significant deviations\n",
    "        - 3: Partially faithful, generally aligned but some unsupported claims\n",
    "        - 4: Mostly faithful, well-grounded with minor unsupported details\n",
    "        - 5: Completely faithful, all claims supported by or inferable from context\n",
    "        \n",
    "        Check for:\n",
    "        - Claims that go beyond what's stated in the context\n",
    "        - Information that contradicts the context\n",
    "        - Proper grounding of all assertions\n",
    "        \n",
    "        Provide your response in this exact JSON format:\n",
    "        {{\n",
    "            \"score\": <number>,\n",
    "            \"reasoning\": \"<detailed explanation with specific examples>\"\n",
    "        }}\n",
    "        \"\"\"\n",
    "        \n",
    "        return self._evaluate_with_template(template, self._get_base_input_variables())\n",
    "\n",
    "    @score\n",
    "    def evaluate_hallucination(self) -> Dict:\n",
    "        template = \"\"\"\n",
    "        Evaluate whether the answer contains HALLUCINATED information - facts, claims, or details that are NOT present in the provided context.\n",
    "        \n",
    "        Question: {question}\n",
    "        \n",
    "        Context: {context}\n",
    "        \n",
    "        Answer: {generated_answer}\n",
    "        \n",
    "        Rate hallucination on a scale of 1-5 where:\n",
    "        - 1: Severe hallucination, answer contains mostly fabricated information\n",
    "        - 2: Significant hallucination, multiple unsupported claims\n",
    "        - 3: Moderate hallucination, some fabricated details\n",
    "        - 4: Minor hallucination, mostly grounded with few unsupported claims\n",
    "        - 5: No hallucination, all information comes from or is inferable from context\n",
    "        \n",
    "        Specifically look for:\n",
    "        - Facts mentioned in answer but not in context\n",
    "        - Specific numbers, dates, names not in context\n",
    "        - Detailed explanations beyond what context provides\n",
    "        - Claims that go beyond reasonable inference\n",
    "        \n",
    "        Provide your response in this exact JSON format:\n",
    "        {{\n",
    "            \"score\": <number>,\n",
    "            \"reasoning\": \"<detailed explanation identifying specific hallucinations if any>\"\n",
    "        }}\n",
    "        \"\"\"\n",
    "        \n",
    "        return self._evaluate_with_template(template, self._get_base_input_variables())\n",
    "\n",
    "    def comprehensive_evaluation(self):\n",
    "        print(f\"Evaluating {self.question[:500]}...\")\n",
    "        # Extract Each metric\n",
    "        relevance_score, relevance_reasoning = self.evaluate_relevance().values()\n",
    "        correctness_score, correctness_reasoning = self.evaluate_correctness().values()\n",
    "        faithfulness_score, faithfulness_reasoning = self.evaluate_faithfulness().values()\n",
    "        hallucination_score, hallucination_reasoning = self.evaluate_hallucination().values()\n",
    "\n",
    "        # Perform a weighted average\n",
    "        overall_score = (\n",
    "            relevance_score * 0.25 + \n",
    "            correctness_score * 0.30 + \n",
    "            faithfulness_score * 0.25 + \n",
    "            hallucination_score * 0.20\n",
    "        )\n",
    "\n",
    "        return RAGEvaluationResult(\n",
    "            question=self.question,\n",
    "            context=self.context[:500] + \"...\" if len(self.context) > 500 else self.context,\n",
    "            generated_answer=self.generated_answer,\n",
    "            relevance_score=relevance_score,\n",
    "            correctness_score=correctness_score,\n",
    "            faithfulness_score=faithfulness_score,\n",
    "            hallucination_score=hallucination_score,\n",
    "            detailed_feedback={\n",
    "                \"relevance\": relevance_reasoning,\n",
    "                \"correctness\": correctness_reasoning,\n",
    "                \"faithfulness\": faithfulness_reasoning,\n",
    "                \"hallucination\": hallucination_reasoning\n",
    "            },\n",
    "            overall_score=overall_score\n",
    "        )\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellUniqueIdByVincent": "ed4ab"
   },
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class TestCase:\n",
    "    question: str\n",
    "    context: str\n",
    "    expected_quality: str  # \"high\", \"medium\", \"low\" for evaluation testing\n",
    "    notes: str = \"\"\n",
    "\n",
    "TEST_CASES = [\n",
    "    TestCase(\n",
    "        question=\"What is the capital of France?\",\n",
    "        context=\"France is a country in Western Europe. Paris is the capital and largest city of France, located in the north-central part of the country along the Seine River. The city has a population of over 2 million people within its administrative limits.\",\n",
    "        expected_quality=\"high\",\n",
    "        notes=\"Simple factual question with directly relevant context\"\n",
    "    ),\n",
    "    \n",
    "    TestCase(\n",
    "        question=\"How does photosynthesis work?\",\n",
    "        context=\"Photosynthesis is the process by which plants convert light energy into chemical energy. During photosynthesis, plants use sunlight, carbon dioxide from the air, and water from their roots to produce glucose and oxygen. The process occurs mainly in the chloroplasts of plant cells, specifically in structures called thylakoids. Chlorophyll, the green pigment in plants, captures light energy to drive this process.\",\n",
    "        expected_quality=\"high\",\n",
    "        notes=\"Scientific explanation with comprehensive context\"\n",
    "    ),\n",
    "    \n",
    "    TestCase(\n",
    "        question=\"What are the side effects of aspirin?\",\n",
    "        context=\"Aspirin is a common pain reliever and anti-inflammatory medication. It belongs to a class of drugs called nonsteroidal anti-inflammatory drugs (NSAIDs). Common side effects include stomach upset, heartburn, and nausea. More serious side effects can include stomach bleeding, ulcers, and increased risk of bleeding disorders. Aspirin should not be given to children due to risk of Reye's syndrome.\",\n",
    "        expected_quality=\"high\",\n",
    "        notes=\"Medical information with relevant safety context\"\n",
    "    ),\n",
    "    \n",
    "    TestCase(\n",
    "        question=\"What is the GDP of Brazil in 2024?\",\n",
    "        context=\"Brazil is the largest economy in South America. The country has experienced various economic challenges in recent years, including inflation and political instability. Brazil's main exports include soybeans, coffee, and iron ore. The Brazilian real is the country's currency.\",\n",
    "        expected_quality=\"low\",\n",
    "        notes=\"Specific data question with irrelevant context - should test hallucination\"\n",
    "    ),\n",
    "    \n",
    "    TestCase(\n",
    "        question=\"How do you bake chocolate chip cookies?\",\n",
    "        context=\"Chocolate chip cookies were invented by Ruth Wakefield in 1938 at the Toll House Inn. The original recipe called for butter, sugar, eggs, flour, and chocolate chips. Baking typically requires preheating the oven and careful timing to achieve the right texture.\",\n",
    "        expected_quality=\"medium\",\n",
    "        notes=\"Recipe question with historical context but missing specific instructions\"\n",
    "    ),\n",
    "    \n",
    "    TestCase(\n",
    "        question=\"What causes climate change?\",\n",
    "        context=\"The Earth's atmosphere contains greenhouse gases like carbon dioxide, methane, and water vapor. These gases trap heat from the sun, creating a greenhouse effect. Human activities such as burning fossil fuels, deforestation, and industrial processes have significantly increased the concentration of greenhouse gases in the atmosphere since the Industrial Revolution, leading to global warming.\",\n",
    "        expected_quality=\"high\",\n",
    "        notes=\"Environmental science with comprehensive explanation\"\n",
    "    ),\n",
    "    \n",
    "    TestCase(\n",
    "        question=\"Who won the 2023 FIFA World Cup?\",\n",
    "        context=\"The FIFA World Cup is held every four years and is the most prestigious tournament in international football. The tournament features 32 national teams competing over several weeks. Previous winners include Brazil, Germany, Argentina, and France. The tournament generates billions in revenue and attracts viewers worldwide.\",\n",
    "        expected_quality=\"low\",\n",
    "        notes=\"Specific event question with general context - tests factual accuracy\"\n",
    "    ),\n",
    "    \n",
    "    TestCase(\n",
    "        question=\"What are the benefits of exercise?\",\n",
    "        context=\"Regular physical activity has numerous health benefits. Exercise can improve cardiovascular health by strengthening the heart and improving circulation. It also helps maintain healthy weight, builds muscle strength and bone density, and can reduce the risk of chronic diseases like diabetes and hypertension. Additionally, exercise releases endorphins which can improve mood and reduce stress.\",\n",
    "        expected_quality=\"high\",\n",
    "        notes=\"Health question with comprehensive relevant context\"\n",
    "    ),\n",
    "    \n",
    "    TestCase(\n",
    "        question=\"How does blockchain technology work?\",\n",
    "        context=\"Cryptocurrency markets have been volatile in recent years. Bitcoin was the first cryptocurrency, created by an anonymous person or group known as Satoshi Nakamoto. Many companies are now accepting cryptocurrency as payment. The value of cryptocurrencies can fluctuate dramatically based on market sentiment.\",\n",
    "        expected_quality=\"low\",\n",
    "        notes=\"Technical question with tangentially related context about crypto\"\n",
    "    ),\n",
    "    \n",
    "    TestCase(\n",
    "        question=\"What is the treatment for type 2 diabetes?\",\n",
    "        context=\"Type 2 diabetes is a chronic condition that affects how the body processes blood sugar (glucose). Treatment typically involves lifestyle changes such as diet modification and regular exercise. Medications may include metformin, insulin, or other blood sugar-lowering drugs. Regular monitoring of blood glucose levels is important. Some patients may also benefit from weight loss surgery in severe cases.\",\n",
    "        expected_quality=\"high\",\n",
    "        notes=\"Medical treatment question with appropriate clinical context\"\n",
    "    ),\n",
    "]\n",
    "\n",
    "async def generate_answer_with_llm_async(question: str, context: str, llm_model) -> str:\n",
    "    \"\"\"\n",
    "    Asynchronously generate answers with the llm\n",
    "    \n",
    "    Args:\n",
    "        question: The question to answer\n",
    "        context: The context/background information\n",
    "        llm_model: Your LLM model instance \n",
    "    \n",
    "    Returns:\n",
    "        Generated answer string\n",
    "    \"\"\"\n",
    "    prompt_template = \"\"\"\n",
    "    Based on the provided context, please answer the following question. \n",
    "    Use only the information provided in the context. If the context doesn't \n",
    "    contain enough information to fully answer the question, say so explicitly.\n",
    "    \n",
    "    Context: {context}\n",
    "    \n",
    "    Question: {question}\n",
    "    \n",
    "    Answer:\n",
    "    \"\"\"\n",
    "    \n",
    "    # Format the prompt\n",
    "    formatted_prompt = prompt_template.format(\n",
    "        context=context,\n",
    "        question=question\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        # Use async invoke else switch to .to_thread for non_async models\n",
    "        if hasattr(llm_model, 'ainvoke'):\n",
    "            response = await llm_model.ainvoke(formatted_prompt)\n",
    "        else:\n",
    "            # Fallback for non-async models - run in thread pool\n",
    "            response = await asyncio.to_thread(llm_model.invoke, formatted_prompt)\n",
    "        \n",
    "        return response.strip()\n",
    "    except Exception as e:\n",
    "        return f\"Error generating answer: {str(e)}\"\n",
    "\n",
    "async def run_evaluation_test_async(test_case: TestCase, llm_model, evaluator_llm) -> Dict:\n",
    "    \"\"\"\n",
    "    Asynchronusly run the evaluation tests.\n",
    "    \n",
    "    Args:\n",
    "        test_case: TestCase instance with question and context\n",
    "        llm_model: LLM for generating answers\n",
    "        evaluator_llm: LLM for evaluation (can be the same as llm_model)\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with test results\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Testing: {test_case.question}\")\n",
    "    print(f\"Expected Quality: {test_case.expected_quality}\")\n",
    "    print(f\"Notes: {test_case.notes}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    # Generate answer asynchronously\n",
    "    generated_answer = await generate_answer_with_llm_async(\n",
    "        test_case.question, \n",
    "        test_case.context, \n",
    "        llm_model\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nGenerated Answer: {generated_answer[:200]}...\")\n",
    "    \n",
    "    # Create evaluation instance\n",
    "    evaluation = Evaluation(\n",
    "        question=test_case.question,\n",
    "        generated_answer=generated_answer,\n",
    "        llm=evaluator_llm,\n",
    "        context=test_case.context\n",
    "    )\n",
    "    \n",
    "    # Run comprehensive evaluation asynchronously\n",
    "    if hasattr(evaluation, 'comprehensive_evaluation_async'): # I'll need to fix this in the morning\n",
    "        result = await evaluation.comprehensive_evaluation_async()\n",
    "    else:\n",
    "        # If evaluation doesn't have async method, run in thread pool\n",
    "        result = await asyncio.to_thread(evaluation.comprehensive_evaluation)\n",
    "    \n",
    "    return {\n",
    "        \"test_case\": test_case,\n",
    "        \"generated_answer\": generated_answer,\n",
    "        \"evaluation_result\": result,\n",
    "        \"expected_quality\": test_case.expected_quality\n",
    "    }\n",
    "\n",
    "async def run_full_evaluation_suite_async(\n",
    "    llm_model, \n",
    "    evaluator_llm, \n",
    "    test_cases: List[TestCase] = None,\n",
    "    max_concurrent: int = 5\n",
    ") -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Run evaluation on all test cases concurrently using semaphores.\n",
    "    \n",
    "    Args:\n",
    "        llm_model: LLM for generating answers\n",
    "        evaluator_llm: LLM for evaluation\n",
    "        test_cases: Optional custom test cases (defaults to TEST_CASES)\n",
    "        max_concurrent: Maximum number of concurrent requests\n",
    "    \n",
    "    Returns:\n",
    "        List of evaluation results\n",
    "    \"\"\"\n",
    "    if test_cases is None:\n",
    "        test_cases = TEST_CASES\n",
    "    \n",
    "    # Create semaphore to limit concurrent requests\n",
    "    semaphore = asyncio.Semaphore(max_concurrent)\n",
    "    completed = 0\n",
    "    total = len(test_cases)\n",
    "    \n",
    "    async def run_single_test_with_semaphore(i, test_case):\n",
    "        \"\"\"Run a single test with semaphore control\"\"\"\n",
    "        nonlocal completed\n",
    "        \n",
    "        async with semaphore:  # This limits concurrent execution\n",
    "            print(f\"\\nStarting Test {i+1}/{total}\")\n",
    "            \n",
    "            try:\n",
    "                result = await run_evaluation_test_async(test_case, llm_model, evaluator_llm)\n",
    "                \n",
    "                completed += 1\n",
    "                # Print summary\n",
    "                eval_result = result[\"evaluation_result\"]\n",
    "                print(f\"\\nTest {i+1}/{total} completed ({completed}/{total} done)\")\n",
    "                print(f\"SCORES:\")\n",
    "                print(f\"   Relevance: {eval_result.relevance_score}\")\n",
    "                print(f\"   Correctness: {eval_result.correctness_score}\")\n",
    "                print(f\"   Faithfulness: {eval_result.faithfulness_score}\")\n",
    "                print(f\"   Hallucination: {eval_result.hallucination_score}\")\n",
    "                print(f\"   Overall: {eval_result.overall_score:.2f}\")\n",
    "                \n",
    "                return result\n",
    "                \n",
    "            except Exception as e:\n",
    "                completed += 1\n",
    "                error_result = {\n",
    "                    \"test_case\": test_case,\n",
    "                    \"error\": str(e),\n",
    "                    \"evaluation_result\": None\n",
    "                }\n",
    "                print(f\"Error in test {i+1}: {str(e)} ({completed}/{total} done)\")\n",
    "                return error_result\n",
    "    \n",
    "    # Create tasks for all test cases\n",
    "    tasks = [\n",
    "        run_single_test_with_semaphore(i, test_case) \n",
    "        for i, test_case in enumerate(test_cases)\n",
    "    ]\n",
    "    \n",
    "    # Run all tasks concurrently\n",
    "    print(f\"Starting {total} evaluations with max {max_concurrent} concurrent requests...\")\n",
    "    results = await asyncio.gather(*tasks, return_exceptions=True)\n",
    "    \n",
    "    # Handle any exceptions that weren't caught\n",
    "    processed_results = []\n",
    "    for i, result in enumerate(results):\n",
    "        if isinstance(result, Exception):\n",
    "            processed_results.append({\n",
    "                \"test_case\": test_cases[i],\n",
    "                \"error\": str(result),\n",
    "                \"evaluation_result\": None\n",
    "            })\n",
    "        else:\n",
    "            processed_results.append(result)\n",
    "    \n",
    "    return processed_results\n",
    "\n",
    "# Synchronous wrapper for easy use\n",
    "def run_full_evaluation_suite_concurrent(\n",
    "    llm_model, \n",
    "    evaluator_llm, \n",
    "    test_cases: List[TestCase] = None,\n",
    "    max_concurrent: int = 5\n",
    ") -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Synchronous wrapper for async evaluation suite.\n",
    "    \n",
    "    Args:\n",
    "        llm_model: LLM for generating answers\n",
    "        evaluator_llm: LLM for evaluation\n",
    "        test_cases: Optional custom test cases (defaults to TEST_CASES)\n",
    "        max_concurrent: Maximum number of concurrent requests\n",
    "    \n",
    "    Returns:\n",
    "        List of evaluation results\n",
    "    \"\"\"\n",
    "    return asyncio.run(\n",
    "        run_full_evaluation_suite_async(llm_model, evaluator_llm, test_cases, max_concurrent)\n",
    "    )\n",
    "\n",
    "def print_evaluation_summary(results: List[Dict]):\n",
    "    \"\"\"Print a summary of all evaluation results.\"\"\"\n",
    "    print(f\"\\n\\n{'='*80}\")\n",
    "    print(\"EVALUATION SUMMARY\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    successful_results = [r for r in results if r.get(\"evaluation_result\")]\n",
    "    \n",
    "    if not successful_results:\n",
    "        print(\"No successful evaluations to summarize\")\n",
    "        return\n",
    "    \n",
    "    # Calculate averages\n",
    "    avg_relevance = sum(r[\"evaluation_result\"].relevance_score for r in successful_results) / len(successful_results)\n",
    "    avg_correctness = sum(r[\"evaluation_result\"].correctness_score for r in successful_results) / len(successful_results)\n",
    "    avg_faithfulness = sum(r[\"evaluation_result\"].faithfulness_score for r in successful_results) / len(successful_results)\n",
    "    avg_hallucination = sum(r[\"evaluation_result\"].hallucination_score for r in successful_results) / len(successful_results)\n",
    "    avg_overall = sum(r[\"evaluation_result\"].overall_score for r in successful_results) / len(successful_results)\n",
    "    \n",
    "    print(f\"Average Scores (n={len(successful_results)}):\")\n",
    "    print(f\"Relevance: {avg_relevance:.2f}\")\n",
    "    print(f\"Correctness: {avg_correctness:.2f}\") \n",
    "    print(f\"Faithfulness: {avg_faithfulness:.2f}\")\n",
    "    print(f\"Hallucination: {avg_hallucination:.2f}\")\n",
    "    print(f\"Overall: {avg_overall:.2f}\")\n",
    "    \n",
    "    # Identify problematic cases\n",
    "    low_scoring = [r for r in successful_results if r[\"evaluation_result\"].overall_score < 0.6]\n",
    "    if low_scoring:\n",
    "        print(f\"\\nLow-scoring cases ({len(low_scoring)} cases with overall < 0.6):\")\n",
    "        for result in low_scoring:\n",
    "            question = result[\"test_case\"].question[:50] + \"...\"\n",
    "            score = result[\"evaluation_result\"].overall_score\n",
    "            print(f\"{question} (Score: {score:.2f})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "cellUniqueIdByVincent": "5e6f2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting 10 evaluations with max 5 concurrent requests...\n",
      "\n",
      "Starting Test 1/10\n",
      "\n",
      "================================================================================\n",
      "Testing: What is the capital of France?\n",
      "Expected Quality: high\n",
      "Notes: Simple factual question with directly relevant context\n",
      "================================================================================\n",
      "\n",
      "Starting Test 2/10\n",
      "\n",
      "================================================================================\n",
      "Testing: How does photosynthesis work?\n",
      "Expected Quality: high\n",
      "Notes: Scientific explanation with comprehensive context\n",
      "================================================================================\n",
      "\n",
      "Starting Test 3/10\n",
      "\n",
      "================================================================================\n",
      "Testing: What are the side effects of aspirin?\n",
      "Expected Quality: high\n",
      "Notes: Medical information with relevant safety context\n",
      "================================================================================\n",
      "\n",
      "Starting Test 4/10\n",
      "\n",
      "================================================================================\n",
      "Testing: What is the GDP of Brazil in 2024?\n",
      "Expected Quality: low\n",
      "Notes: Specific data question with irrelevant context - should test hallucination\n",
      "================================================================================\n",
      "\n",
      "Starting Test 5/10\n",
      "\n",
      "================================================================================\n",
      "Testing: How do you bake chocolate chip cookies?\n",
      "Expected Quality: medium\n",
      "Notes: Recipe question with historical context but missing specific instructions\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generated Answer: The capital of France is Paris....\n",
      "Evaluating What is the capital of France?...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 0.489964 seconds\n",
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:root:Raw score: 5, Normalized: 1.0\n",
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generated Answer: Photosynthesis works by plants using sunlight, carbon dioxide from the air, and water from their roots to produce glucose and oxygen. This process occurs mainly in the chloroplasts of plant cells, spe...\n",
      "Evaluating How does photosynthesis work?...\n",
      "\n",
      "Generated Answer: To bake chocolate chip cookies, you need to preheat the oven and use careful timing to achieve the right texture. The original recipe calls for ingredients such as butter, sugar, eggs, flour, and choc...\n",
      "Evaluating How do you bake chocolate chip cookies?...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generated Answer: The context does not contain enough information to answer the question about Brazil's GDP in 2024. The provided information discusses Brazil's economy, exports, and currency but does not include speci...\n",
      "Evaluating What is the GDP of Brazil in 2024?...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generated Answer: The side effects of aspirin include:\n",
      "\n",
      "1. Stomach upset\n",
      "2. Heartburn\n",
      "3. Nausea\n",
      "4. Stomach bleeding\n",
      "5. Ulcers\n",
      "6. Increased risk of bleeding disorders\n",
      "\n",
      "Note that these side effects can be categorized int...\n",
      "Evaluating What are the side effects of aspirin?...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Raw score: 5, Normalized: 1.0\n",
      "INFO:root:Raw score: 5, Normalized: 1.0\n",
      "INFO:root:Raw score: 3, Normalized: 0.5\n",
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:root:Raw score: 2, Normalized: 0.25\n",
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:root:Raw score: 5, Normalized: 1.0\n",
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:root:Raw score: 5, Normalized: 1.0\n",
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:root:Raw score: 5, Normalized: 1.0\n",
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:root:Raw score: 5, Normalized: 1.0\n",
      "INFO:root:Raw score: 4, Normalized: 0.75\n",
      "INFO:root:Raw score: 5, Normalized: 1.0\n",
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:root:Raw score: 5, Normalized: 1.0\n",
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:root:Raw score: 5, Normalized: 1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test 1/10 completed (1/10 done)\n",
      "SCORES:\n",
      "   Relevance: 1.0\n",
      "   Correctness: 1.0\n",
      "   Faithfulness: 1.0\n",
      "   Hallucination: 1.0\n",
      "   Overall: 1.00\n",
      "\n",
      "Starting Test 6/10\n",
      "\n",
      "================================================================================\n",
      "Testing: What causes climate change?\n",
      "Expected Quality: high\n",
      "Notes: Environmental science with comprehensive explanation\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:root:Raw score: 5, Normalized: 1.0\n",
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:root:Raw score: 5, Normalized: 1.0\n",
      "INFO:root:Raw score: 5, Normalized: 1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test 2/10 completed (2/10 done)\n",
      "SCORES:\n",
      "   Relevance: 1.0\n",
      "   Correctness: 1.0\n",
      "   Faithfulness: 1.0\n",
      "   Hallucination: 1.0\n",
      "   Overall: 1.00\n",
      "\n",
      "Starting Test 7/10\n",
      "\n",
      "================================================================================\n",
      "Testing: Who won the 2023 FIFA World Cup?\n",
      "Expected Quality: low\n",
      "Notes: Specific event question with general context - tests factual accuracy\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Raw score: 5, Normalized: 1.0\n",
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:root:Raw score: 5, Normalized: 1.0\n",
      "INFO:root:Raw score: 5, Normalized: 1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test 4/10 completed (3/10 done)\n",
      "SCORES:\n",
      "   Relevance: 0.25\n",
      "   Correctness: 1.0\n",
      "   Faithfulness: 1.0\n",
      "   Hallucination: 1.0\n",
      "   Overall: 0.81\n",
      "\n",
      "Starting Test 8/10\n",
      "\n",
      "================================================================================\n",
      "Testing: What are the benefits of exercise?\n",
      "Expected Quality: high\n",
      "Notes: Health question with comprehensive relevant context\n",
      "================================================================================\n",
      "\n",
      "Test 5/10 completed (4/10 done)\n",
      "SCORES:\n",
      "   Relevance: 0.5\n",
      "   Correctness: 0.75\n",
      "   Faithfulness: 1.0\n",
      "   Hallucination: 1.0\n",
      "   Overall: 0.80\n",
      "\n",
      "Starting Test 9/10\n",
      "\n",
      "================================================================================\n",
      "Testing: How does blockchain technology work?\n",
      "Expected Quality: low\n",
      "Notes: Technical question with tangentially related context about crypto\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Raw score: 5, Normalized: 1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test 3/10 completed (5/10 done)\n",
      "SCORES:\n",
      "   Relevance: 1.0\n",
      "   Correctness: 1.0\n",
      "   Faithfulness: 1.0\n",
      "   Hallucination: 1.0\n",
      "   Overall: 1.00\n",
      "\n",
      "Starting Test 10/10\n",
      "\n",
      "================================================================================\n",
      "Testing: What is the treatment for type 2 diabetes?\n",
      "Expected Quality: high\n",
      "Notes: Medical treatment question with appropriate clinical context\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generated Answer: The benefits of exercise include:\n",
      "\n",
      "1. Improving cardiovascular health by strengthening the heart and improving circulation.\n",
      "2. Maintaining a healthy weight.\n",
      "3. Building muscle strength and bone densit...\n",
      "Evaluating What are the benefits of exercise?...\n",
      "\n",
      "Generated Answer: There is not enough information in the context to answer the question. The context does not mention the 2023 FIFA World Cup or its winner. It only provides general information about the tournament....\n",
      "Evaluating Who won the 2023 FIFA World Cup?...\n",
      "\n",
      "Generated Answer: The treatment for type 2 diabetes typically involves lifestyle changes, such as diet modification and regular exercise, and may also include medications like metformin, insulin, or other blood sugar-l...\n",
      "Evaluating What is the treatment for type 2 diabetes?...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generated Answer: The context doesn't contain enough information to fully answer the question. The provided context discusses the volatility of cryptocurrency markets, the creation of Bitcoin, and the acceptance of cry...\n",
      "Evaluating How does blockchain technology work?...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:root:Raw score: 5, Normalized: 1.0\n",
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:root:Raw score: 5, Normalized: 1.0\n",
      "INFO:root:Raw score: 2, Normalized: 0.25\n",
      "INFO:root:Raw score: 2, Normalized: 0.25\n",
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:root:Raw score: 5, Normalized: 1.0\n",
      "INFO:root:Raw score: 5, Normalized: 1.0\n",
      "INFO:root:Raw score: 5, Normalized: 1.0\n",
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:root:Raw score: 5, Normalized: 1.0\n",
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:root:Raw score: 5, Normalized: 1.0\n",
      "INFO:root:Raw score: 5, Normalized: 1.0\n",
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:root:Raw score: 5, Normalized: 1.0\n",
      "INFO:root:Raw score: 5, Normalized: 1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test 7/10 completed (6/10 done)\n",
      "SCORES:\n",
      "   Relevance: 0.25\n",
      "   Correctness: 1.0\n",
      "   Faithfulness: 1.0\n",
      "   Hallucination: 1.0\n",
      "   Overall: 0.81\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:root:Raw score: 5, Normalized: 1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test 8/10 completed (7/10 done)\n",
      "SCORES:\n",
      "   Relevance: 1.0\n",
      "   Correctness: 1.0\n",
      "   Faithfulness: 1.0\n",
      "   Hallucination: 1.0\n",
      "   Overall: 1.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Raw score: 5, Normalized: 1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generated Answer: Based on the provided context, the cause of global warming (which is related to climate change) is the increase in greenhouse gases in the atmosphere, primarily due to human activities such as burning...\n",
      "Evaluating What causes climate change?...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:root:Raw score: 5, Normalized: 1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test 10/10 completed (8/10 done)\n",
      "SCORES:\n",
      "   Relevance: 1.0\n",
      "   Correctness: 1.0\n",
      "   Faithfulness: 1.0\n",
      "   Hallucination: 1.0\n",
      "   Overall: 1.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Raw score: 5, Normalized: 1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test 9/10 completed (9/10 done)\n",
      "SCORES:\n",
      "   Relevance: 0.25\n",
      "   Correctness: 1.0\n",
      "   Faithfulness: 1.0\n",
      "   Hallucination: 1.0\n",
      "   Overall: 0.81\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Raw score: 4, Normalized: 0.75\n",
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:root:Raw score: 4, Normalized: 0.75\n",
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:root:Raw score: 4, Normalized: 0.75\n",
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:root:Raw score: 5, Normalized: 1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test 6/10 completed (10/10 done)\n",
      "SCORES:\n",
      "   Relevance: 0.75\n",
      "   Correctness: 0.75\n",
      "   Faithfulness: 0.75\n",
      "   Hallucination: 1.0\n",
      "   Overall: 0.80\n",
      "\n",
      "\n",
      "================================================================================\n",
      "EVALUATION SUMMARY\n",
      "================================================================================\n",
      "Average Scores (n=10):\n",
      "Relevance: 0.70\n",
      "Correctness: 0.95\n",
      "Faithfulness: 0.97\n",
      "Hallucination: 1.00\n",
      "Overall: 0.90\n"
     ]
    }
   ],
   "source": [
    "import nest_asyncio\n",
    "# Patch the current event loop\n",
    "nest_asyncio.apply()\n",
    "# Initialize LLMs\n",
    "llm_model = OpenRouterModel()  \n",
    "evaluator_llm = OpenRouterModel()  \n",
    "\n",
    "results = run_full_evaluation_suite_concurrent(\n",
    "    llm_model, \n",
    "    evaluator_llm,\n",
    "    max_concurrent=5  \n",
    ")\n",
    "\n",
    "\n",
    "print_evaluation_summary(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellUniqueIdByVincent": "c59e7"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  },
  "vincent": {
   "sessionId": "b75523e7856d7a763194bb99_2025-05-29T13-09-17-251Z"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
