{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellUniqueIdByVincent": "0a411"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from typing import List, Any, Tuple, Dict, Optional\n",
    "from dataclasses import dataclass\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# Your existing classes\n",
    "@dataclass\n",
    "class RAGEvaluationResult:\n",
    "    \"\"\"Structure to hold evaluation results for a single question\"\"\"\n",
    "    question: str\n",
    "    context: str\n",
    "    generated_answer: str\n",
    "    relevance_score: float\n",
    "    correctness_score: float\n",
    "    faithfulness_score: float\n",
    "    hallucination_score: float\n",
    "    detailed_feedback: Dict[str, str]\n",
    "    overall_score: float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "cellUniqueIdByVincent": "d50d5"
   },
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class OpenRouterModel:\n",
    "    api_key: Optional[str] = os.getenv(\"OPENROUTER_API_KEY\")\n",
    "\n",
    "    def __post_init__(self):\n",
    "        if not self.api_key:\n",
    "            raise ValueError(\"Please set a valid api key in the environment variables\")\n",
    "        self.client = OpenAI(\n",
    "            base_url=\"https://openrouter.ai/api/v1\",\n",
    "            api_key=self.api_key,\n",
    "        )\n",
    "        \n",
    "        self.model = \"meta-llama/llama-3.3-70b-instruct\"\n",
    "\n",
    "    def invoke(self, prompt: str, temperature: float = 0.0, max_tokens: int = 2000) -> str:\n",
    "        \"\"\"Generate response using Llama 3.3 70B Instruct\"\"\"\n",
    "        try:\n",
    "            response = self.client.chat.completions.create(\n",
    "                model=self.model,\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                temperature=temperature,\n",
    "                max_tokens=max_tokens\n",
    "            )\n",
    "            return response.choices[0].message.content\n",
    "        except Exception as e:\n",
    "            print(f\"Error calling OpenRouter API: {e}\")\n",
    "            return \"\"\n",
    "        \n",
    "    def invoke_with_json_output(self, prompt_template, input_variables):\n",
    "        \"\"\"Generate structured JSON output from the model\"\"\"\n",
    "        \n",
    "        formatted_prompt = prompt_template.format(**input_variables)\n",
    "        \n",
    "        # Get the raw response from the model\n",
    "        raw_response = self.invoke(formatted_prompt)\n",
    "        \n",
    "        # Extract JSON from the response\n",
    "        try:\n",
    "            # Find JSON content within the response\n",
    "            json_start = raw_response.find('{')\n",
    "            json_end = raw_response.rfind('}') + 1\n",
    "            \n",
    "            if json_start >= 0 and json_end > json_start:\n",
    "                json_str = raw_response[json_start:json_end]\n",
    "                result = json.loads(json_str)\n",
    "                return result\n",
    "            else:\n",
    "                # If no JSON found, try to parse the whole response\n",
    "                return json.loads(raw_response)\n",
    "        except json.JSONDecodeError:\n",
    "            print(\"Failed to parse JSON from response. Raw response:\")\n",
    "            print(raw_response)\n",
    "            # Return a default structure if parsing fails\n",
    "            return {\"score\": 0, \"reasoning\": \"Failed to parse response\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "cellUniqueIdByVincent": "2a1cc"
   },
   "outputs": [],
   "source": [
    "class ResultScore(BaseModel):\n",
    "    score: float = Field(..., description=\"The score of the result, ranging from 0 to 1 where 1 is the best possible score.\")\n",
    "    reasoning: str = Field(..., description=\"An extensive explanation of the score.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "cellUniqueIdByVincent": "a10db"
   },
   "outputs": [],
   "source": [
    "def evaluate_relevance(question: str, context: str, generated_answer: str, llm: OpenRouterModel):\n",
    "    prompt = PromptTemplate(\n",
    "        input_variables=[\"question\", \"context\", \"generated_answer\"],\n",
    "        template = \"\"\"\n",
    "        Evaluate the RELEVANCE of the retrieved context to the given question.\n",
    "        \n",
    "        Question: {question}\n",
    "        \n",
    "        Retrieved Context: {context}\n",
    "\n",
    "        Generated Answer: {generated_answer}\n",
    "        \n",
    "        Rate the relevance on a scale of 1-5 where:\n",
    "        - 1: Completely irrelevant, no connection to the question\n",
    "        - 2: Minimally relevant, tangential connection\n",
    "        - 3: Somewhat relevant, partial connection\n",
    "        - 4: Highly relevant, strong connection\n",
    "        - 5: Perfectly relevant, directly addresses the question\n",
    "        \n",
    "        Provide your response in this format:\n",
    "        {{\n",
    "            \"score\": <number>,\n",
    "            \"reasoning\": \"<detailed explanation of why you gave this score>\"\n",
    "        }}\n",
    "        \n",
    "        Ensure your response is valid JSON.\n",
    "        \"\"\"\n",
    "    )\n",
    "\n",
    "    # Use the custom method for structured output\n",
    "    result = llm.invoke_with_json_output(\n",
    "        prompt_template=prompt.template,\n",
    "        input_variables={\n",
    "            \"question\": question,\n",
    "            \"context\": context,\n",
    "            \"generated_answer\": generated_answer\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    # Normalize the score to 0-1 range since it's on a 1-5 scale\n",
    "    if \"score\" in result and result[\"score\"] > 1:\n",
    "        result[\"score\"] = result[\"score\"] / 5.0\n",
    "        \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "cellUniqueIdByVincent": "1f302"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Relevance Score: 1.0\n",
      "Reasoning: The retrieved context directly addresses the question 'What causes climate change?' by providing a clear and concise explanation of the primary causes of climate change, including human activities such as burning fossil fuels, deforestation, and industrial processes. The generated answer accurately summarizes the main points from the retrieved context, demonstrating a perfect connection between the question, the context, and the answer. Therefore, the relevance of the retrieved context to the given question is perfectly relevant, warranting a score of 5.\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "llm = OpenRouterModel()\n",
    "question = \"What causes climate change?\"\n",
    "context = \"Climate change is primarily caused by human activities that increase greenhouse gas concentrations in the atmosphere. The main contributors include burning fossil fuels (coal, oil, gas) for electricity, heat, and transportation, which releases carbon dioxide. Deforestation reduces the Earth's capacity to absorb CO2. Industrial processes and agriculture also contribute significantly.\"\n",
    "generated_answer = \"Climate change is primarily caused by human activities, especially burning fossil fuels for energy and transportation, which releases carbon dioxide into the atmosphere. Deforestation and industrial processes also contribute by increasing greenhouse gas concentrations.\"\n",
    "\n",
    "result = evaluate_relevance(question, context, generated_answer, llm)\n",
    "print(f\"Relevance Score: {result['score']}\")\n",
    "print(f\"Reasoning: {result['reasoning']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellUniqueIdByVincent": "6865c"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  },
  "vincent": {
   "sessionId": "b75523e7856d7a763194bb99_2025-05-29T13-09-17-251Z"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
