{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "cellUniqueIdByVincent": "016df"
   },
   "source": [
    "# Tunneling to kaggle, we have to install popper-utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellUniqueIdByVincent": "d8ea1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading package lists... Done\n",
      "Building dependency tree... Done\n",
      "Reading state information... Done\n",
      "poppler-utils is already the newest version (22.02.0-2ubuntu0.8).\n",
      "0 upgraded, 0 newly installed, 0 to remove and 84 not upgraded.\n"
     ]
    }
   ],
   "source": [
    "!apt-get install -y poppler-utils\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellUniqueIdByVincent": "5bd60"
   },
   "source": [
    "# Use asynchronous programming to read the pdf files in parallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cellUniqueIdByVincent": "4996b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting PDF: /kaggle/input/knbs-real-estate/2023-24-Real-Estate-Survey-Report_1.pdf\n",
      "Output directory: real_estate_images\n",
      "Converted 60 pages\n",
      "Compressing and saving images...\n",
      "✅ Conversion complete!\n",
      "📁 Total files: 60\n",
      "💾 Total size: 11.87 MB\n",
      "📊 Average size per image: 202.61 KB\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import asyncio\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from pdf2image import convert_from_path\n",
    "from PIL import Image\n",
    "import io\n",
    "\n",
    "async def compress_and_save_image(image, filepath, quality=85, optimize=True):\n",
    "    def _compress_save(img=image):  # pass image explicitly\n",
    "        if img.mode in ('RGBA', 'LA', 'P'):\n",
    "            background = Image.new('RGB', img.size, (255, 255, 255))\n",
    "            background.paste(img, mask=img.split()[-1])\n",
    "            img = background\n",
    "        else:\n",
    "            img = img.convert('RGB')\n",
    "\n",
    "        img.save(filepath, 'JPEG', quality=quality, optimize=optimize, progressive=True)\n",
    "        return os.path.getsize(filepath)\n",
    "\n",
    "    loop = asyncio.get_event_loop()\n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        return await loop.run_in_executor(executor, _compress_save)\n",
    "\n",
    "\n",
    "async def convert_pdf_to_compressed_images(pdf_path, output_dir=\"economic_survey_images\", \n",
    "                                         dpi=150, quality=85):\n",
    "    \"\"\"Convert PDF to compressed images asynchronously\"\"\"\n",
    "    \n",
    "    # Create output directory\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    print(f\"Converting PDF: {pdf_path}\")\n",
    "    print(f\"Output directory: {output_dir}\")\n",
    "    \n",
    "    # Convert PDF to images (reduce DPI for smaller file size)\n",
    "    def _convert_pdf():\n",
    "        return convert_from_path(pdf_path, dpi=dpi, output_folder=output_dir)\n",
    "    \n",
    "    loop = asyncio.get_event_loop()\n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        images = await loop.run_in_executor(executor, _convert_pdf)\n",
    "    \n",
    "    print(f\"Converted {len(images)} pages\")\n",
    "    \n",
    "    # Create tasks for compressing and saving all images concurrently\n",
    "    tasks = []\n",
    "    for i, image in enumerate(images):\n",
    "        filepath = os.path.join(output_dir, f'page_{i+1:03d}.jpg')  # Use .jpg extension\n",
    "        task = compress_and_save_image(image, filepath, quality=quality)\n",
    "        tasks.append(task)\n",
    "    \n",
    "    # Execute all compression tasks concurrently\n",
    "    print(\"Compressing and saving images...\")\n",
    "    file_sizes = await asyncio.gather(*tasks)\n",
    "    \n",
    "    # Report results\n",
    "    total_size = sum(file_sizes)\n",
    "    avg_size = total_size / len(file_sizes) if file_sizes else 0\n",
    "    \n",
    "    print(f\"✅ Conversion complete!\")\n",
    "    print(f\"📁 Total files: {len(file_sizes)}\")\n",
    "    print(f\"💾 Total size: {total_size / (1024*1024):.2f} MB\")\n",
    "    print(f\"📊 Average size per image: {avg_size / 1024:.2f} KB\")\n",
    "    \n",
    "    return len(images), total_size\n",
    "\n",
    "# Main execution\n",
    "async def main():\n",
    "    pdf_path = \"/kaggle/input/knbs-real-estate/2023-24-Real-Estate-Survey-Report_1.pdf\"\n",
    "    \n",
    "    await convert_pdf_to_compressed_images(\n",
    "        pdf_path=pdf_path,\n",
    "        output_dir=\"real_estate_images\",\n",
    "        dpi=150,\n",
    "        quality=85\n",
    "    )\n",
    "\n",
    "# Run the async function\n",
    "if __name__ == \"__main__\":\n",
    "    await main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellUniqueIdByVincent": "a974b"
   },
   "source": [
    "# Set up cohere"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellUniqueIdByVincent": "c7896"
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "import io\n",
    "import base64\n",
    "from PIL import Image\n",
    "import tqdm\n",
    "import time\n",
    "import numpy as np\n",
    "import cohere\n",
    "import os\n",
    "cohere_api_key = 'HBTo...'\n",
    "co = cohere.ClientV2(api_key=cohere_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "cellUniqueIdByVincent": "684bb"
   },
   "outputs": [],
   "source": [
    "max_pixels = 1568*1568  #Max resolution for images\n",
    "\n",
    "def resize_image(pil_image):\n",
    "    org_width, org_height = pil_image.size\n",
    "\n",
    "    # Resize image if too large\n",
    "    if org_width * org_height > max_pixels:\n",
    "        scale_factor = (max_pixels / (org_width * org_height)) ** 0.5\n",
    "        new_width = int(org_width * scale_factor)\n",
    "        new_height = int(org_height * scale_factor)\n",
    "        pil_image.thumbnail((new_width, new_height))\n",
    "\n",
    "# Convert images to a base64 string before sending it to the API\n",
    "def base64_from_image(img_path):\n",
    "    pil_image = Image.open(img_path)\n",
    "    img_format = pil_image.format if pil_image.format else \"PNG\"\n",
    "\n",
    "    resize_image(pil_image)\n",
    "\n",
    "    with io.BytesIO() as img_buffer:\n",
    "        pil_image.save(img_buffer, format=img_format)\n",
    "        img_buffer.seek(0)\n",
    "        img_data = f\"data:image/{img_format.lower()};base64,\"+base64.b64encode(img_buffer.read()).decode(\"utf-8\")\n",
    "\n",
    "    return img_data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "cellUniqueIdByVincent": "2f057"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['page_055.jpg',\n",
       " 'page_060.jpg',\n",
       " 'da6c3794-ca92-4071-aa6d-917796be8ebe-28.ppm',\n",
       " 'da6c3794-ca92-4071-aa6d-917796be8ebe-45.ppm',\n",
       " 'da6c3794-ca92-4071-aa6d-917796be8ebe-55.ppm',\n",
       " 'page_024.jpg',\n",
       " 'da6c3794-ca92-4071-aa6d-917796be8ebe-25.ppm',\n",
       " 'da6c3794-ca92-4071-aa6d-917796be8ebe-09.ppm',\n",
       " 'da6c3794-ca92-4071-aa6d-917796be8ebe-36.ppm',\n",
       " 'da6c3794-ca92-4071-aa6d-917796be8ebe-23.ppm']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(\"/kaggle/working/real_estate_images\")[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellUniqueIdByVincent": "f14df"
   },
   "source": [
    "# Remove the .ppm files to remain with images only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "cellUniqueIdByVincent": "48309"
   },
   "outputs": [],
   "source": [
    "folder = \"/kaggle/working/real_estate_images\"\n",
    "\n",
    "for file in os.listdir(folder):\n",
    "    if file.endswith(\".ppm\"):\n",
    "        os.remove(os.path.join(folder, file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellUniqueIdByVincent": "d30b0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['page_055.jpg',\n",
       " 'page_060.jpg',\n",
       " 'page_024.jpg',\n",
       " 'page_004.jpg',\n",
       " 'page_012.jpg',\n",
       " 'page_007.jpg',\n",
       " 'page_030.jpg',\n",
       " 'page_035.jpg',\n",
       " 'page_027.jpg',\n",
       " 'page_011.jpg']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(folder)[:10] # Great, now we only have images to work with"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellUniqueIdByVincent": "d07c6"
   },
   "source": [
    "# Use asynchronous programmning again tomake multiple calls to the cohere api to generate embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellUniqueIdByVincent": "7ae50"
   },
   "outputs": [],
   "source": [
    "\n",
    "import asyncio\n",
    "import aiohttp\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "from tqdm.asyncio import tqdm\n",
    "import time\n",
    "import json\n",
    "\n",
    "# Config\n",
    "BATCH_SIZE = 50  \n",
    "CONCURRENT_REQUESTS = 8 \n",
    "RATE_LIMIT_DELAY = 0.1  \n",
    "\n",
    "img_folder = folder\n",
    "\n",
    "async def process_batch_async(session, img_batch, semaphore):\n",
    "    \"\"\"Process a batch of images asynchronously\"\"\"\n",
    "    async with semaphore:  # Limit concurrent requests\n",
    "        try:\n",
    "            # Prepare batch input\n",
    "            api_inputs = []\n",
    "            for img_path in img_batch:\n",
    "                api_input_document = {\n",
    "                    \"content\": [\n",
    "                        {\"type\": \"image\", \"image\": base64_from_image(img_path)},\n",
    "                    ]\n",
    "                }\n",
    "                api_inputs.append(api_input_document)\n",
    "\n",
    "            # Prepare API call data\n",
    "            payload = {\n",
    "                \"model\": \"embed-v4.0\",\n",
    "                \"input_type\": \"search_document\",\n",
    "                \"embedding_types\": [\"float\"],\n",
    "                \"inputs\": api_inputs\n",
    "            }\n",
    "\n",
    "            # Make async API call \n",
    "            api_response = co.embed(\n",
    "                model=\"embed-v4.0\",\n",
    "                input_type=\"search_document\",\n",
    "                embedding_types=[\"float\"],\n",
    "                inputs=api_inputs,\n",
    "            )\n",
    "\n",
    "            # Extract embeddings\n",
    "            batch_embeddings = [np.asarray(emb) for emb in api_response.embeddings.float]\n",
    "            \n",
    "            await asyncio.sleep(RATE_LIMIT_DELAY)  # Rate limiting\n",
    "            return batch_embeddings, img_batch, None\n",
    "            \n",
    "        except Exception as e:\n",
    "            return None, img_batch, str(e)\n",
    "\n",
    "async def main_async():\n",
    "    \"\"\"Main async processing function\"\"\"\n",
    "    # Get all images\n",
    "    image_extensions = ['*.jpg', '*.jpeg', '*.png', '*.gif', '*.bmp', '*.webp']\n",
    "    img_paths = []\n",
    "    for extension in image_extensions:\n",
    "        img_paths.extend(glob.glob(os.path.join(img_folder, extension)))\n",
    "        img_paths.extend(glob.glob(os.path.join(img_folder, extension.upper())))\n",
    "\n",
    "    print(f\"Found {len(img_paths)} images\")\n",
    "    \n",
    "    # Create batches\n",
    "    img_batches = [img_paths[i:i + BATCH_SIZE] for i in range(0, len(img_paths), BATCH_SIZE)]\n",
    "    print(f\"Processing {len(img_batches)} batches with {CONCURRENT_REQUESTS} concurrent requests\")\n",
    "\n",
    "    # Semaphore to limit concurrent requests\n",
    "    semaphore = asyncio.Semaphore(CONCURRENT_REQUESTS)\n",
    "    \n",
    "    doc_embeddings = []\n",
    "    failed_batches = []\n",
    "    \n",
    "    # Create session for async requests\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        # Create all tasks\n",
    "        tasks = [\n",
    "            process_batch_async(session, batch, semaphore) \n",
    "            for batch in img_batches\n",
    "        ]\n",
    "        \n",
    "        # Process all batches concurrently with progress bar\n",
    "        results = await tqdm.gather(*tasks, desc=\"Processing batches\")\n",
    "        \n",
    "        # Collect results\n",
    "        for batch_embeddings, img_batch, error in results:\n",
    "            if batch_embeddings is not None:\n",
    "                doc_embeddings.extend(batch_embeddings)\n",
    "            else:\n",
    "                failed_batches.append((img_batch, error))\n",
    "    \n",
    "    return doc_embeddings, failed_batches\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "vincent": {
   "sessionId": "0e66d2624a6beb6c12502d5f_2025-05-26T13-17-44-147Z"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
