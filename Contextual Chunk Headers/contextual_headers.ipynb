{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cellUniqueIdByVincent": "6a2ac"
   },
   "outputs": [],
   "source": [
    "import google.generativeai as genai\n",
    "import os\n",
    "import warnings\n",
    "from dotenv import load_dotenv\n",
    "from langchain_community.document_loaders import DirectoryLoader, TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "from langchain_core.messages import HumanMessage\n",
    "from datetime import datetime\n",
    "from typing import List, Dict, Any, Optional\n",
    "import uuid\n",
    "from pinecone import Pinecone, ServerlessSpec\n",
    "import numpy as np\n",
    "import time\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Suppress TensorFlow warnings\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "def get_api_key(key_name=\"OPENROUTER_API_KEY\"): \n",
    "    \"\"\"\n",
    "    Get API key from environment variables\n",
    "    \"\"\"\n",
    "    api_key = os.getenv(key_name)\n",
    "    if not api_key:\n",
    "        raise ValueError(f\"Invalid API key: {key_name} not found in environment variables\")\n",
    "    return api_key\n",
    "\n",
    "def initialize_llm(model_name=\"meta-llama/llama-3.1-8b-instruct\",\n",
    "                  temperature=0.4,\n",
    "                  use_streaming=True):\n",
    "    \"\"\"\n",
    "    Initialize LLM\n",
    "    \"\"\"\n",
    "    api_key = get_api_key()\n",
    "    callbacks = [StreamingStdOutCallbackHandler()]\n",
    "    llm = ChatOpenAI(\n",
    "        model_name=model_name,\n",
    "        temperature=temperature,\n",
    "        streaming=use_streaming,\n",
    "        callbacks=callbacks,\n",
    "        openai_api_key=api_key,\n",
    "        openai_api_base=\"https://openrouter.ai/api/v1\"\n",
    "    )\n",
    "    return llm\n",
    "\n",
    "llm = initialize_llm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cellUniqueIdByVincent": "3457b"
   },
   "outputs": [],
   "source": [
    "def initialize_google_embedding_model():\n",
    "    api_key = os.getenv(\"GEMINI_API_KEY\")\n",
    "    if not api_key:\n",
    "        raise ValueError(\"GEMINI_API_KEY not found in environment variables\")\n",
    "\n",
    "    genai.configure(api_key=api_key)\n",
    "    # Get the text-embedding-004 model\n",
    "    embedding_model = \"text-embedding-004\"\n",
    "    return embedding_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "cellUniqueIdByVincent": "91818"
   },
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import DirectoryLoader, TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from typing import List, Optional\n",
    "\n",
    "def load_documents(data_dir: str, file_extension:str = \".txt\") -> List:\n",
    "    loader = DirectoryLoader(data_dir,\n",
    "            glob = \"**/*.txt\",\n",
    "            loader_cls = TextLoader, \n",
    "            show_progress = True)\n",
    "    documents = loader.load()\n",
    "    print(\"\\nLoaded {len(documents)} documents from {data_dir}\")\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "cellUniqueIdByVincent": "727a6"
   },
   "outputs": [],
   "source": [
    "def split_documents(documents: List, chunk_size: int = 2000, chunk_overlap: int = 200) -> List:\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size = chunk_size,\n",
    "        chunk_overlap = chunk_overlap,\n",
    "        length_function = len,\n",
    "        is_separator_regex = False,\n",
    "    )\n",
    "\n",
    "    chunks = text_splitter.split_documents(documents)\n",
    "    print(f\"\\nSplit into {len(chunks)} chunks\")\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "cellUniqueIdByVincent": "07589"
   },
   "outputs": [],
   "source": [
    "def generate_contextual_headers(chunk, llm, doc_metadata = Optional[None]) -> List:\n",
    "    # Extract the document metadata if any\n",
    "    doc_title = doc_metadata.get(\"title\", \"Unknown Document\") if doc_metadata else \"Unknown Document\"\n",
    "    source = chunk.metadata.get(\"source\", \"Unknown Source\")\n",
    "\n",
    "    # Create a prompt for the LLM to generate a header\n",
    "    prompt = f\"\"\"\n",
    "        You are an expert in document analysis and information synthesis. Your task is to create a concise, informative header for a text excerpt that will help readers quickly understand its significance and context.\n",
    "\n",
    "        Document: \"{doc_title}\" \n",
    "        Source: {source}\n",
    "        Text excerpt (first 500 characters): \n",
    "        {chunk.page_content[:500]}...\n",
    "\n",
    "        Please create a brief header (1-2 sentences, maximum 20 words) that:\n",
    "        1. Identifies the main topic or concept discussed in this excerpt\n",
    "        2. Provides essential context about how this section relates to the broader document\n",
    "        3. Includes any key terms, names, or technical concepts that are central to this passage\n",
    "        4. Uses clear, precise language appropriate to the document's domain/field\n",
    "\n",
    "        Your header should be immediately useful to someone scanning through document chunks, helping them quickly determine relevance and understand the excerpt's place in the overall document structure.\n",
    "\n",
    "        CONTEXTUAL HEADER:\n",
    "        \"\"\"\n",
    "\n",
    "    # Generate the header\n",
    "    header = llm.invoke([HumanMessage(content = prompt)]).content.strip()\n",
    "    return header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "cellUniqueIdByVincent": "23269"
   },
   "outputs": [],
   "source": [
    "def create_chunks_with_headers(chunks, llm):\n",
    "    chunks_with_headers = []\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        header = generate_contextual_header(chunk, llm, doc_metadata = chunk.metadata)\n",
    "\n",
    "        # Create new chunk with header\n",
    "        enriched_content = f\"CONTEXT: {header}\\n\\nCONTENT: {chunk.page_content}\"\n",
    "        # Create a new document with the same metadata but enhanced content\n",
    "        chunk.page_content = enriched_content\n",
    "        chunks_with_headers.append(chunk)\n",
    "        # Print progress\n",
    "        if (i+1) % 10 == 0:\n",
    "            print(f\"Processed {i+1}/{len(chunks)} chunks\")\n",
    "    \n",
    "    return chunks_with_headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "cellUniqueIdByVincent": "efdc2"
   },
   "outputs": [],
   "source": [
    "def get_google_embeddings(texts, embedding_model = \"text-embedding-004\"):\n",
    "    if not embedding_model:\n",
    "        embedding_model = initialize_google_embedding_model()\n",
    "\n",
    "    # Create batch request\n",
    "    embeddings = []\n",
    "    for text in texts:\n",
    "        # Get embedding for the text\n",
    "        result = genai.embed_content(\n",
    "            model=embedding_model,\n",
    "            content=text,\n",
    "            task_type=\"retrieval_document\",  # Optimize for retrieval\n",
    "            title=\"Document chunk\"\n",
    "        )\n",
    "\n",
    "        # Extract the embedding\n",
    "        embedding = result[\"embedding\"]\n",
    "\n",
    "        embeddings.append(embedding)\n",
    "\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "cellUniqueIdByVincent": "4600c"
   },
   "outputs": [],
   "source": [
    "def initialize_pinecone(index_name = \"rag_comparison\"):\n",
    "    api_key = os.getenv(\"PINECONE_API_KEY\")\n",
    "    if not api_key:\n",
    "        raise ValueError(\"PINECONE_API_KEY not found in environment variables\")\n",
    "    \n",
    "    # Initialize Pinecone Client\n",
    "    pc = Pinecone(api_key = api_key)\n",
    "\n",
    "    # Check if the index already exists, create if it doesnt\n",
    "    if index_name not in pc.list_indexes().names():\n",
    "        # Create the index\n",
    "        pc.create_index(\n",
    "            name = index_name,\n",
    "            dimension = 768,\n",
    "            metric = \"cosine\",\n",
    "            spec = ServerlessSpec(cloud=\"aws\", region=\"us-east-1\")\n",
    "        )\n",
    "\n",
    "        print(f\"Created new Pinecone index: {index_name}\")\n",
    "    \n",
    "    # Connect to index\n",
    "    index = pc.Index(index_name)\n",
    "    print(f\"Connected to Pinecone index: {index_name}\")\n",
    "    \n",
    "    return index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "cellUniqueIdByVincent": "defec"
   },
   "outputs": [],
   "source": [
    "def upsert_to_pinecone(chunks, index, namespace = \"standard\", embedding_model = Optional[None]):\n",
    "    if not embedding_model:\n",
    "        embedding_model = initialize_google_embedding_model()\n",
    "\n",
    "    # Batch processing\n",
    "    batch_size = 20\n",
    "    total_chunks = len(chunks_with_headers)\n",
    "    for i in range(0, total_chunks, batch_size):\n",
    "        end_idx = min(i + batch_size, total_chunks)\n",
    "        batch = chunks_with_headers[i:end_idx]\n",
    "        \n",
    "        # Extract text content from batch\n",
    "        texts = [chunk.page_content for chunk in batch]\n",
    "        \n",
    "        # Generate embeddings\n",
    "        embeddings = get_google_embeddings(texts, embedding_model)\n",
    "        \n",
    "        # Prepare vectors for Pinecone\n",
    "        vectors = []\n",
    "        for j, (chunk, embedding) in enumerate(zip(batch, embeddings)):\n",
    "            # Create a unique ID for each vector\n",
    "            vector_id = str(uuid.uuid4())\n",
    "            \n",
    "            # Prepare metadata\n",
    "            metadata = {\n",
    "                \"text\": chunk.page_content,\n",
    "                \"source\": chunk.metadata.get(\"source\", \"Unknown\"),\n",
    "                \"chunk_id\": i + j,\n",
    "                \"namespace\": namespace\n",
    "            }\n",
    "            \n",
    "            # Add vector to batch\n",
    "            vectors.append({\n",
    "                \"id\": vector_id,\n",
    "                \"values\": embedding,\n",
    "                \"metadata\": metadata\n",
    "            })\n",
    "        \n",
    "        # Upsert vectors to Pinecone\n",
    "        index.upsert(vectors=vectors, namespace=namespace)\n",
    "        \n",
    "        print(f\"Processed and upserted {end_idx}/{total_chunks} chunks\")\n",
    "    \n",
    "    print(f\"Successfully upserted {total_chunks} contextual chunks to Pinecone\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellUniqueIdByVincent": "c01ce"
   },
   "outputs": [],
   "source": [
    "def semantic_search(query, index, namespace = \"standard\", top_k = 5, embedding_model = Optional[None]):\n",
    "    if not embedding_model:\n",
    "        embedding_model = initialize_google_embedding_model()\n",
    "\n",
    "    # Generate query embeddings\n",
    "    # Generate embedding for the query\n",
    "    query_embedding_result = genai.embed_content(\n",
    "        model=embedding_model,\n",
    "        content=query,\n",
    "        task_type=\"retrieval_query\"  # Optimize for query\n",
    "    )\n",
    "    query_embedding = query_embedding_result[\"embedding\"]\n",
    "     # Search Pinecone\n",
    "    results = index.query(\n",
    "        vector=query_embedding,\n",
    "        namespace=namespace,\n",
    "        top_k=top_k,\n",
    "        include_metadata=True\n",
    "    )\n",
    "    \n",
    "    return results\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  },
  "vincent": {
   "sessionId": "e7eadd67553f88b2e8135e81_2025-05-20T08-40-04-678Z"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
