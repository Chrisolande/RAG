{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellUniqueIdByVincent": "dcf3e"
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'ContextualHeaderRAGChain' from 'llm_interface' (/home/olande/Desktop/Rag_Techniques/Contextual Chunk Headers/llm_interface.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 12\u001b[39m\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mvector_store\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m VectorstoreManager\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mretrieval\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m StandardRetriever, ContextualHeaderRetriever\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mllm_interface\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m get_openrouter_llm, StandardRAGChain, ContextualHeaderRAGChain\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mevaluation\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m RAGEvaluator\n\u001b[32m     15\u001b[39m \u001b[38;5;66;03m# Check if environment variables are set\u001b[39;00m\n",
      "\u001b[31mImportError\u001b[39m: cannot import name 'ContextualHeaderRAGChain' from 'llm_interface' (/home/olande/Desktop/Rag_Techniques/Contextual Chunk Headers/llm_interface.py)"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "from embedding import GeminiEmbeddings\n",
    "from document_processor import DocumentProcessor, ContextualHeaderProcessor\n",
    "from vector_store import VectorstoreManager\n",
    "from retrieval import StandardRetriever, ContextualHeaderRetriever\n",
    "from llm_interface import get_openrouter_llm, StandardRAGChain, ContextualHeaderRAGChain\n",
    "from evaluation import RAGEvaluator\n",
    "\n",
    "# Check if environment variables are set\n",
    "required_env_vars = [\"GEMINI_API_KEY\", \"PINECONE_API_KEY\", \"OPENROUTER_API_KEY\"]\n",
    "missing_vars = [var for var in required_env_vars if not os.getenv(var)]\n",
    "\n",
    "if missing_vars:\n",
    "    print(f\"Missing environment variables: {', '.join(missing_vars)}\")\n",
    "    print(\"Please set these variables in your .env file.\")\n",
    "else:\n",
    "    print(\"All required environment variables are set.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "866d9da6",
   "metadata": {
    "cellUniqueIdByVincent": "e9f37"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellUniqueIdByVincent": "4fd84"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellUniqueIdByVincent": "98e14"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellUniqueIdByVincent": "19102"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1368000"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Optimized evaluation metrics module for the RAG experiment.\n",
    "Implements metrics for evaluating RAG performance using established libraries.\n",
    "\"\"\"\n",
    "import time\n",
    "from typing import List, Dict, Any, Callable, Optional, Tuple, Union\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.embeddings import Embeddings\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Import specialized libraries for metrics\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics import precision_score\n",
    "import pandas as pd\n",
    "\n",
    "# Optional: Import RAGAS if available (recommended installation)\n",
    "try:\n",
    "    import ragas\n",
    "    from ragas.metrics import (\n",
    "        context_precision,\n",
    "        context_relevancy,\n",
    "        faithfulness,\n",
    "        answer_relevancy\n",
    "    )\n",
    "    RAGAS_AVAILABLE = True\n",
    "except ImportError:\n",
    "    RAGAS_AVAILABLE = False\n",
    "    print(\"RAGAS library not found. Some advanced metrics will not be available.\")\n",
    "    print(\"Install with: pip install ragas\")\n",
    "\n",
    "\n",
    "class RAGEvaluator:\n",
    "    \"\"\"\n",
    "    Class for evaluating RAG system performance using optimized metrics.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, embeddings: Embeddings, use_ragas: bool = True):\n",
    "        \"\"\"\n",
    "        Initialize the RAGEvaluator.\n",
    "        \n",
    "        Args:\n",
    "            embeddings: Embedding model to use for semantic similarity\n",
    "            use_ragas: Whether to use RAGAS library if available\n",
    "        \"\"\"\n",
    "        self.embeddings = embeddings\n",
    "        self.use_ragas = use_ragas and RAGAS_AVAILABLE\n",
    "        self.results = {\n",
    "            \"standard\": {},\n",
    "            \"contextual\": {}\n",
    "        }\n",
    "    \n",
    "    def evaluate_retrieval_accuracy(\n",
    "        self,\n",
    "        query: str,\n",
    "        retrieved_docs: List[Document],\n",
    "        relevant_docs: List[Document]\n",
    "    ) -> float:\n",
    "        \"\"\"\n",
    "        Evaluate retrieval accuracy by comparing retrieved documents to relevant documents.\n",
    "        \n",
    "        Args:\n",
    "            query: Query string\n",
    "            retrieved_docs: Documents retrieved by the system\n",
    "            relevant_docs: Documents known to be relevant\n",
    "            \n",
    "        Returns:\n",
    "            Precision score (0-1)\n",
    "        \"\"\"\n",
    "        if self.use_ragas:\n",
    "            # Convert to RAGAS format (would require implementation depending on exact API)\n",
    "            # This is a placeholder for the actual implementation\n",
    "            try:\n",
    "                # Convert docs to RAGAS format and use context_precision\n",
    "                precision = self._ragas_context_precision(query, retrieved_docs, relevant_docs)\n",
    "                return precision\n",
    "            except Exception as e:\n",
    "                print(f\"RAGAS precision calculation failed: {e}. Falling back to standard method.\")\n",
    "                # Fall back to standard method\n",
    "                pass\n",
    "        \n",
    "        # Standard method (optimized from original)\n",
    "        # Get document IDs for comparison\n",
    "        retrieved_ids = [doc.metadata.get(\"id\", doc.metadata.get(\"source\", \"\")) for doc in retrieved_docs]\n",
    "        relevant_ids = [doc.metadata.get(\"id\", doc.metadata.get(\"source\", \"\")) for doc in relevant_docs]\n",
    "        \n",
    "        # Calculate precision (proportion of retrieved documents that are relevant)\n",
    "        if not retrieved_ids:\n",
    "            return 0.0\n",
    "        \n",
    "        # Create binary arrays for precision calculation\n",
    "        y_true = [1 if doc_id in relevant_ids else 0 for doc_id in retrieved_ids]\n",
    "        y_pred = [1] * len(retrieved_ids)  # All retrieved docs are \"predicted\" relevant\n",
    "        \n",
    "        # Calculate precision using sklearn\n",
    "        if sum(y_true) == 0:  # No relevant docs retrieved\n",
    "            return 0.0\n",
    "        \n",
    "        return sum(y_true) / len(y_true)  # Average precision\n",
    "    \n",
    "    def _ragas_context_precision(self, query, retrieved_docs, relevant_docs):\n",
    "        \"\"\"\n",
    "        Helper method to calculate precision using RAGAS if available.\n",
    "        Implementation would depend on the exact RAGAS API.\n",
    "        \"\"\"\n",
    "        if not RAGAS_AVAILABLE:\n",
    "            raise ImportError(\"RAGAS not available\")\n",
    "            \n",
    "        # This is a placeholder - actual implementation would convert to RAGAS format\n",
    "        # and call the appropriate metric\n",
    "        \n",
    "        # Example implementation pattern (adjust based on actual RAGAS API):\n",
    "        # from ragas.metrics import context_precision\n",
    "        # dataset = convert_to_ragas_format(query, retrieved_docs, relevant_docs)\n",
    "        # result = context_precision.compute(dataset)\n",
    "        # return result[\"context_precision\"]\n",
    "        \n",
    "        # For now, we'll just use our standard implementation\n",
    "        retrieved_ids = [doc.metadata.get(\"id\", doc.metadata.get(\"source\", \"\")) for doc in retrieved_docs]\n",
    "        relevant_ids = [doc.metadata.get(\"id\", doc.metadata.get(\"source\", \"\")) for doc in relevant_docs]\n",
    "        \n",
    "        if not retrieved_ids:\n",
    "            return 0.0\n",
    "            \n",
    "        relevant_retrieved = [doc_id for doc_id in retrieved_ids if doc_id in relevant_ids]\n",
    "        precision = len(relevant_retrieved) / len(retrieved_ids)\n",
    "        \n",
    "        return precision\n",
    "    \n",
    "    def evaluate_semantic_relevance(\n",
    "        self,\n",
    "        query: str,\n",
    "        retrieved_docs: List[Document]\n",
    "    ) -> float:\n",
    "        \"\"\"\n",
    "        Evaluate semantic relevance of retrieved documents to the query.\n",
    "        \n",
    "        Args:\n",
    "            query: Query string\n",
    "            retrieved_docs: Documents retrieved by the system\n",
    "            \n",
    "        Returns:\n",
    "            Average cosine similarity score (0-1)\n",
    "        \"\"\"\n",
    "        if not retrieved_docs:\n",
    "            return 0.0\n",
    "            \n",
    "        # Get query embedding\n",
    "        query_embedding = self.embeddings.embed_query(query)\n",
    "        \n",
    "        # Get document embeddings\n",
    "        doc_contents = [doc.page_content for doc in retrieved_docs]\n",
    "        doc_embeddings = self.embeddings.embed_documents(doc_contents)\n",
    "        \n",
    "        # Calculate cosine similarities using sklearn\n",
    "        query_embedding_reshaped = np.array(query_embedding).reshape(1, -1)\n",
    "        doc_embeddings_array = np.array(doc_embeddings)\n",
    "        \n",
    "        # Use sklearn's cosine_similarity for efficient calculation\n",
    "        similarities = cosine_similarity(query_embedding_reshaped, doc_embeddings_array)[0]\n",
    "        \n",
    "        # Return average similarity\n",
    "        return np.mean(similarities)\n",
    "    \n",
    "    def evaluate_response_quality(\n",
    "        self,\n",
    "        query: str,\n",
    "        response: str,\n",
    "        reference_answer: Optional[str] = None,\n",
    "        llm_evaluator = None\n",
    "    ) -> float:\n",
    "        \"\"\"\n",
    "        Evaluate the quality of the response.\n",
    "        If a reference answer is provided, compare the response to it.\n",
    "        Otherwise, use an LLM to evaluate the response quality.\n",
    "        \n",
    "        Args:\n",
    "            query: Query string\n",
    "            response: Response from the RAG system\n",
    "            reference_answer: Optional reference answer\n",
    "            llm_evaluator: Optional LLM for evaluation\n",
    "            \n",
    "        Returns:\n",
    "            Quality score (0-1)\n",
    "        \"\"\"\n",
    "        if self.use_ragas and reference_answer:\n",
    "            try:\n",
    "                # Use RAGAS for evaluation if available\n",
    "                # This would need implementation based on the RAGAS API\n",
    "                score = self._ragas_answer_evaluation(query, response, reference_answer)\n",
    "                return score\n",
    "            except Exception as e:\n",
    "                print(f\"RAGAS answer evaluation failed: {e}. Falling back to standard method.\")\n",
    "                # Fall back to standard method\n",
    "                pass\n",
    "                \n",
    "        if reference_answer and self.embeddings:\n",
    "            # Compare response to reference answer using semantic similarity\n",
    "            response_embedding = self.embeddings.embed_query(response)\n",
    "            reference_embedding = self.embeddings.embed_query(reference_answer)\n",
    "            \n",
    "            # Use sklearn's cosine_similarity\n",
    "            response_reshaped = np.array(response_embedding).reshape(1, -1)\n",
    "            reference_reshaped = np.array(reference_embedding).reshape(1, -1)\n",
    "            similarity = cosine_similarity(response_reshaped, reference_reshaped)[0][0]\n",
    "            \n",
    "            return similarity\n",
    "        \n",
    "        elif llm_evaluator:\n",
    "            # Use LLM to evaluate response quality\n",
    "            evaluation = llm_evaluator.evaluate_response(query, response)\n",
    "            return evaluation\n",
    "            \n",
    "        else:\n",
    "            # If no reference answer or LLM evaluator, return None\n",
    "            return None\n",
    "            \n",
    "    def _ragas_answer_evaluation(self, query, response, reference_answer):\n",
    "        \"\"\"\n",
    "        Helper method to evaluate answer using RAGAS metrics if available.\n",
    "        Implementation would depend on the exact RAGAS API.\n",
    "        \"\"\"\n",
    "        if not RAGAS_AVAILABLE:\n",
    "            raise ImportError(\"RAGAS not available\")\n",
    "            \n",
    "        # This is a placeholder - actual implementation would convert to RAGAS format\n",
    "        # and call the appropriate metrics\n",
    "        \n",
    "        # Example implementation pattern (adjust based on actual RAGAS API):\n",
    "        # from ragas.metrics import answer_relevancy, faithfulness\n",
    "        # dataset = convert_to_ragas_format(query, response, reference_answer)\n",
    "        # relevancy_result = answer_relevancy.compute(dataset)\n",
    "        # faithfulness_result = faithfulness.compute(dataset)\n",
    "        # return (relevancy_result[\"answer_relevancy\"] + faithfulness_result[\"faithfulness\"]) / 2\n",
    "        \n",
    "        # For now, we'll calculate cosine similarity using embeddings\n",
    "        response_embedding = self.embeddings.embed_query(response)\n",
    "        reference_embedding = self.embeddings.embed_query(reference_answer)\n",
    "        \n",
    "        # Use sklearn's cosine_similarity\n",
    "        response_reshaped = np.array(response_embedding).reshape(1, -1)\n",
    "        reference_reshaped = np.array(reference_embedding).reshape(1, -1)\n",
    "        similarity = cosine_similarity(response_reshaped, reference_reshaped)[0][0]\n",
    "        \n",
    "        return similarity\n",
    "    \n",
    "    def measure_query_time(\n",
    "        self,\n",
    "        query_func: Callable[[str], Any],\n",
    "        query: str\n",
    "    ) -> float:\n",
    "        \"\"\"\n",
    "        Measure the time taken to process a query.\n",
    "        \n",
    "        Args:\n",
    "            query_func: Function that processes the query\n",
    "            query: Query string\n",
    "            \n",
    "        Returns:\n",
    "            Time taken in seconds\n",
    "        \"\"\"\n",
    "        start_time = time.time()\n",
    "        query_func(query)\n",
    "        end_time = time.time()\n",
    "        \n",
    "        return end_time - start_time\n",
    "    \n",
    "    def run_evaluation(\n",
    "        self,\n",
    "        queries: List[str],\n",
    "        standard_rag,\n",
    "        contextual_rag,\n",
    "        relevant_docs: Optional[Dict[str, List[Document]]] = None,\n",
    "        reference_answers: Optional[Dict[str, str]] = None,\n",
    "        llm_evaluator = None\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Run a comprehensive evaluation on both RAG systems.\n",
    "        \n",
    "        Args:\n",
    "            queries: List of query strings\n",
    "            standard_rag: Standard RAG system\n",
    "            contextual_rag: Contextual header RAG system\n",
    "            relevant_docs: Optional dictionary mapping queries to relevant documents\n",
    "            reference_answers: Optional dictionary mapping queries to reference answers\n",
    "            llm_evaluator: Optional LLM for evaluation\n",
    "        \"\"\"\n",
    "        print(f\"Running evaluation on {len(queries)} queries...\")\n",
    "        for query in tqdm(queries, desc=\"Evaluating queries\"):\n",
    "            # Evaluate standard RAG\n",
    "            standard_time = self.measure_query_time(standard_rag.invoke, query)\n",
    "            standard_response = standard_rag.invoke(query)\n",
    "            standard_docs = standard_rag.retriever.get_relevant_documents(query)\n",
    "            \n",
    "            # Evaluate contextual header RAG\n",
    "            contextual_time = self.measure_query_time(contextual_rag.invoke, query)\n",
    "            contextual_response = contextual_rag.invoke(query)\n",
    "            contextual_docs = contextual_rag.retriever.get_relevant_documents(query)\n",
    "            \n",
    "            # Store results for this query\n",
    "            self.results[\"standard\"][query] = {\n",
    "                \"response\": standard_response,\n",
    "                \"retrieved_docs\": standard_docs,\n",
    "                \"query_time\": standard_time,\n",
    "                \"semantic_relevance\": self.evaluate_semantic_relevance(query, standard_docs)\n",
    "            }\n",
    "            \n",
    "            self.results[\"contextual\"][query] = {\n",
    "                \"response\": contextual_response,\n",
    "                \"retrieved_docs\": contextual_docs,\n",
    "                \"query_time\": contextual_time,\n",
    "                \"semantic_relevance\": self.evaluate_semantic_relevance(query, contextual_docs)\n",
    "            }\n",
    "            \n",
    "            # Add retrieval accuracy if relevant docs are provided\n",
    "            if relevant_docs and query in relevant_docs:\n",
    "                self.results[\"standard\"][query][\"retrieval_accuracy\"] = self.evaluate_retrieval_accuracy(\n",
    "                    query, standard_docs, relevant_docs[query]\n",
    "                )\n",
    "                self.results[\"contextual\"][query][\"retrieval_accuracy\"] = self.evaluate_retrieval_accuracy(\n",
    "                    query, contextual_docs, relevant_docs[query]\n",
    "                )\n",
    "            \n",
    "            # Add response quality if reference answers or LLM evaluator are provided\n",
    "            if reference_answers and query in reference_answers:\n",
    "                self.results[\"standard\"][query][\"response_quality\"] = self.evaluate_response_quality(\n",
    "                    query, standard_response, reference_answers[query]\n",
    "                )\n",
    "                self.results[\"contextual\"][query][\"response_quality\"] = self.evaluate_response_quality(\n",
    "                    query, contextual_response, reference_answers[query]\n",
    "                )\n",
    "            elif llm_evaluator:\n",
    "                self.results[\"standard\"][query][\"response_quality\"] = self.evaluate_response_quality(\n",
    "                    query, standard_response, llm_evaluator=llm_evaluator\n",
    "                )\n",
    "                self.results[\"contextual\"][query][\"response_quality\"] = self.evaluate_response_quality(\n",
    "                    query, contextual_response, llm_evaluator=llm_evaluator\n",
    "                )\n",
    "    \n",
    "    def get_summary_metrics(self) -> Dict[str, Dict[str, float]]:\n",
    "        \"\"\"\n",
    "        Get summary metrics for both RAG systems.\n",
    "        \n",
    "        Returns:\n",
    "            Dictionary of summary metrics\n",
    "        \"\"\"\n",
    "        summary = {\n",
    "            \"standard\": {},\n",
    "            \"contextual\": {}\n",
    "        }\n",
    "        \n",
    "        for system in [\"standard\", \"contextual\"]:\n",
    "            # Calculate average query time\n",
    "            query_times = [result[\"query_time\"] for result in self.results[system].values()]\n",
    "            summary[system][\"avg_query_time\"] = np.mean(query_times)\n",
    "            \n",
    "            # Calculate average semantic relevance\n",
    "            semantic_relevances = [result[\"semantic_relevance\"] for result in self.results[system].values()]\n",
    "            summary[system][\"avg_semantic_relevance\"] = np.mean(semantic_relevances)\n",
    "            \n",
    "            # Calculate average retrieval accuracy if available\n",
    "            retrieval_accuracies = [\n",
    "                result.get(\"retrieval_accuracy\") \n",
    "                for result in self.results[system].values() \n",
    "                if \"retrieval_accuracy\" in result\n",
    "            ]\n",
    "            if retrieval_accuracies:\n",
    "                summary[system][\"avg_retrieval_accuracy\"] = np.mean(retrieval_accuracies)\n",
    "            \n",
    "            # Calculate average response quality if available\n",
    "            response_qualities = [\n",
    "                result.get(\"response_quality\") \n",
    "                for result in self.results[system].values() \n",
    "                if \"response_quality\" in result\n",
    "            ]\n",
    "            if response_qualities:\n",
    "                summary[system][\"avg_response_quality\"] = np.mean(response_qualities)\n",
    "        \n",
    "        return summary\n",
    "    \n",
    "    def visualize_results(self, output_path: Optional[str] = None):\n",
    "        \"\"\"\n",
    "        Visualize the evaluation results.\n",
    "        \n",
    "        Args:\n",
    "            output_path: Optional path to save the visualization\n",
    "        \"\"\"\n",
    "        summary = self.get_summary_metrics()\n",
    "        \n",
    "        # Use pandas and seaborn for better visualizations\n",
    "        import seaborn as sns\n",
    "        sns.set_style(\"whitegrid\")\n",
    "        \n",
    "        # Prepare data for plotting\n",
    "        metrics = []\n",
    "        values = []\n",
    "        systems = []\n",
    "        \n",
    "        for system in [\"standard\", \"contextual\"]:\n",
    "            for metric, value in summary[system].items():\n",
    "                metrics.append(metric)\n",
    "                values.append(value)\n",
    "                systems.append(\"Standard RAG\" if system == \"standard\" else \"Contextual Header RAG\")\n",
    "        \n",
    "        # Create DataFrame\n",
    "        df = pd.DataFrame({\n",
    "            \"Metric\": metrics,\n",
    "            \"Value\": values,\n",
    "            \"System\": systems\n",
    "        })\n",
    "        \n",
    "        # Create figure with subplots\n",
    "        fig, axs = plt.subplots(2, 2, figsize=(14, 10))\n",
    "        fig.suptitle('RAG System Comparison: Standard vs. Contextual Header', fontsize=16)\n",
    "        \n",
    "        # Get unique metrics\n",
    "        unique_metrics = df[\"Metric\"].unique()\n",
    "        \n",
    "        # Plot each metric\n",
    "        plot_positions = [(0, 0), (0, 1), (1, 0), (1, 1)]\n",
    "        \n",
    "        for i, metric in enumerate(unique_metrics):\n",
    "            if i < len(plot_positions):\n",
    "                row, col = plot_positions[i]\n",
    "                metric_data = df[df[\"Metric\"] == metric]\n",
    "                \n",
    "                # Use seaborn for better looking bars\n",
    "                sns.barplot(x=\"System\", y=\"Value\", data=metric_data, ax=axs[row, col])\n",
    "                \n",
    "                # Format the plot\n",
    "                axs[row, col].set_title(f'Average {\" \".join(metric.split(\"_\")[1:]).title()}')\n",
    "                axs[row, col].set_ylabel(self._get_metric_unit(metric))\n",
    "                \n",
    "                # Add value labels on top of bars\n",
    "                for j, p in enumerate(axs[row, col].patches):\n",
    "                    axs[row, col].annotate(f'{p.get_height():.3f}', \n",
    "                                         (p.get_x() + p.get_width() / 2., p.get_height()),\n",
    "                                         ha='center', va='bottom')\n",
    "        \n",
    "        # Fill empty plots if any\n",
    "        for i in range(len(unique_metrics), len(plot_positions)):\n",
    "            row, col = plot_positions[i]\n",
    "            axs[row, col].text(0.5, 0.5, 'Metric Not Available', \n",
    "                             horizontalalignment='center', verticalalignment='center')\n",
    "        \n",
    "        # Adjust layout\n",
    "        plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "        \n",
    "        # Save or show the figure\n",
    "        if output_path:\n",
    "            plt.savefig(output_path, dpi=300, bbox_inches='tight')\n",
    "        else:\n",
    "            plt.show()\n",
    "    \n",
    "    def _get_metric_unit(self, metric: str) -> str:\n",
    "        \"\"\"Helper function to get appropriate y-axis label for metrics\"\"\"\n",
    "        if \"time\" in metric:\n",
    "            return \"Time (seconds)\"\n",
    "        elif \"relevance\" in metric:\n",
    "            return \"Cosine Similarity\"\n",
    "        elif \"accuracy\" in metric:\n",
    "            return \"Precision\"\n",
    "        elif \"quality\" in metric:\n",
    "            return \"Quality Score\"\n",
    "        else:\n",
    "            return \"Value\"\n",
    "            \n",
    "    def export_results(self, output_path: str):\n",
    "        \"\"\"\n",
    "        Export evaluation results to CSV.\n",
    "        \n",
    "        Args:\n",
    "            output_path: Path to save the CSV\n",
    "        \"\"\"\n",
    "        # Prepare data for export\n",
    "        data = []\n",
    "        \n",
    "        for system in [\"standard\", \"contextual\"]:\n",
    "            for query, result in self.results[system].items():\n",
    "                row = {\n",
    "                    \"system\": system,\n",
    "                    \"query\": query,\n",
    "                    \"query_time\": result[\"query_time\"],\n",
    "                    \"semantic_relevance\": result[\"semantic_relevance\"]\n",
    "                }\n",
    "                \n",
    "                # Add optional metrics if available\n",
    "                if \"retrieval_accuracy\" in result:\n",
    "                    row[\"retrieval_accuracy\"] = result[\"retrieval_accuracy\"]\n",
    "                \n",
    "                if \"response_quality\" in result:\n",
    "                    row[\"response_quality\"] = result[\"response_quality\"]\n",
    "                \n",
    "                data.append(row)\n",
    "        \n",
    "        # Create DataFrame and export\n",
    "        df = pd.DataFrame(data)\n",
    "        df.to_csv(output_path, index=False)\n",
    "        print(f\"Results exported to {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellUniqueIdByVincent": "a7948"
   },
   "outputs": [],
   "source": [
    "ggggggggggggggggggggggggggggggggggggggggffffgghghghghghggggggggggggggggggggggggggggggggggggggggggggggghuhuhuhuh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellUniqueIdByVincent": "00737"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellUniqueIdByVincent": "1e98a"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  },
  "vincent": {
   "sessionId": "05dd7bc0e84d6773754e6673_2025-05-20T18-15-25-922Z"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
